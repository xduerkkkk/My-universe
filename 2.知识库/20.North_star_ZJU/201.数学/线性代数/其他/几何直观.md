---
share_link: https://share.note.sx/e9oalpf3#lL9Q0KMq7hW3RvZ+DmNY/pKpDYUvk92C2/Rgz0fj1u8
share_updated: 2025-09-23T14:07:00+08:00
---
Span的概念，就算R的概念嘛
就是说几维嘛 哦 好像有点概念
基础的空间就是R，然后基向量的个数定义形成向量的空间，这个空间是span？
没问题，R就是舞台。R方永远是2维的。

比如二维空间，叫R2，然后如果两个基向量， 那就形成一个二维平面span
如果三个基向量，在三维空间
- 若第三个向量就在那俩个基向量形成的span上，那结果不变
- 若不在，那就有无数个二维span的可能，第三个基向量决定了span的位置。

对于空间的变换
让坐标系的变换做到等间距，保持坐标轴直线 就叫线性变换

矩阵就算空间变换的方法，且是线性变换
多个矩阵在一起，应该从右往左读，就好比f（g（x）） 我们要从右往左读一样，但是我还是不能非常get到。我理解顺序不同效果不同，但不同顺序一定都可以解读，所以从右往左只是个习惯吗？
就是固定顺序，规定的，我们肯定和最近的矩阵先作用

线性变换的特性，就是原本比如v =  -1i + 2 j
变换后依旧是
这就引出一个思想
如果我们想追踪v怎么变换（即空间任意向量）
我们追踪i和j怎么变换就好了

比如，一个2×2的矩阵 A（我不知道是不是叫二维矩阵） ，乘一个列向量B `[5,7]` （竖着）

其实就可以理解成，这个列向量的基向量，其i变成了A的第一列， j变成了A的第二列
原本，列向量B是 5i + 7j 现在i和j都变了
所以最后就是 5新i（A的第一列） +  7新j（A的第二列） 

所以，矩阵就代表新的基向量，也就代表了新的空间
这就可以理解，加入2×2矩阵，第一列与第二列呈倍数
则空间变成了一维，因为俩个基向量在同一条线上，形成的span就是一条线 

拿俩个矩阵相乘举例  假设俩个矩阵就是2×2，即，二维空间。第一列就是基向量i，第二列就是基向量j。
第二个矩阵的第一列，直接表面了i向量到哪了去，然后最终的i向量呢
就是第一个矩阵作用在i向量上得到的结果。  这很好理解，就是俩个连续的对基向量的变换 
第二个矩阵的第二列同理
也就是，俩个2x2矩阵相乘，最终是分解乘俩个，上文所述的，2x2矩阵与一个列向量相乘

## 三维空间矩阵乘法
3乘3的矩阵，应该和2乘2的矩阵分析，一模一样。线性变换的基向量变成3个而已

## 行列式
矩阵线性变换，将原来两个基向量，围住的的面积，扩大的倍数，就是行列式的值
代表所有区域的面积扩大倍数 
所以变成0就是空间压缩到一个点。
啊哦，负数是什么意思呢？
就是变换时，空间翻转了一下，即坐标轴翻转，”空间定向发生改变“。
故准确来说行列式的绝对值是面积扩大倍数
三维的话，就说”体积“
空间定向仍然是三个坐标轴变
计算行列式的方法也很简单， 就是围的新面积， 初中几何数学。
稍稍复杂的推导而已，大概是三角形、平行四边形、正方形捣鼓，
得出来对角线相乘再相减，就是那个围的平行四边形的面积 
那么我们说，两个矩阵相乘的行列式，等于两个矩阵的行列式相乘
为什么呢？ 根据直觉来说，  第一个算的面积， 和第二个方法的俩个面积... 咦，为什么呢？ 
1. 我们有一个单位面积的正方形。
    
2. 它先经过了 M₂ 的变换。根据行列式的定义，它的面积变成了 det(M₂)。
    
3. 现在，这个**面积已经是 det(M₂)** 的图形，要**再次**经过 M₁ 的变换。
    
4. M₁ 变换的作用是什么？是将它“看到”的所有面积，都再乘以 det(M₁) 倍。
    
5. 所以，最终的面积，就是在 det(M₂) 的基础上，再乘以 det(M₁)。
    
6. 因此，总的面积缩放倍数 det(M₁M₂)，就等于两次缩放倍数的乘积：det(M₁) * det(M₂)。

## 线性方程组
首先，假如是二维线性方程组可以写为
矩阵 乘一个列向量 x，y = 一个向量的形式
这实在太形象了！
这个式子描述的是，我们要求的向量，经过了一个矩阵的空间变换，变成了另一个向量
我们要求这个向量，那就把得到的向量，逆用我们的空间变换，就追溯回去了啊！ 
刚提到的过程，拓展到高维也完全没问题
这样也能理解，行列式为0的话没有解是为什么，因为根本没有这个逆变换啊！没有逆变换就无法追溯！
引入一个名词，rank。 就是行列式为0，看我们降维到哪，
等一下有个疑惑，为什么三维降维到平面，行列式也为0呢？不是存在面积吗？...emm 正方体的体积是..哦 没有体积啊，只要没有体积那就行列式为0.
ok 然后三维若降维的是平面，rank=2，若降维是直线，rank=1   
还记得第一次听rank是lora方法，说低rank矩阵， 原来是这个意思！ 空间降维 后的维度数
又引出一个知识点，看到一个矩阵，直接能通过列向量所构成的span，判断矩阵的rank
若所有列向量都不共线，就叫满秩
三维空间，若rank=2  就是降维成一个平面，那原始三维空间的线，会全被压缩到零向量
若rank=1 降维成一条线，那原始三维空间的平面，会被压缩到零向量。 、
但我不知道怎么理解，只能模糊的想象
- 这背后有一个非常优美的定理，叫做**“秩-零度定理”(Rank-Nullity Theorem)**：  
    Rank(A) + Nullity(A) = n  
    (变换后空间的维度 + 被压缩成零的源空间的维度 = 变换前空间的维度)
    

变换后落在原点的点的集合，就是矩阵的核/零空间
所以，如果说A乘x等于列向量0，那么这个x一定是A矩阵的核

## 点积
数学运算很熟悉， 几何计算也很熟悉（投影，然后长度相乘，若方向相反加个负号）
但之前一直没自问过， 为啥这个投影计算，与顺序无关？
哦，这个思路好，从俩向量长度一样开始想，再拓展到不一样，然后再思考顺序，确实顺序无关
什么叫对偶性？
举例，一个行向量1，-2 乘一个列向量 4， 3
行向量，理解成本来是两个列向量，新i和新j，但是被压扁了，  新i和新j跑到一维的数轴上去了
然后，这就是个空间变换， 作用列向量4，3  就和当时说2乘2矩阵乘一个列向量的思路一样了

再进一步思考
一个行向量的两列的两个数
分别代表了，数轴上单位向量，投影到二维空间的x坐标和y坐标，这里的证明是初中几何的对称性  
所以依然很好理解一个行向量1，-2 乘一个列向量 4， 3 这个例子
就是把列向量所处的i和j，投影到数轴后，列向量变化后的样子

这个思考方式的意义就是，把两个向量点积看作一个矩阵空间变化作用在一个向量

## 叉积
其实就是行列式那块的定义
然后具体看围的面积的大小，
同样正负号也是行列式正负号
不过这是二维，，
其实但是叉积运算和潜在几何直观联系又有大学问... 
两个列向量，每个向量三个元素，他们的叉积
是一个矩阵的行列式，这个矩阵是三个列向量
第一列是i，j，k ？？ 第二第三是那俩个列向量
为什么？  
详细  
Cross products in the light of linear transformations | Chapter 11, Essence of linear algebra视频讲完了，我感觉每一帧都能看懂，但我感觉大框架没厘清，到底在讲什么，为什么要这么讲

就说，我们已知，三个三维列向量（到底怎么称呼他们）的叉积，是三个列向量拼成矩阵的行列式  其中第一个向量A我们说未知向量，后面俩个假设是已知向量
我们怎么转换一下？
我们转换成， 能不能找一个向量，p，点积A，得到的结果刚好上刚才说的行列式？
是的，可以转换， 左右列一个等式，就能写出p向量的表达式了 

然后几何上解释就是， p向量与A向量的点积，等于空间上，A向量与另外俩个固定的向量，所构成的六面体的体积 
进一步证明  那个六面体体积，底是后面俩固定的向量围成的，   高，是A向量投影到底的法向量的长度  
这个过程，等价于 A向量 投影到  垂直与底 且 长度等于底面积的向量 

# 克拉默法则待完成 

先回到简单的线性方程组
这里有个误区
就是，原本正常的空间  其矩阵是 俩个列向量 1，0 和 0， 1
向量，二维 x，y，他的x就是x点积1，0  得x    y是y点积0，1 得y
那么转换的矩阵空间，列向量是 我们说新i，新j吧
转换后的x，点积新i  转后后的y 点积新j  能得到之前空间的x，y吗？
直觉上就是，x 点积 i     和转换后的x 点积相同转换后的i  可能会一样
然而实际是大概率不一样
点积不能保证空间变换后还一致
除了，正交矩阵 也就是能让空间90°变换的矩阵

# 基向量
额，不同基向量代表不同视角
很好理解
如何互相转化视角呢？
比如，一个a向量，在我们矩阵变换后，视角中的A向量，如何变成，在b矩阵变换下，视角中的B向量？
答案是  b矩阵的逆 乘 A向量 
为什么？这个过程怎么能等于
A向量 还原回a向量 再去b矩阵作用 
哦 矩阵交换律 那个b矩阵的作用 与还原a向量 合在一起 就是b矩阵的逆
还能怎么更顺畅的理解？
嗯...b矩阵，是在我们的视角下，新的i、j
b矩阵的逆，是逆的空间变换，，， 

她的向量 乘我们视角的她的矩阵（即她的基向量）  是我们视角的她的向量  
然后再乘我们视角的变换矩阵  达到我们的效果，但这个效果得到的是我们的语言
所以在左乘一个她的矩阵的逆 就是她的坐标系下 转换后的向量

# 特征向量和特征值
理解了空间变换后
理解特征向量的那个公式实在太简单了
矩阵作用后  原空间会进行空间变换 几乎每个向量都会变
但可能会有特殊的向量，只在其方向上变换，即拉伸，这些向量就叫该矩阵的特征向量
拉伸的长度，就叫特征向量的值
基向量就是特征向量的情况，我看就是对角矩阵

然后引申出一个知识点
一个矩阵，计算出它的特征向量，  若用俩特征向量作为基
就可以拼接成一个特征矩阵  
特征矩阵 左乘矩阵   额，会怎样？
再左乘特征矩阵的逆 就是，特征向量为基的视角下， 变换后的向量 
一定是对角矩阵


