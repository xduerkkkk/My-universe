# 第1章 **建模的基础：模型如何学习？**

# Chapter 0： **连接理论与现实：表征的艺术**
## 0.1 为什么不能用原始数据

## 0.2 数据预处理：清洗画布
- 标准化/归一化
- **处理类别特征**
- 特征缩放
## 0.3 特征工程

## Chapter 1: 学习的语言：概率与信息

### 1.1 世界观： 贝叶斯
- 贝叶斯定理
- 似然、先验、后验、证据
### 1.2 最大似然与最大后验？
- 推导参数估计的两种基本哲学
- 从概率视角将MAP与“正则化”思想连接
### 1.3 度量：熵与交叉熵
- 信息论
- 为什么交叉熵是分类问题最自然的损失函数

## Chapter 2： **学习的过程：优化与评估**
### 2.1 梯度下降
- 损失函数地貌
- Batch, SGD, Mini-batch

### 2.2 过拟合问题
- 偏差-方差
- 正则化：是为了防止模型过于复杂而产生的过拟合。你可以用多项式回归的例子，直观地展示没有正则化和有正则化的拟合曲线区别。这里重点是“What”**和**“Why (intuitive)”

### 2.3 验证评估
- 训练集/测试集、交叉验证
- 各个验证指标


特征工程和数据预处理呢？
# 第二章：线性世界

## Chapter3 ： **作为概率模型的线性模型**


## **3.1 The Ancestor: Perceptron.** (始祖：感知机)
    
- The simplest linear decision boundary.
	
- The error-driven update rule.


### 3.2 线性回归
- 证明最小化MSE等价于高斯噪声假设下的最大似然  ？
- 从MAP视角，用高斯和拉普拉斯先验推导出岭回归和Lasso？
？ 在这里介绍接近过拟合中的正则化？
### 3.3 逻辑回归
- 最小化交叉熵等价于伯努利分布假设下的最大似然？

### 3.4 Fisher判别

# 第三章 非线性世界
## **Chapter 4** **核技巧：隐式地升维**
### 4.1 SVM
### 4.2 核函数
核技巧的本质：无需进行高维投影即可计算结果


## **Chapter 5: 分而治之：树的力量**
### 5.1 基本单元：决策树

### 5.2群众的智慧：集成方法
- **Bagging (Random Forest):**
- **Boosting (GBDT/XGBoost):**
这里call back了bias-variance tradeoff

## **Chapter 6:不可见的结构：从无标签数据中学习)**

### **6.1 Finding Groups: Clustering.** (寻找族群：聚类)

- K-Means: 
- Gaussian Mixture Models (GMM):  (高斯混合模型：聚类的概率视角)
- **Hierarchical Clustering:** 层次聚类

还有个层次聚类吧

### **6.2 Finding Essence: Dimensionality Reduction.** (寻找本质：降维)
    
- Principal Component Analysis (PCA): Finding the directions of maximum variance. (寻找最大方差的方向)
	
- Connecting PCA to matrix factorization (Eigen-decomposition of the covariance matrix). (将PCA与矩阵分解联系起来)

## **Chapter 7: Learning by Analogy - Instance-based Methods (通过类比学习：基于实例的方法)**

### **7.1 The Core Idea: k-Nearest Neighbors (k-NN).** (核心思想：k近邻)
    
- No "training" phase, just storing data. (没有“训练”阶段，只是存储数据)
	
- The importance of the distance metric and the choice of 'k'. (距离度量和'k'值选择的重要性)
	
- Connecting this back to the "Curse of Dimensionality". (将其与“维度灾难”联系起来)