实例驱动 

一化 (Normalization) —— “公平的竞技场”**

为了解决这个“量纲霸权”问题，我们需要在进行kNN计算之前，对数据进行一次预处理。这个预处理，就叫做**归一化**或**标准化**。

**战略目标**：消除不同特征之间由于量纲和数值范围不同所带来的不公平影响，让**每个特征都在同等重要的地位上**参与距离计算。

**常用战术 (Slide 72)**：**最小-最大规范化 (Min-Max Normalization)**

*   **公式**：
    $y_i^j = \frac{x_i^j - \min(x^j)}{\max(x^j) - \min(x^j)}$
*   **解码**：
    *   $x_i^j$：第`i`个样本的第`j`个特征值。
    *   $\min(x^j)$：在**所有样本**中，第`j`个特征的**最小值**。
    *   $\max(x^j)$：在**所有样本**中，第`j`个特征的**最大值**。
*   **作用**：这个公式，会把每一个特征的所有值，都**线性地缩放**到 **** 这个区间内。
    *   原来是最小值的点，现在变成了0。
    *   原来是最大值的点，现在变成了1。
    *   所有中间值，等比例地分布在0和1之间。

**战后效果**：
经过归一化之后，我们所有特征（身高、鞋码、体重、收入...）的数值范围都变成了 `[0, 1]`。现在，再用欧式距离去计算，任何一个特征都不会因为其原始的数值范围巨大而占据主导地位了。我们为所有特征，构建了一个**公平的竞技场**。


### **揭示“看不见”的边界：Voronoi 图**

让我们来揭示kNN决策面的真实面目。

**1. k=1 的情况（最近邻算法）**

- **思想实验**：想象一下，平面上有许多属于不同类别的训练样本点（路标）。对于空间中的任意一个新点x，它的类别由离它**最近**的那个路标决定。
    
- **几何划分**：那么，所有判定为“路标A”的区域，就是所有“离A比离任何其他路标都近”的点的集合。
    
- **这是什么？** 这在计算几何中有一个专门的名字，叫做 **Voronoi 图 (泰森多边形)**。
    
- **可视化**：每个训练样本点，都会“掌控”一块属于自己的“地盘”（一个多边形区域）。这些地盘的边界，都是由**相邻样本点连线的垂直平分线**组成的。
    
- **结论**：对于k=1的kNN，它的决策面，就是这些**Voronoi多边形的边界**。它是一组**分段线性**的复杂边界。
    

**2. k > 1 的情况**

- **思想实验**：当k变大时，情况变得更复杂。一个点的类别由周围k个邻居投票决定。
    
- **几何划分**：决策边界出现在那些“投票结果发生改变”的地方。比如，一个点稍微移动一点点，它的k个最近邻居中，一个蓝点被踢出圈，一个红点进入圈内，导致投票结果从“蓝色”翻转为“红色”。这个发生翻转的点，就位于决策边界上。
    
- **结论**：对于k>1的kNN，它的决策面通常会比k=1时**更平滑**，但形状依然是高度**不规则、非线性**的。它的复杂程度，完全由训练数据的分布所决定。
    

**总结：**  
您说“看不见”，是因为它没有一个简单的**公式**。  
但它在几何上是**看得见的**，它是由训练样本的位置，隐式地、共同地定义出来的一组复杂的**分段边界**。