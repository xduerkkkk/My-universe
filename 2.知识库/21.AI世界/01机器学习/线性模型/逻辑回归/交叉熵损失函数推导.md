h(x)可以理解为靠近1的概率。
若标签为1，h(x)即表示此样本正确的概率
若标签为0，1-h(x)表现此样本正确的概率
那么单个样本正确的概率为$h(x)^{y}(1-h(x))^{1-y}$
所以概率正确总概率为$L(w)=Πh(x)^{y}(1-h(x))^{1-y}$
取对数，$log L(w) =\sum({y}log(h(x))+(1-y)log((1-h(x)))$
这是表示正确的程度，不过取对数后，正确程度越高，logL(w)越接近0，正确程度越低，logL(w)越接近负无穷。我们需要最大化这个函数，让他正确程度尽可能高。
梯度下降需要最小化一个大于等于0的损失函数，我们加负号变成J(w)
那么最大化log L(w)就转化成最小化J(w)。
$$ J(\mathbf{w}) = -\frac{1}{m} \log L(\mathbf{w}) $$
最终损失函数为：
$$ J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^{m} \Big[ y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1-y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})) \Big] $$
还有另一种推导方法，见附录图片。
作业1线性模型-1759929670485.jpeg交叉熵损失函数另一种推导
![[作业1线性模型-1759929670485.jpeg]]