# 描述线性回归模型的数学形式。线性回归模型是如何进行预测的？

$$h_{\theta}(x)=\theta^Tx$$
输出$h_{\theta}(x)$就是预测值。
输出等于系数乘输入加偏置项，其中输入是样本的特征值，系数就是每个特征的权重，   这样的话输入和输出是一个线性关系。
在训练阶段，我们通过梯度下降算法找到最优参数$\theta$ 
于预测阶段，用全新输入，代入公式，所得到的就是预测值
# 定义线性回归的损失函数。为什么我们选择这种损失函数？

均方误差损失函数
$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \Big( h_{\theta}(x^{(i)}) - y^{(i)} \Big)^2 $$
- 由于平方，无论正确答案对于预测值是偏大还是偏小，都能展现出俩者差距
- 较大的误差会给予更严厉的惩罚
- 代价函数是凸函数，保证梯度下降算法能找到全局最优解


# 什么是鉴别函数？用于分类的线性鉴别函数有什么优点与缺点？
鉴别函数接收一个输入样本，然后通过公式，决定该样本属于哪一个类别。
线性鉴别函数，是用直线或平面分割数据进行分类。
- 优点：可解释性强，模型简单不容易过拟合
- 缺点：表达能力有限，无法解决非线性问题

#  线性回归模型的误差来源有哪些？在实际构建模型分别应当采用什么策略来减小相应的误差？

误差来源：
- 偏差：“偏差”衡量模型最优情况下的预测值和真实值的差距。欠拟合就指偏差误差大
- 方差：指因模型对训练数据微小波动过于敏感带来的误差，衡量新数据时模型预测结果发生的改变幅度。高方差即过拟合，拟合训练数据太完美以至于拟合了噪声
策略：
- 对于欠拟合，在输入角度上增加更多特征，在模型角度上使用多项式特征，让直线模型能拟合曲线
- 对于过拟合，在输入角度上让数据量更多，特征数量减小，在模型角度上，在代价函数里增加惩罚项，即正则化


# 有哪些策略可以将二类分类器组合构造为多类分类器？这些策略存在什么问题？各策略的计算复杂度是多少？除课堂讲解过的组合策略，是否存在其他组合策略？

策略：
- one-versus-rest：将多分类问题拆解成多个独立的二分类问题，比如有三个类别，第一个分类器训练“c1和其他”的分类。面对新输入，只需三个分类器同时进行打分，选择最高的那个
- one-versus-one: 在所有可能的类别配对之间，都训练一个二分类器，面对新输入，我们让所有分类器进行他们自己的二分类， 统计得到的结果，“票数”最多的是最终分类。
问题：
- ovr中，线性分类器会围出一个中间区域，这个区域中分类器都给出“不是”的判断，无法分类
- ovo中，仍出现中间区域，投票平局
复杂度
- ovr：复杂度 k 
- ovo： 复杂度 K(k-1)/2
新策略：
构建统一的多类分类器，对于一个输入 x，我们计算出所有K个 yₖ 的值，把这K个值通过一个Softmax函数进行转换。Softmax函数能把这K个任意大小的值，变成K个加起来恰好等于1的概率值。能一次性地、没有歧义地处理所有类别。

