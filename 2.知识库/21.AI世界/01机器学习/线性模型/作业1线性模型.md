# 一.理论题
## 1.什么是机器学习
机器学习是构建数学模型，使用算法，利用数据来优化这个模型的参数，最终让模型具备从数据中发现规律并对未知数据做出预测或决策的能力。

## 2.监督学习和非监督学习
监督学习有标签，可以告诉机器“学习是否正确”，以正确答案来修正机器学习的规律，
非监督学习无标签， 只能让机器自己学习规律，无外部指导。
##  3.描述线性回归模型的数学形式。线性回归模型是如何进行预测的？

$$h_{\theta}(x)=\theta^Tx$$
输出$h_{\theta}(x)$就是预测值。
输出等于系数乘输入加偏置项，其中输入是样本的特征值，系数就是每个特征的权重，   这样的话输入和输出是一个线性关系。
在训练阶段，我们通过梯度下降算法找到最优参数$\theta$ 
于预测阶段，用全新输入，代入公式，所得到的就是预测值
## 4.定义线性回归的损失函数。为什么我们选择这种损失函数？

均方误差损失函数
$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \Big( h_{\theta}(x^{(i)}) - y^{(i)} \Big)^2 $$
- 由于平方，无论正确答案对于预测值是偏大还是偏小，都能展现出俩者差距
- 较大的误差会给予更严厉的惩罚
- 代价函数是凸函数，保证梯度下降算法能找到全局最优解
## 5.逻辑回归通常用于分类任务。解释为什么逻辑回归能够用于分类，并描述它是如何做到的
首先，和线性回归一样，通过线性函数处理输入，把输出当作综合得分。
然后，通过非线性的sigmoid函数，将输入数据映射到0与1之间，其映射数值可以看作样本属于"y=1"的概率
最后我们以概率0.5作为决策阈值，定义0到0.5为一类，0.5到1为一类，进行二分类。

## 6.有哪些策略可以将二类分类器组合构造为多类分类器？这些策略存在什么问题？各策略的计算复杂度是多少？除课堂讲解过的组合策略，是否存在其他组合策略？

策略：
- one-versus-rest：将多分类问题拆解成多个独立的二分类问题，比如有三个类别，第一个分类器训练“c1和其他”的分类。面对新输入，只需三个分类器同时进行打分，选择最高的那个
- one-versus-one: 在所有可能的类别配对之间，都训练一个二分类器，面对新输入，我们让所有分类器进行他们自己的二分类， 统计得到的结果，“票数”最多的是最终分类。
问题：
- ovr中，线性分类器会围出一个中间区域，这个区域中分类器都给出“不是”的判断，无法分类
- ovo中，仍出现中间区域，投票平局
复杂度
- ovr：复杂度 k 
- ovo： 复杂度 K(k-1)/2
新策略：
构建统一的多类分类器，对于一个输入 x，我们计算出所有K个 yₖ 的值，把这K个值通过一个Softmax函数进行转换。Softmax函数能把这K个任意大小的值，变成K个加起来恰好等于1的概率值。能一次性地、没有歧义地处理所有类别。
## 7.什么是鉴别函数？用于分类的线性鉴别函数有什么优点与缺点？
鉴别函数接收一个输入样本，然后通过公式，决定该样本属于哪一个类别。
线性鉴别函数，是用直线或平面分割数据进行分类。
- 优点：可解释性强，模型简单不容易过拟合
- 缺点：表达能力有限，无法解决非线性问题

## 8. 线性回归模型的误差来源有哪些？在实际构建模型分别应当采用什么策略来减小相应的误差？

误差来源：
- 偏差：“偏差”衡量模型最优情况下的预测值和真实值的差距。欠拟合就指偏差误差大
- 方差：指因模型对训练数据微小波动过于敏感带来的误差，衡量新数据时模型预测结果发生的改变幅度。高方差即过拟合，拟合训练数据太完美以至于拟合了噪声
策略：
- 对于欠拟合，在输入角度上增加更多特征，在模型角度上使用多项式特征，让直线模型能拟合曲线
- 对于过拟合，在输入角度上让数据量更多，特征数量减小，在模型角度上，在代价函数里增加惩罚项，即正则化
## 9. 什么是过拟合
由于训练数据太少，或者噪音太明显， 模型学习到的规律完全贴合已有的训练数据，而面对新数据时却表现很差。

## 10.总结线性模型相关的英文术语
- **Linear Model** 线性模型
- **Bias** 偏置
- **Loss Function** 损失函数
- **Mean SquaredError (MSE)** 均方误差
- **Gradient Descent** 梯度下降
- **Logistic Regression** 逻辑回归
- **Binary Classification** 二分类
- **Decision Boundary**: 决策边界 
- **Decision Surface**: 决策面 
- **Sigmoid Function** Sigmoid函数
- **Maximum Likelihood Estimation (MLE)**: 最大似然估计
- **Regularization**: 正则化

# 二,线性回归
令线性回归模型为
$$ y = \mathbf{w}^T \mathbf{x} + b $$
## 1. 给定损失函数为均方误差，推导出关于权重w的损失函数。

$$ J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \Big( (\mathbf{w}^T \mathbf{x}^{(i)} + b) - y^{(i)} \Big)^2 $$
## 2. 使用梯度下降法，推导出更新权重w和偏差b的公式。
  即基于损失函数，分别对w、b求偏导
     $$ w_{new} = w_{old} - \alpha ×\frac{1}{m}\sum_{i=1}^m\big((w_{old}^Tx^i+b) -y^i\big)x^i$$
     $$ b_{new} = b_{old}-\alpha ×\frac{1}{m}\sum_{i=1}^m\big((w^Tx^i+b_{old}) -y^i\big)$$
## 3.假设采用L2正则化项，给出加入正则化后的损失函数，并推导出权重更新公式。
正则化后的损失函数：
$$J(w,b) = \frac{1}{2m}\sum_{i=1}^{m} \Big( (\mathbf{w}^T \mathbf{x}^{(i)} + b) - y^{(i)} \Big)^2 + \frac{\lambda}{2m}\sum_{j=1}^{n}w_{j}^2$$
w:
$$ w_{new} = w_{old} - \alpha ×\big(\frac{1}{m}\sum_{i=1}^m\big((w_{old}^Tx^i+b) -y^i\big)x^i + \frac{\lambda}{m}w_{old}\big)$$
其中，最容易错的是$\sum_{j=1}^{n}w_{j}^2$ 对w的求偏导，我们不应该写为$\sum_{j=1}^{n}2w_{j}$ ，每个函数的梯度，**本身就是一个向量**。这个向量的每一个分量，就是该函数对相应变量的**偏导数**，所以应该写为$$ \nabla f(\mathbf{w}) = \begin{bmatrix} 2w_1 \\ 2w_2 \\ \vdots \\ 2w_n \end{bmatrix} $$
即$2w_{old}$

b：惩罚项无b，故保持不变
$$ b_{new} = b_{old}-\alpha ×\frac{1}{m}\sum_{i=1}^m\big((w^Tx^i+b_{old}) -y^i\big)$$


# 三、 感知器 
## 1. 描述感知器的数学模型，并给出它的激活函数

$$ {y} = f({w}^T {x} + b) $$
其中，f函数即为激活函数。
激活函数为阶跃函数
$$
f(z) =
\begin{cases}
+1  & \text{if } z > 0 \\
-1 & \text{if } z < 0
\end{cases}
$$
## 2. 用公式描述感知器的学习规则。当一个样本被错误分类时，权重是如何更新的？
当正确结果是+1错判成-1，w需要往x方向靠近，+x， b增加1
当正确结果是-1错判从+1，w需要往x方向靠近，-x，b减少1
正确值y，刚好能代表增加或减少，在错判的情况下，y是+1刚好w和b就要增加。
y是-1同理。最厚再加上学习率

$$ {w}_{\text{new}} = {w}_{\text{old}} + \alpha y{x} $$
$$ b_{\text{new}} = b_{\text{old}} + \alpha y $$


## 3. 感知器收敛性定理的内容是什么？
如果一个训练数据集是线性可分的 ，那么感知器学习算法保证能够在有限次的迭代之内，找到一个能够将数据完美分开的决策边界。

# 四、逻辑回归
令逻辑回归模型为


$$ h_{\mathbf{w}}(\mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{w}^T \mathbf{x})} $$

## 1. 推导交叉熵损失函数
h(x)可以理解为靠近1的概率。
若标签为1，h(x)即表示此样本正确的概率
若标签为0，1-h(x)表现此样本正确的概率
那么单个样本正确的概率为$h(x)^{y}(1-h(x))^{1-y}$
所以概率正确总概率为$L(w)=Πh(x)^{y}(1-h(x))^{1-y}$
取对数，$log L(w) =\sum({y}log(h(x))+(1-y)log((1-h(x)))$
这是表示正确的程度，不过取对数后，正确程度越高，logL(w)越接近0，正确程度越低，logL(w)越接近负无穷。我们需要最大化这个函数，让他正确程度尽可能高。
梯度下降需要最小化一个大于等于0的损失函数，我们加负号变成J(w)
那么最大化log L(w)就转化成最小化J(w)。
$$ J(\mathbf{w}) = -\frac{1}{m} \log L(\mathbf{w}) $$
最终损失函数为：
$$ J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^{m} \Big[ y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1-y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})) \Big] $$
还有另一种推导方法，见附录图片。

## 2.使用梯度下降法，推导出更新权重w的公式
设$-W^T x$ 为z



logL(w)对h(x)导数
$$ \sum \big( y\frac{1}{h(x)} -(1-y)\frac{1}{1-h(x)}\big) = \sum \frac{y - h(x)}{h(x)(1-h(x))}$$

sigmoid函数即h(x)，对z导数
$$\frac{exp(-x)}{(1+exp(-x))^2} = h(x)(1-h(x))$$

z对w导数
$$-x$$

则我们得到的单个样本的对数似然梯度是：

$$ \frac{\partial \log L}{\partial \mathbf{w}} = {\left( \frac{y - h(x)}{h(x)(1-h(x))} \right)} \cdot {\left( h(x)(1-h(x)) \right)}\cdot {\left( {x} \right)} = (y - h(x)) {x}$$

最终w：


$$\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) \mathbf{x}^{(i)}  $$

## 3.当引入L1正则化项时，写出正则化后的损失函数
 $$J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^{m} \Big[ y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1-y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)}))+\frac{\lambda}{m}  \sum_{j=1}^{n} |w_j|\Big] $$
## 4.试给出线性模型参数优化的贝叶斯解释
贝叶斯解释中，模型参数看作随机变量，而不是一个即将被确定的最优的值。
我们要求的，最优模型参数，出现在`p(w|D)` 中。
`p(w|D)`是一个概率分布，记录每一个w当“最优参数”的概率。
根据贝叶斯定理，$$ P(\mathbf{w} | D) = \frac{P(D | \mathbf{w}) \cdot P(\mathbf{w})}{P(D)} $$
此时视角收敛到离散的点。P都代表概率而不是概率分布。
其中，分子由两部分组成。
`P(D|w)`，理解为参数为w时，数据为D的概率，我们称为“**似然**”。
似然这个命名是与“概率”对应的。概率在这里，指“已知参数为w，求对应数据为D的可能性”。似然对应过来，“已知得到的数据为D，求参数是w的概率”。
我们还要对模型参数有一个“先验概率”，比如“我们猜测w大概率是两位数”。 我们称`p(w)`
先验概率看作最直觉地猜测， 似然看作印证我们猜测的证据，两者结合代表我们的“信念分数”。贝叶斯解释称为“后验概率”。
分母是归一化用的，我们要把分子的数转化为0-1的概率。在这里没有对参数优化的讨论意义。
那么参数优化，
- 等价于最大化` [ log(P(D | w)) + log(P(w)) ]`
- 等价于  最小化 `[ -log(P(D | w)) - log(P(w)) ]`
关键又在于，L(w)=P(D∣w)
- 故而又等价于最小化`[ -log(L(w)) - log(P(w)) ]`
 log(P(w))这个“先验信念”，我们来做一个合理的假设。如果我们“相信”w 的值不太可能太大，更可能都集中在0附近，一个非常好的数学模型就是假设 w 的每个分量都服从均值为0的高斯分布（正态分布）。经查阅，数学推导有 P(w) ∝ exp(-λ Σ wⱼ²)     log(P(w)) ∝ -λ Σ wⱼ²
 - 故而最终等价于最小化`[ J(w) + L2正则化项 ]`
与上面几问推导一致。


# 五.计算题
![[作业1线性模型-1759929667345.jpeg]]


# 附录

交叉熵损失函数另一种推导
![[作业1线性模型-1759929670485.jpeg]]