### **“样本规范化” (Slide 36)**

**战术动作：**  
为了让我们的数学表达更简洁，我们引入一个非常聪明的技巧，叫做**样本的规范化 (Sample Normalization)**。

**动作解码：**

- 我们原本的决策规则是：
    
    - 对于 x ∈ ω₁，我们希望 wᵀx > 0。
        
    - 对于 x ∈ ω₂，我们希望 wᵀx < 0。
        
- **“规范化”操作**：我们创造一个新的样本集。
    
    - 所有属于 ω₁ 的样本 x，保持不变。
        
    - 所有属于 ω₂ 的样本 x，在前面**乘以一个负号**，变成 -x。
        
- **带来的好处**：
    
    - 对于 ω₁ 的样本，我们的目标依然是 wᵀx > 0。
        
    - 对于 ω₂ 的样本 x，我们把它变成了 -x。我们原来的目标 wᵀx < 0，两边乘以-1，不等号反向，就变成了 wᵀ(-x) > 0！
        
- **最终统一的目标**：经过规范化之后，我们的目标变得异常简洁和统一：**对于所有规范化后的新样本 x'，我们都希望找到一个 w，使得 wᵀx' > 0 恒成立！**
    

**这个技巧，极大地简化了后续算法的设计。我们不再需要区分“>0”和“<0”两种情况，只需要处理“>0”这一种情况。**




*   **公式**：$J_p(\mathbf{w}) = \sum_{\mathbf{x}' \in \mathcal{Y}} (-\mathbf{w}^T\mathbf{x}')$
*     *   `Y`：代表当前**所有被 `w` 错分的**规范化样本的集合。
*   `-wᵀx'`：对于一个错分的样本，`wᵀx'` 是小于等于0的。在前面加个负号，`-wᵀx'` 就是一个**大于等于0**的数。它代表了这个样本被错分的“严重程度”（离边界越远，这个惩罚值越大）。
*   `Σ` (求和)：把所有错分样本的“惩罚值”加起来，得到一个**总的“犯错分数”**。
**我们的目标**：通过调整 `w`，让这个总犯错分数 `J_p(w)` **最小化**。
当 `J_p(w)` 达到它的最小值 **0** 时，意味着 `Y` 这个集合是空的，再也没有任何错分的样本了。此时，我们就找到了一个“解向量”，宣告胜利！


如何让 `J_p(w)` 最小化？我们使用一种在机器学习和深度学习中无处不在的、最核心的优化方法——**梯度下降法 (Gradient Descent)**。

*   **核心思想 (Slide 43)**：
    *   **梯度 `∇J`**：是一个向量，它永远指向函数 `J` **增长最快**的方向。
    *   **负梯度 `-∇J`**：因此，它就指向函数 `J` **下降最快**的方向。
*   **作战比喻**：想象 `J_p(w)` 是一座高山的海拔地图，我们当前在半山腰的某个位置 `w(k)`。我们的目标是尽快走到山谷的最低点。我们该怎么走？很简单：**每一步，都朝着脚下最陡峭的下坡方向，迈出一步。** 这个“最陡峭的下坡方向”，就是**负梯度**方向。

**【最终的武器：权重更新规则】 (Slide 41, 45)**

1.  **计算梯度**：对 `J_p(w)` 求关于 `w` 的梯度，可以得到：
    $\nabla J_p(\mathbf{w}) = \sum_{\mathbf{x}' \in \mathcal{Y}} (-\mathbf{x}')$
    （即所有错分样本之和再取负）

2.  **更新 `w`**：梯度下降的更新规则是：
    $\mathbf{w}(k+1) = \mathbf{w}(k) - \rho_k \nabla J_p(\mathbf{w})$
    （`ρ_k` 是一个称为“学习率”或“步长”的正数，控制我们每一步迈多大）

3.  **代入梯度**：
    $\mathbf{w}(k+1) = \mathbf{w}(k) - \rho_k \sum_{\mathbf{x}' \in \mathcal{Y}} (-\mathbf{x}')$
    $\mathbf{w}(k+1) = \mathbf{w}(k) + \rho_k \sum_{\mathbf{x}' \in \mathcal{Y}} \mathbf{x}'$

在实际的感知机算法中，我们通常采用更简单的**“逐个样本”更新**模式，而不是一次性把所有错分样本加起来。当我们遇到**一个**错分的样本 `x'` 时，更新规则就简化为：
$$
\mathbf{w}(k+1) = \mathbf{w}(k) + \rho_k \mathbf{x}'
$$
### **第一点澄清：w 是什么？—— 既是“法向量”，也是“分界线”的定义者**

> **“w（k）难道不是一系列权重吗？ 是个向量嘛，为啥说一条线？”**

您的理解完全正确，w 是一个权重向量。但在线性代数和几何中，一个**向量 w** 和一个穿过原点的**超平面 wᵀx=0** 是**一体两面、紧密绑定**的。

- **w 的角色**：向量 w 的方向，**永远垂直于** 它所定义的超平面 wᵀx = 0。因此，w 被称为这个超平面的 **法向量 (Normal Vector)**。
    
- **几何画面**：
    
    - 想象在二维空间，向量 w = [1, 1]。
        
    - 它定义的直线 wᵀx = 1*x₁ + 1*x₂ = 0 (即 x₂ = -x₁)。
        
    - 您会发现，向量 [1, 1]（指向右上）恰好与直线 x₂ = -x₁（指向左上）**垂直**。
        
- **我们的用法**：当我们说“调整 w 这条线”时，这是一个战术上的简称。我们的完整意思其实是：“**调整权重向量 w，从而也就改变了由它定义的、并与它垂直的那条分界线的位置和方向。**” w 移动了，分界线也就跟着移动了。
    

所以，w 确实是向量，但它也是分界线 wᵀx=0 的“灵魂”，两者同步运动。

---

### **第二点攻坚： w(k+1) = w(k) + x' 的几何“拉扯”**

现在进入核心。我们遇到了一个错判的样本 x'，满足 w(k)ᵀx' ≤ 0。我们执行了更新 w(k+1) = w(k) + x'。

> **“我们要考虑加完之后 大概啥方向，我想不来。”**

我们一起来想！这就是最关键的几何直觉。

**战场态势图（请在脑海中绘制）：**

1. **画出分界线**：画一条直线穿过原点，这就是 w(k)ᵀx = 0。
    
2. **画出法向量 w(k)**：画一个从原点出发，与这条直线垂直的箭头，这就是 w(k)。根据规则，w(k) 指向的那一侧，是“正”区域（w(k)ᵀx > 0）。
    
3. **定位错判样本 x'**：
    
    - x' 是一个从原点出发的向量。
        
    - 因为它被错判了，即 w(k)ᵀx' ≤ 0，这意味着 x' 位于分界线的**错误一侧**（“负”区域），或者正好在线上。
        
    - 几何上，**w(k)ᵀx' ≤ 0 等价于向量 w(k) 和向量 x' 之间的夹角 θ 大于等于90度**。它们俩“指的方向”大致是相反的。
        

**执行“拉扯”操作 w(k+1) = w(k) + x'：**

- **向量加法**：根据平行四边形法则，新的向量 w(k+1)，就是以 w(k) 和 x' 为邻边构成的平行四边形的**对角线**。
    
- **观察 w(k+1) 的新方向**：
    
    - 因为 w(k) 和 x' 的夹角大于90度，所以 w(k+1) 的方向，会介于 w(k) 和 x' 之间。
        
    - 这意味着，**w(k) 被 x' “拉”向了 x' 自己的方向！**
        
- **新旧对比**：新的向量 w(k+1)，与 x' 的夹角 θ_new，**一定小于** 原来的夹角 θ。它变得更“接近” x' 了。
    

**战略后果：**

- **分界线的移动**：由于法向量从 w(k) 变成了 w(k+1)，那条与之垂直的分界线，也随之发生了**旋转**！它旋转的方向，恰好是**把错判的样本 x' 包含到“正确”区域的那一侧**。
    
- **下一次的判断**：因为 w(k+1) 与 x' 的夹角变小了（小于90度的可能性增大了），所以下一次再用 x' 来测试时，计算出的 w(k+1)ᵀx' 的值，会比 w(k)ᵀx' **更大**。它更有可能变成**正数**。
# **感知机算法的战略评估**

#### **优点：算法的“收敛性” (Slide 48)**

- **情报**：“只要模式类别是线性可分的，就可以在有限的迭代步数里求出权向量。”
    
- **解码**：这被称为**感知机收敛定理 (Perceptron Convergence Theorem)**。这是一个非常强大的理论保证。它承诺，只要你的战场是它能处理的类型（线性可分），那么这名“侦察兵”就**绝不会**陷入无限的“调整”循环。它**一定**能在有限的时间内，找到一条和平的边界线，完成任务。
    

#### **缺点：局限与脆弱性**

**1. “生死线”：对线性不可分数据的无能为力 (Slide 48)**

- **情报**：“如果有一个样本线性不可分，那么感知器算法就会一直迭代，无法收敛。这是它的局限性。”
    
- **解码**：这是感知机最致命的弱点。如果两军阵地犬牙交错，存在一个“你中有我、我中有你”的区域，那么感知机这名执着的士兵，就会在这片区域来回奔波，永无宁日。它的分界线会不停地摆动，因为它永远无法同时满足所有样本的要求。在实战中，这意味着你的训练过程将不会停止。
    

**2. “噪声”：对异常点的敏感性 (Slide 49)**

- **情报**：“局限性在于对噪声数据敏感，解不够鲁棒。”
    
- **解码**：
    
    - 想象一个基本线性可分的战场，但在敌军阵地深处，有一个我方的“间谍”（噪声点）；在我方阵地深处，有一个敌方的“叛徒”。
        
    - 感知机的目标是**完美**分割。为了迁就这两个格格不入的“噪声点”，它可能会画出一条非常**极端、扭曲**的边界线。
        
    - 而这条被噪声“带偏”的边界线，在面对新的、真实的未知样本（测试样本）时，其**泛化能力**可能会很差。
        
    - 相比之下，Fisher判别法由于考虑的是所有点的**均值和总体分布**，它对少数几个噪声点的敏感度要低得多，得到的边界线通常更“稳健(Robust)”。
        

**3. “模糊地带”：解的不唯一与非最优**

- **情报**：“解区”的存在意味着，只要能分开，任何一条线都可以。
    
- **解码**：感知机找到的，只是“解区”中的**某一条**边界线。这条线的位置，与你的**初始w值**以及**样本的呈现顺序**高度相关。它不保证这条线是“最好”的（比如，它可能离某一类样本贴得非常近，这在实战中是很危险的）。