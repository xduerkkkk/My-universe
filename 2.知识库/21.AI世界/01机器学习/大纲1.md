

# **第0章：AI基石与通用方法论

## **模块一：机器学习世界观 **

- **1. 机器学习的三大范式**:  （未完成）
    
    - **监督学习**: 从“标签”中学习映射 f(x) -> y。
        
    - **非监督学习**: 从“无标签”数据中发现内在结构。
        
    - **强化学习**: 通过与环境“交互”和“试错”来学习最优策略。
        
    - 作用：这是整个AI知识星图最高层的“星图索引”。
        
- **2. 模型的“好”与“坏”：偏差-方差权衡 ** （完成）
    
    - **偏差 (Bias)**: 模型的“偏见”，描述了模型预测值与真实值之间的差距（欠拟合的根源）。
        
    - **方差 (Variance)**: 模型的“敏感度”，描述了模型对于训练数据微小变化的敏感程度（过拟合的根源）。
        
    - **权衡**: 理解模型复杂性与偏差、方差之间的“U型”关系，这是理解所有正则化方法和模型选择的“第一性原理”。
        

## 模块二：模型评估 （完成）

- **3. 训练、验证与测试集**:
    
    - **训练集**: 模型的“教科书”，用于学习参数。
        
    - **验证集**: 模型的“模拟考”，用于**调整超参数**和**模型选择**（比如，我应该用SVM还是随机森林？随机森林的树应该多深？正则化系数λ该取多少？）。
        
    - **测试集**: 模型的“最终高考”，在整个项目结束前**只能用一次**，用于客观评估最终模型的泛化能力。
        
- **4. 交叉验证 (Cross-Validation)**:
    
    - **动机**: 当数据集很小时，简单的“验证集”划分可能带来偶然性。
        
    - **机制**: K折交叉验证，将数据分成K份，轮流做验证集，结果取平均。
        
- **5. 核心评估指标 (Metrics)**:
    
    - **分类**: 准确率、混淆矩阵、**精确率、召回率、F1分数**、ROC曲线、AUC面积。
        
    - **回归**: MSE, RMSE, MAE, R-squared。
        

## 模块三：模型优化

- **6. 损失函数 (Loss Functions)**: （未完成）
    
    - **角色**: 优化的“目标地图”。
        
    - **家族**: 均方误差 (MSE)、交叉熵 (Cross-Entropy)、Hinge Loss (用于SVM) 等。
        
- **7. 梯度下降家族 (Gradient Descent Family)**: （完成）
    
    - **核心思想**: 沿着梯度的反方向，迭代地走向损失函数的谷底。
        
    - **核心要素**: 学习率 (Learning Rate) 的意义与选择。
        
    - **三种模式**: Batch, SGD, Mini-batch 的区别、优缺点与“平均梯度”的实现。
        
    - **进阶**: (可选) 动量(Momentum)、Adam等更高级的优化器思想。
        

## 模块四：模型通用

- ### **8. 正则化 (Regularization)**:（未完成）
    
    - **核心思想**: 通过在损失函数中添加“惩罚项”来限制模型复杂度，从而防止过拟合。
        
    - **两大主角**:
        
        - **L2正则化 (Ridge / 权重衰减)**: 惩罚权重的平方和，使权重分布更平滑。
            
        - **L1正则化 (Lasso)**: 惩罚权重的绝对值和，能产生稀疏解，可用于特征选择。
            
    - **其他方法**: **早停 (Early Stopping)**，Dropout (常用于深度学习)
- ### **9. 数据预处理 (Data Preprocessing)**:（未完成）
    
    - **处理缺失值**: 删除、填充（均值、中位数、众数、模型预测填充）。
        
    - **处理类别特征**:
        
        - **标签编码 (Label Encoding)**: 将['cat', 'dog', 'fish']变成[0, 1, 2]。适用于有序类别。
            
        - **独热编码 (One-Hot Encoding)**: 将['cat', 'dog']变成[[1,0], [0,1]]。适用于无序类别，这是最重要的类别特征处理方式。
            
    - **特征缩放 (Feature Scaling)**:
        
        - **动机**: 解决不同特征量纲差异过大的问题（比如“房间数”和“房屋面积”），这对梯度下降和SVM等对距离敏感的模型至关重要。
            
        - **方法**: **标准化 (Standardization)** (Z-Score归一化) 和 **归一化 (Normalization)** (Min-Max缩放)。
  
- ### **10. 特征工程 (Feature Engineering)**:（完成） 
    
    - **特征创造**: 从现有数据中构造新的、信息量更大的特征。
        
        - **多项式特征**: x1, x2 -> x1^2, x2^2, x1*x2 (我们讨论过的map_feature)。
            
        - **业务逻辑特征**: 结合具体业务知识创造特征（如电商中计算“最近7天购买频率”）。
            
    - **特征选择 (Feature Selection)**:
        
        - **动机**: 减少特征数量，降低过拟合风险，提升模型效率和可解释性。
            
        - **三大流派**:
            
            - **过滤式 (Filter Methods)**: 独立于模型，根据特征本身的统计属性（如方差、相关系数、卡方检验）进行筛选。速度快。
                
            - **包裹式 (Wrapper Methods)**: 用最终要使用的模型作为“评估器”，通过反复训练来寻找最佳特征子集（如递归特征消除 RFE）。效果好，但计算量大。
                
            - **嵌入式 (Embedded Methods)**: 将特征选择**嵌入**到模型训练过程中。最经典的例子就是 **Lasso回归 (L1正则化)**，它在训练时会自动将不重要的特征权重压缩为0。

# **第一章 线性世界 **



## 1 **感知准则函数** （完成）



        
## **2. 线性回归 (Linear Regression)**  （完成）
    
- **定位**: 回归任务的“Hello World”。
	
- **核心原理**: 最小二乘法。
	
- **损失函数**: 均方误差 (MSE)。
	
- **优化**: 梯度下降法 vs. 正规方程解。
	
- **扩展**:
	
	- **多项式回归**: 引入非线性的第一次尝试 (map_feature)。
		
	- **正则化**: 解决过拟合问题，引出**岭回归 (Ridge, L2正则化)** 和 **Lasso回归 (L1正则化)**，并理解Lasso的“特征选择”特性。
            
## **3. 逻辑回归 (Logistic Regression)**  （完成）
    
- **定位**: 分类任务的“Hello World”，也是通往神经网络的桥梁。
	
- **核心原理**: 将线性回归的输出通过Sigmoid函数映射到(0,1)区间，解释为概率。
	
- **损失函数**: **交叉熵 (Cross-Entropy)**，并理解为什么它比MSE更适合分类任务。
	
- **决策边界**: 深入理解 w*x+b=0 这条直线。

## 4 Fisher线性判别  （完成）



# 第二章：非线性世界



## **4. 几何大师 —— 支持向量机 (SVM)**  （未完成）
    
- **核心思想**: 从“正确划分”到“鲁棒划分”，追求**最大化几何间隔**。
	
- **核心概念**: 支持向量、间隔、软间隔（允许犯错）。
	
- **灵魂**: **核技巧 (Kernel Trick)**。
	
	- 理解其动机：避免“维度灾难”，隐式地在高维空间中进行计算。
		
	- 掌握常用核函数，特别是**高斯核 (RBF Kernel)**，并理解它如何能创造出无限维的特征空间和任意复杂的决策边界。
		
- **扩展**: 了解SVR（支持向量回归）。
        
## **5. 流派二：分而治之 —— 树模型 (Tree-based Models)**
    
- **5.1 基础单元：决策树 (Decision Tree)**  （完成）
	
	- **核心思想**: 递归地、贪心地选择最优特征进行分裂，将复杂问题分解。
		
	- **分裂标准**: 信息熵、**信息增益**、**基尼不纯度**（用于分类）；**均方差**（用于回归）。
		
	- **弱点与对策**: 容易过拟合，需要**剪枝 (Pruning)**。
		
- **5.2 集成智慧：从“三个臭皮匠”到“算法之王”**  （未完成）
	
	- **核心思想**: “群众的智慧”——将多个弱学习器组合成强学习器。
		
	- **分支A (并行派): Bagging -> 随机森林 (Random Forest)**
		
		- **机制**: 通过自助采样和特征随机，构建多个“多样化”的决策树，最后**投票/取平均**。
			
		- **效果**: 降低模型的方差，鲁棒性强，不易过拟合。
			
	- **分支B (串行派): Boosting -> GBDT -> XGBoost/LightGBM**
		
		- **机制**: 序贯地训练模型，每个新模型都专注于修正前面模型的**残差 (residuals)**。
			
		- **效果**: 降低模型的偏差，精度上限高，但对噪声敏感，更容易过拟合。

## **5.3 流派三：概率视角 —— 图模型**  （未完成）
    
- 贝叶斯网络 (朴素贝叶斯)，马尔可夫模型 (HMM)。

## **5.4 流派四：实例驱动 —— k近邻 (k-NN)** （完成）

# **第三章：数据的天然结构 **

## 6. 非监督学习 (Unsupervised Learning)
    
- **任务一：物以类聚 —— 聚类 (Clustering)**  （完成）
	
	- **代表算法**: **K-均值 (K-Means)**。  **高斯混合模型 (GMM)**     **层次聚类** 
		
	- **学习重点**: "分配-更新"的迭代流程，初始值敏感性，如何选择K值。
		
- **任务二：去粗取精 —— 降维 (Dimensionality Reduction)**  （未完成）
	
	- **代表算法**: **主成分分析 (PCA)**。
		
	- **核心思想**: 寻找方差最大的方向（主成分）进行投影，以最少的信息损失压缩数据。
		
	- **数学内核**: 理解其与**协方差矩阵特征值分解**的深刻联系。