[[2.知识库/21.AI世界/02深度学习/激活函数|激活函数]]
### ReLU函数


给定元素$x$，ReLU函数被定义为该元素与$0$的最大值：


**$$\operatorname{ReLU}(x) = \max(x, 0).$$**


### sigmoid函数


**对于一个定义域在$\mathbb{R}$中的输入，

*sigmoid函数*将输入变换为区间(0, 1)上的输出**]。

因此，sigmoid通常称为*挤压函数*（squashing function）：

它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：

  

(**$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$**)

  

在最早的神经网络中，科学家们感兴趣的是对“激发”或“不激发”的生物神经元进行建模。

因此，这一领域的先驱可以一直追溯到人工神经元的发明者麦卡洛克和皮茨，他们专注于阈值单元。

阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。

  

当人们逐渐关注到到基于梯度的学习时，

sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。

当我们想要将输出视作二元分类问题的概率时，

sigmoid仍然被广泛用作输出单元上的激活函数

（sigmoid可以视为softmax的特例）。

然而，sigmoid在隐藏层中已经较少使用，

它在大部分时候被更简单、更容易训练的ReLU所取代。