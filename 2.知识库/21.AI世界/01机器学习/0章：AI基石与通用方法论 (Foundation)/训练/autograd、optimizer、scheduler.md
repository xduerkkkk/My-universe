
# 回顾梯度，反向传播

初高中都学过导数，但实际上，这是在一元函数中，y=f(x) 沿x轴正方向的变化率。
那么我们进阶一点，偏导数呢？ 其实就是函数沿坐标轴正方向的变化率
诶，可以说导数是偏导数家族的一份子。是一元函数情况下的偏导数。
方向导数呢？  刚才在讲沿坐标轴，方向导数的范围就更大了， 是某一点在某一趋近方向的导值。是一个向量 
总而言之，这些导数，都叫做变化率对吧！
那一个点，放在一个函数里， 有无数种方向的可能吧！ 那么，他就有无数个方向导数！
而这其中，变化率最大的方向导数，就叫梯度！  
刚才讲的导数、偏导数、方向导数，都有方向有大小，所以， 他们都是向量！
那么梯度，就也是向量！ 向量的模，为梯度的值。

现在回到我们的机器学习，为什么，我们说，模型权重不符合我们想要的，那我们就用正确答案计算的梯度，来更新权重？
什么叫正确答案带来的梯度？
其实，具体来说应该是损失函数带来的梯度？。
我们应用的情景，是正确答案和错误答案带来的损失，， 我们想让损失最小，， 间接地该别了模型参数。

我们目的，是我们的参数，能让损失达到最小甚至0.
我们的输入是固定的，输出的正确答案是固定的，需要调整的就是这个处理输入的参数。
loss = 正确答案（固定的） - 输出答案（由参数影响的）  哦，也就是说，loss里面有个变量！即参数！ 我们定义为w吧。    那这个就可以用最简单的高中数学理解了，  参数影响着loss，参数可能让loss成为任何值，而我们呢，只想让参数让loss成为0！我们可以对这个loss式子，求关于w的导数，
极值的时候 绝对是导数为0的！ 至于说，导数绝对值大就离极值远，就把这个解释抛给数学吧，反正图就是这样画的，我们画平滑的曲线，临近极值的时候导数一定是越来越小直至为0的



### 1. Autograd 机制 (自动求导)

每一个张量，都有grad（梯度），grad_fn（路标）
一创建就有?
当然不是！ 如果一个张量是独立的， 哪来“梯度”的说法。
我们最终都是为了算，该张量如何影响loss的，算这个梯度的，所以怎么滴也要跟其他张量连接在一起。 
那么grad_fn 应该就是记录了张量的联系，  假设我们手动创建出张量A，loss直接与张量B挂钩，但是张量B是基于张量A生成的，那B的gradfn，应该要记录到A，是用操作记录的（我是xx用乘法/加法...得来的） 。 到最后更新的时候，更新谁呢？    其实我们要研究的是A对loss的影响，但我们不能一步就为，有可能还有A->C->loss 这个路线呢， 所以我们把中间过程的梯度也计算上，记住这些中间过程的gradfn，找到A,  继续计算，然后累计梯度，算出最终的A如何影响loss。  那有没有可能连续经过俩个没有gradfn的节点影响loss呢？ 不可能， 想想树， 假设这就是树的图， 只有**叶子节点A（requires_grad=True的始祖们）才是我们最终关心的、影响**树根loss**的原因。而所有的**中间节点（枝干）**，它们存在的唯一意义，就是通过它们的.grad_fn（连接关系），构建起一条条从**树根loss**回溯到**叶子A**的路径，让梯度能够顺利地流淌回去。”

还有一点，autograd的性质，叫做“动态创建图”！ 怎么理解，就是， 现实在训练时，我们不是训练一次， 计算一次loss，而是要训练很多次。  forward很多次。 我们想想 ，基于直觉，那就图的结构不变吧，只是更新其中的grad就可以了啊， 关系又不变。
但是！ 如果训练模型比较特殊怎么办？一个for循环，每次输入的数据长度都不一样，那按我们刚才的想法（称作静态图吧） 那不就每次弄一个新的图。  pytorch的特性叫动态图， 他像python一样，一边运行一边构造，每次循环都是新的图。 咦，突然疑惑了，那静态图只要每次编译运行完，再废弃掉，下一个循环继续构造新的静态图，那不就是pytorch的动态图吗？
不对，静态图，是像c++一样，预先完全编译好，全部构造好，再开始运行的。

总之，我们就是计算loss， 
然后loss.backward()！ 这下所有刚才说的中间变量，都收到通知，哦，我要被计算梯度了，是关于loss的！  所以说虽然我们没调用其他任何参数，只调用了loss.backward() 但由于autograd默默的把所有中间变量的关系都记录着，所以牵一发而动全身， 一层一层的中间变量都在计算梯度， 最后把梯度累积到了可学习参数上。
此时我们无论是打印可学习参数还是中间参数的.grad 都能看到数值？
- **默认情况下，只有叶子节点（Leaf Nodes）的.grad才会被保留。**
    
- PyTorch为了节省内存，在反向传播计算完成后，所有**中间节点（Internal Nodes）**的梯度**会立即被释放掉**
那么梯度准备好了， 下面就是用optimizer，基于这些梯度，更新可学习参数了。 


 最后，我们使用Optimizer（比如Adam）的.step()方法，它会根据每个参数的.grad值，按照梯度下降的原理来更新参数。同时，在下一次迭代前，我们需要用optimizer.zero_grad()来清空之前的梯度。


---

### 2. Optimizer 更新公式 (Adam / AdamW)
传统的**梯度下降更新公式**公式，就是新参数 = 旧参数 - 学习率 × 损失函数关于参数的梯度    这个梯度有大小有方向（方向怎么在代码中体现的啊？ ）  
简单，内存占用小，但对学习率的调整要求高， 
还有就是，完全依赖于梯度。
那，为了避免梯度的突变带来的不稳定，
我们可以加上一个“动量”，我觉得这在物理上比喻也可以说惯性吧？ 我们不是学习率成纯粹的梯度，关于这个梯度，我们还要再做调整。 很简单，当前的梯度跟旧的梯度方向差不多， 那我们旧加快点， 因为现在可能离收敛还早， 如果方向突然改变，那说明出问题了，不管是说明问题， 我们现在都得精细地调整了！于是我们让梯度小一点。  这个动量，或者说惯性，完美实现了我们刚讲的调整策略。 落实到数学上，我也不太清楚具体为什么要设计成那样的公式，反正公式里面一定有旧梯度，有目前梯度，有参数进行调节。
除此之外，学习率到底怎么去调整呢？ 
梯度大的参数，减小学习率，稍微精细一点，梯度小的参数，增大学习率，稍微大胆一点。这就是自适应学习率。
方向一致的话，大胆走。方向改变，小心走。 迈的步子长，小心走，迈的步子短，大胆走。
Adam就是把上述两个过程结合在一起。

那么还有一个点，就是L2正则化。
防止过拟合，怎么理解？过拟合是死记硬背对吧，死记硬背是怎么来的？是因为模型的某些权重过大，这样的话权重就不均匀不精细，大权重的知识太深刻了，就死记硬背了嘛。
那我们就变成了防止权重过大。L2正则化的意思就是，损失函数 = 损失+系数成权重的平方
这个稀疏就是惩罚力度。
在adam中，


---

### 3. Scheduler (学习率调度器)

