# 手工搓小工坊

# 使用huggingface生态

我想总结一下，如果用huggingface模式，我们的训练流程。
## 数据处理
首先，在线上的数据集，可以通过Dataset库直接load，直接是dataset的数据类型。
该数据类型，可以使用索引访问内容，还有一个特性，就是可以使用.map()方法，去改写数据。
如果该dataset，直接就是符合我们要求的json，那就好了，
我们可以直接传入Trainer这个工具中的dataset参数。本来我们手工建造数据集的话，还要padding，还要实现batchsize，也就是我们要写一个dataloader。 但是Trainer我们不用写，我们传入datacollator即可， 它回实现“批量处理”，“动态padding（根据当前batch的最长长度）”。
总之，如果加载的dataset符合要求，我们可以直接用了。
那如果不符合呢？
有可能，dataset中的源数据，是人家作者自己的一套模板。
我们期望的数据模板不符。
那我们就要修改了， 
整体当然就是 dataset--》jsonl--》newdataset（符合我们需求的）
首先dataset怎么变成jsonl？当然是使用索引，用一个for循环把他们全处理了。
然后把有用的信息填到符合我们需求的模板上来，json.dump()保存一下
jsonl又怎么重装成dataset呢？ 
当然是使用Dataset.from_json
咦？有没有突然疑惑，既然jsonl就是我们想要的格式，为什么我们不直接用jsonl训练，而是要包装成dataset呢？
当然，首先想用huggingface就必须用dataset传数据集，其次，huggingface这么做是有原因的。 dataset类的加载是惰性加载， 不会一次性读取完， 然后最重要的是使用map函数，能统一改变每一个索引的数据。方便我们修改。

### dataset数据到底长啥样

attentionmask和label作用不一样！
attentionmask是attention阶段，label是计算lofits阶段发挥作用

| 模型类型                 | **input_ids** | **attention_mask**         | **labels**    | **其他关键输入**                     |
| -------------------- | ------------- | -------------------------- | ------------- | ------------------------------ |
| **自回归LLM (SFT)**     | 完整对话          | Padding Mask               | 带-100的完整对话    | pixel_values (如果是VLM)          |
| **BERT (MLM)**       | 带[MASK]的句子    | Padding Mask               | 只在[MASK]位置有ID | 无                              |
| **T5 (Seq2Seq)**     | "损坏"的句子       | Padding Mask (for Encoder) | "完好"的句子       | decoder_input_ids              |
| **ViT (分类)**         | 无             | 无                          | 单个整数类别ID      | pixel_values                   |
| **Stable Diffusion** | 描述文本          | Padding Mask (for Text)    | **真实的噪声张量**   | pixel_values (加噪图片), timesteps |

## 训练
我们的dataset数据应是怎样的呢？  
不是纯纯应用template后的text，至少我们也要借助tokenizer或processor，让text变成向量。纯向量也不够，我们到底要mask掉谁，trainer仍然不知道。 所以我们还得有个
attention_mask
我想不同类型的任务应该都不一样