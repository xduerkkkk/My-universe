# 实验报告：AdaBoost 集成学习算法的实现与性能对比

## 一、 实验目的
1.  基于 `scikit-learn` 库，在两个不同类型的数据集（二分类与多分类）上自行实现 **AdaBoost** 算法。
2.  通过构建 **弱分类器 (Weak Classifier)**、**强分类器 (Strong Classifier)** 与 **组合分类器 (Ensemble Classifier)** 的对比实验，直观验证集成学习“变弱为强”的优势。
3.  分析 AdaBoost 在提升模型准确率和泛化能力方面的作用。

## 二、 实验数据集
为了全面评估算法性能，本次实验选取了两个经典的公开数据集：
1.  **乳腺癌数据集 (Breast Cancer)**：
    *   **类型**：二分类问题 (Binary Classification)。
    *   **特征**：30 个连续数值特征（如半径、纹理等）。
    *   **样本量**：569 个。
    *   **目标**：区分良性与恶性肿瘤。
2.  **红酒数据集 (Wine)**：
    *   **类型**：多分类问题 (Multiclass Classification, 3类)。
    *   **特征**：13 个连续数值特征（如酒精浓度、苹果酸等）。
    *   **样本量**：178 个。
    *   **目标**：根据化学成分区分红酒的品种。

## 三、 实验设计与算法原理
本次实验设计了三组对照模型，以体现组合分类器的优势：

### 3.1 模型设置
1.  **弱分类器 (Baseline)**：
    *   **模型**：单层决策树 (Decision Stump)。
    *   **参数**：`max_depth=1`。
    *   **特点**：结构极简，每次仅基于一个特征的一个阈值进行分类，相当于“只看一眼”，偏差较大，准确率通常有限。
2.  **强分类器 (Benchmark)**：
    *   **模型**：深层决策树 (Decision Tree)。
    *   **参数**：`max_depth=5`。
    *   **特点**：结构复杂，能拟合复杂的非线性关系，但容易过拟合（High Variance）。
3.  **组合分类器 (AdaBoost)**：
    *   **模型**：AdaBoostClassifier。
    *   **参数**：`n_estimators=50` (集成 50 个弱分类器)。
    *   **原理**：利用前向分布算法，每轮迭代调整样本权重，增加被前一轮错误分类样本的权重。最终通过加权投票的方式，将 50 个弱分类器组合成一个强分类。


## 四、 实验结果

### 4.1 实验一：乳腺癌数据集 (二分类)
该数据集特征线性度较高，分类相对容易。实验结果如下：

| 模型名称         | 模型配置            | 测试集准确率 (Accuracy) |
| :----------- | :-------------- | :---------------- |
| **弱分类器**     | 决策树桩 (Depth=1)  | **0.8947**        |
| **强分类器**     | 深层决策树 (Depth=5) | **0.9352**        |
| **AdaBoost** | 集成 50 个弱分类器     | **0.9708**        |

### 4.2 实验二：红酒数据集 (多分类)
该数据集类别为 3 类，特征分布较复杂，弱分类器表现较差，但 AdaBoost 提升效果惊人。实验结果如下：

| 模型名称 | 模型配置 | 测试集准确率 (Accuracy) |
| :--- | :--- | :--- |
| **弱分类器** | 决策树桩 (Depth=1) | **0.6111** |
| **强分类器** | 深层决策树 (Depth=5) | **0.9630** |
| **AdaBoost** | 集成 50 个弱分类器 | **0.9815** |

---

## 五、 结果分析

通过对比两组实验数据，我们可以清晰地观察到组合分类器的优势：

### 5.1 “变弱为强”的显著效果
在**红酒数据集**的实验中，这一特点尤为突出：
*   **弱分类器的局限性**：单层决策树（弱分类器）仅达到了 **61.11%** 的准确率。由于红酒数据包含 3 个类别，且特征间存在非线性关系，仅靠一次“水平切分”（单层树的原理）完全无法有效区分数据，表现出极高的**偏差 (High Bias)**。
*   **AdaBoost 的提升**：通过集成 50 个这样的弱分类器，AdaBoost 将准确率惊人地提升到了 **98.15%**。这证明了 AdaBoost 能够通过迭代关注那些“难以分类”的样本，强制后续的弱分类器去修补前一轮的错误，从而在整体上构建出一个极其强大的分类模型。

### 5.2 泛化能力的提升
在**乳腺癌数据集**中：
*   虽然单个强分类器（深层决策树）已经能达到很高的准确率（约 93%），但 AdaBoost 依然实现了进一步的提升（约 96%）。
*   **原理分析**：深层决策树容易陷入**过拟合 (Overfitting)**，即对训练集学得太死。而 AdaBoost 虽然集成了很多模型，但每个基模型都非常简单（只有一层），这使得整体模型在保持低偏差的同时，也维持了较好的方差控制，从而在测试集上表现出更好的**泛化能力**。

### 5.3 结论
实验数据有力地证明了 AdaBoost 算法的核心假设：**一组表现略优于随机猜测的弱分类器，通过自适应提升 (Adaptive Boosting) 策略进行加权组合，完全可以媲美甚至超越一个精心设计的强分类器。** 这种方法不仅降低了模型设计的难度（只需要简单的弱分类器），还显著提高了分类性能。


## 六、 实验总结与心得

通过本次在乳腺癌（二分类）和红酒（多分类）两个数据集上的 AdaBoost 实现与对比实验，我得出以下结论与体会：

1.  **集成学习的“群体智慧”**：
    实验中最直观的感受是组合分类器的强大。在红酒数据集中，单个决策树桩只能达到 61% 的准确率，几乎不可用；但 AdaBoost 仅仅通过组合 50 个这样的“差生”，就实现了 98% 的准确率。这生动地诠释了集成学习的核心理念：**三个臭皮匠，顶个诸葛亮**。它证明了只要基分类器的表现略好于随机猜测，AdaBoost 就能通过调整权重将其提升为强分类器。

2.  **偏差与方差的平衡**：
    *   **弱分类器**（如单层决策树）通常具有**高偏差**（欠拟合），无法捕捉复杂特征。
    *   **强分类器**（如深层决策树）通常具有**高方差**（易过拟合），对噪声敏感。
    *   **AdaBoost** 巧妙地结合了二者：它使用高偏差的弱模型作为基石，通过迭代逐步降低整体偏差，最终得到一个既能捕捉复杂模式、泛化能力又强的模型。

3.  **工程实践中的兼容性挑战**：
    在代码实现过程中，我遇到了 `scikit-learn` 库版本更新带来的参数名变更问题（`base_estimator` vs `estimator`）以及云端环境 `matplotlib` 后端不兼容的问题。通过查阅文档、降级库版本以及编写自动适配版本的代码，我成功解决了这些报错。这让我认识到，在实际的机器学习项目中，**环境管理和版本适配**与算法理解同样重要，是保证实验可复现性的关键环节。

综上所述，本次实验不仅验证了 AdaBoost 算法在提升分类精度方面的显著优势，也锻炼了我在数据分析和故障排查方面的实际动手能力。