本质区别是 “软聚类” 一个物体可以属于多个类 硬聚类可以属于一个类  
那概率模型显然 就是多个类的概率 有可能是任何一类 非概率模型是一个类 一个确定的值


|   |   |   |
|---|---|---|
|特征|**概率模型 (Probabilistic Model)**|**非概率模型 (Non-Probabilistic Model)**|
|**输出形式**|P(Y\|X) <br> (给定输入X，输出属于每个类别Y的**概率分布**)|Y = f(X) <br> (给定输入X，输出一个**确定的类别Y**)|
|**聚类结果**|**软聚类 (Soft Clustering)**|**硬聚类 (Hard Clustering)**|
|**结果描述**|一个数据点可以按不同概率<br>同时属于多个聚类簇。|一个数据点只能属于<br>一个聚类簇。|
高斯混合模型（GMM） ： 
⚫ 一种典型的概率模型。假设所有数据点都是由 具有未知参数的有限个高斯分布混合产生的。 ⚫ 可以把高斯混合模型看作是广义的k-均值聚类， 它包含了关于数据协方差结构和隐含的高斯中 心的信息。 
⚫ 作为一种基于概率模型的软聚类算法，高斯混 合模型采用高斯分布作为参数模型，并使用期 望最大化（EM）算法来求解模型参数。


高斯混合模型（GMM）： 
⚫ 高斯混合模型是用于估计样本的概率密度分布 的方法，其估计采用的模型是几个在训练前就 已经建立好的高斯模型的加权和。高斯模型的 数目是一个超参数 ，在模型建立前给定，每个 聚类簇都对应于一个高斯分布。对样本中的数 据给定的几个高斯模型上分别进行投影，就会 得到样本对应在各个聚类簇上的概率。 
⚫ 可以根据实际数据定义任何分布的混合模型， 但是定义高斯分布更有利于计算。理论上，高 斯混合模型可用于近似任何概率分布。


为什么我们需要用“几个”模型的“加权和”，而不是直接用“一个”更复杂、更大的模型来拟合所有数据点呢？这种“混合”的思想，试图解决什么问题？

在这里，GMM采用“混合”思想，有一个更直接、更根本的原因。它源于对数据本身来源的一种**假设**。

让我们看一个简单的例子：假设我们收集了某小学所有学生和老师的身高数据。如果我们把这些数据画出来，它可能看起来像一个不规则的、有两个峰的驼峰形状。

- **用“一个”模型：** 如果我们试图用一个“钟形曲线”（即一个高斯分布）去拟ar这个双峰驼峰，效果会非常差。这个模型会太平、太宽，无法捕捉到“学生一个群体”和“老师一个群体”这个内在结构。
    
- **用“混合”模型：** GMM的做法是，它不认为这些数据来自同一个源头。它会假设：“这些数据其实是由**两个**不同的群体混合而成的，一个群体是身高较矮的（学生），另一个是身高较高的（老师），而且每个群体内部的身高分布都近似一个标准的高斯分布。”
    

因此，GMM会用两个高斯模型：一个去拟合学生的身高分布，另一个去拟合老师的身高分布。然后把这两个模型**“加权和”**在一起（比如，如果学生有500人，老师有50人，那么学生模型的权重就大概是老师的10倍），就能完美地描述整个数据集的分布。

**请看PPT第15页**，那张图完美地展示了这个思想。左边的图试图用**一个**椭圆（代表一个二维高斯分布）去覆盖所有数据点，效果很差。而右边的图用了**两个**椭圆，就很好地描述了数据其实是“两簇”这个事实。

---

**深化 (Deepen):**

**核心思想 (❗):** GMM中“混合”的本质，是**假设数据是由多个潜在的、不同的子群体（sub-populations）组成的**。我们不是用一个复杂的模型去硬套一个复杂的数据集，而是认为这个复杂的数据集其实是**多个简单子群体的简单叠加**。G-M-M 这三个字母的含义就是：

- **G (Gaussian):** 我们假设每个子群体都服从高斯分布。
    
- **M (Mixture):** 整个数据集是这些子群体的混合体。
    
- **M (Model):** 我们用这个混合假设来建立模型。
    

所以，GMM要解决的核心问题是：**“一个看起来很复杂的数据集，它背后可能隐藏着哪几个简单的、符合高斯分布的子群体？”**


