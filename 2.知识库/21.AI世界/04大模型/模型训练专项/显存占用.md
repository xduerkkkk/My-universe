# 非训练状态
![[显存占用-1757471703716.jpeg]]
看看非训练要占多少空间。

首先，embedding和最后lmhead的反embedding
都是vocab×hidden
最后一层归一化，也是hidden （归一化有学习的参数）
下面看看每一个block
首先Q矩阵和O矩阵，都是hidden方
KV矩阵呢， headdim = hidden / n_head  然后我们的矩阵是， `[hidden，n-kvhead ×headdim ]` 

**“16Φ级别”是怎么来的？**  
这是我们之前推导过的，在**FP32精度下，使用AdamW优化器进行训练**时，一个模型所需要的**最小静态显存**（不含激活值）的粗略估算：  
M_total = M_model + M_grad + M_optim

- **M_model = 4 * Φ**: 模型本身的参数，每个参数占4字节(FP32)。
    
- **M_grad = 4 * Φ**: 每个参数都需要一个梯度，也是FP32。
    
- **M_optim = 8 * Φ**: AdamW需要为每个参数存2个状态（动量m和方差v），都是FP32，所以是 4Φ + 4Φ = 8Φ。
    

**Total = 4Φ + 4Φ + 8Φ = 16Φ** 字节。

**一个7B模型的例子**:

- Φ = 7 * 10⁹
    
- 16 * Φ 字节 = 16 * 7 * 10⁹ 字节 = 112 * 10⁹ 字节 ≈ **112 GB**。
    
- 这就是为什么全参数微调一个7B模型，光是加载模型、梯度和优化器状态，就需要接近112GB的显存（这还没算激活值！），通常需要多张A100/H100 GPU才能做到。
## FP32训练和混合精度训练
#### **场景一：FP32训练 (所有东西都用精装版)**

1. **M_model = 4Φ (模型参数)**:
    
    - 厨师的**精装菜谱**。需要 4Φ 的空间存放。
        
2. **M_grad = 4Φ (梯度)**:
    
    - 厨师在做菜时，发现味道不对，需要在精装菜谱旁边用**同样高级的钢笔**写下修改意见（梯度）。大小和菜谱一样，也需要 4Φ 的空间。
        
3. **M_optim = 8Φ (AdamW优化器状态)**:
    
    - AdamW这位“副厨”，需要两本额外的**精装笔记本**来做记录：
        
        - 一本记录“最近的调整趋势”（**动量 Momentum**），需要 4Φ 空间。
            
        - 一本记录“每个步骤调整的幅度大小”（**方差 Variance**），需要 4Φ 空间。
            
    - 所以总共是 4Φ + 4Φ = 8Φ。
        
4. **M_total = 4Φ + 4Φ + 8Φ = 16Φ**:
    
    - 为了训练这个模型，光是存放菜谱、修改意见和副厨的笔记本，就需要16Φ的固定空间。
        

---

#### **场景二：混合精度训练 (聪明地使用“工作副本”)**

1. **M_model = 2Φ (模型参数)**:
    
    - 在厨房里实际使用的，是那本**FP16的“工作副本菜谱”**。它占的空间小，只需要2Φ。**前向传播和反向传播的计算，都在这张便签纸上进行，所以速度很快！**
        
2. **M_grad = 2Φ (梯度)**:
    
    - 厨师在这张FP16的便签纸上，用**铅笔**快速写下修改意见（梯度）。所以梯度也是FP16的，只需要2Φ。
        
3. **M_optim = 12Φ (AdamW优化器状态) - 这是最关键的地方！**
    
    - **问题**: FP16的数值范围很小，梯度更新量（lr * grad）可能会小到变成0（数值下溢），导致模型不学习。如果直接在FP16的菜谱上反复修改，很快就会因为精度损失而把菜谱改得面目全非。
        
    - **解决方案**: **“更新必须在高精度下进行！”**
        
    - AdamW这位“副厨”说：“你们厨房用便签纸无所谓，但我这里必须用**精装版**来保证最终菜谱的准确性！”
        
    - 所以，AdamW的优化器状态里，存了**三样东西**，而且**全是FP32**的：
        
        1. **一份FP32的模型权重主副本 (M_model,FP32)**: 这就是那本**精装原始菜谱**！每次更新时，是先在它上面修改，然后再抄一份新的FP16工作副本给厨房用。**占用 4Φ**。
            
        2. **FP32的动量 (M_momentum,FP32)**: 副厨的动量笔记本，必须是精装的。**占用 4Φ**。
            
        3. **FP32的方差 (M_variance,FP32)**: 副厨的方差笔记本，也必须是精装的。**占用 4Φ**。
            
    - 所以总共是 4Φ + 4Φ + 4Φ = 12Φ。
        
4. **M_total = 2Φ (FP16模型) + 2Φ (FP16梯度) + 12Φ (FP32优化器) = 16Φ**:
    
    - **惊人的结论**: 算下来，总的静态显存占用，和FP32训练时**一模一样**！
        

联想到[[optimizer，scheduler，scaler]]  实现混合精度训练

## 激活值显存
我们要把激活值保存，才能autograd
我们来解读**Attention部分**的“账单”（s是seq_len, b是batch_size）：

1. 2 x b x s x d_hidden: **归一化后的输入**。因为反向传播到LayerNorm时需要它。前面的2是因为通常使用FP16/BF16，每个元素占2字节。
    
2. 2 x b x s x d_hidden: **Q, K, V**。在计算对W_q, W_k, W_v的梯度时，需要原始的输入x。在计算对x的梯度时，需要W_q, W_k, W_v。这里为了简化，它把QKV的缓存合并估算了。
    
3. 2 x b x n_head x s x s: **注意力得分 logits (Q@K.T)**。在对Q和K求导时，需要这个得分矩阵。
    
4. 1 x b x n_head x s x s: **Dropout的掩码矩阵**。反向传播时需要用完全相同的mask来“关闭”梯度。
    
5. 2 x b x n_head x s x s: **Dropout后的注意力权重**。
    
6. 2 x b x s x d_hidden: **与V相乘后的结果**。
    

**FFN部分**的账单同理，它也需要保存up_proj和gate_proj的输出，以及激活函数后的结果，才能正确地计算反向传播

seqlen是显存杀手啊 
- **思想**: “我能不能不保存所有的‘犯罪现场证据’？太占地方了！”
    
- **做法**:
    
    1. 在前向传播时，对于某些计算开销不大但显存占用巨大的激活值（比如注意力矩阵），我们**不保存**它！
        
    2. 在反向传播，当Autograd需要用到这个被丢弃的激活值时，它会暂停一下，拿着当时的输入，**临时地、重新地**把这个激活值**再计算一遍**，用完之后马上丢掉。
        
- **权衡 (Trade-off)**:
    
    - **用计算换显存 (Trade compute for memory)**。
        
    - **优点**: 极大地**降低了**激活值的显存占用，让我们可以用更长的序列、更大的batch size进行训练。
        
    - **缺点**: 增加了额外的**重计算**开销，导致训练速度会变慢一些（通常是20-30%）。


**所以，当你听到人们讨论“Transformer的平方复杂度瓶颈”时，他们指的通常是两件事：**

1. 在**训练**时，**激活值**的**显存**占用是O(s²)。
    
2. 在**训练**和**推理的Prefill阶段**，**注意力计算**的**时间**复杂度是O(s²d)。