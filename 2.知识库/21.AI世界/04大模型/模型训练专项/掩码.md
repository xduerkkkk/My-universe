# 预训练
下面讲讲预训练时，输入文本训练，我们需要的掩码

训练！我们输入的是个完整的句子
比如
句子"我爱人工智能”
`[1, 4, d_model]`

我们根据attention的知识，整个句子一起生成Q向量，聚合成Q矩阵
接着我们进行QK矩阵相乘，得到注意力得分矩阵

```
     K:"我"  K:"爱" K:"人工" K:"智能"  <-- Key (被关注的对象)
Q:"我"    [[ 2.5,  1.3,   0.8,   1.1],   # "我"对所有词的关注度
Q:"爱"     [ 1.8,  3.1,   2.2,   0.9],   # "爱"对所有词的关注度
Q:"人工"   [ 1.2,  2.8,   4.5,   3.5],   # "人工"对所有词的关注度
Q:"智能"   [ 0.7,  1.1,   3.9,   5.1]]   # "智能"对所有词的关注度
 ^
 |
Query (发起关注的主体)
```
比如第一行，代表什么？ “我”对句子的每一个字的注意力关注程度。
可是我们要训练模型的生成单个字的能力。

走一遍流程
训练流程！
"我 爱 人工智能 [EOS]"，假设Token ID是 [10, 17, 27, 99, 2]。
- **输入 X (input_ids)**: [10, 17, 27, 99] (去掉最后一个token)
    
- **目标 Y (labels)**: [17, 27, 99, 2] (去掉第一个token)
    
    - X和Y的形状都是 [batch_size, seq_len]，这里是 [1, 4]。
接下来，我们直接把输入送进tranformer，会怎样？
会进入QK注意力分数。
如果我们不做任何处理，我们的X，就是 4行4列， 我那一行是看过所有词后，打注意力分数，组合成的词向量  第二行，是爱看过所有词后，打注意力分数，组合成的词向量
现在掩码作用就来了！
我们保留自身以及之前的注意力分数
我们把第一行，只留一个”我“的信息，也就是第一行只有第一列保留注意力分数。
第二行是保留”我爱“
拿第二行来说，这就是”爱“对当前句子，”我爱“的注意力信息，得到的词向量！
跳出来，我们把整个注意力矩阵都送往lmhead
我们原来的词向量，变成了vocab_size维度， 每一个位置对应词库上的一个词
`【0.2，0.5，0.2.....]` 数字代表概率
我们是每一行都把词向量变成这种格式了！
但我们仍然拿第二行说一下，
第二行就是`loguts[0,1,:]`  这！就代表，爱在接收对”我爱“的注意力信息后，蕴含的信息！
就能蕴含着，”下一个词是谁“  
而每一行都像第二行一般！
所以就达到了并行预测！ 
现在反过来看， 如果不用掩码，第二行代表，爱在接收对”我爱人工智能“的注意力信息后，蕴含的信息！蕴含着，”下一个词是谁“  。这就属于”作弊“
接着我们进行交叉熵函数计算损失
在代码里如果用nn.crossEntropyLoss
要注意展平
```python
- logits_flat = logits.view(-1, vocab_size) -> 形状 [4, vocab_size]
    
- Y_flat = Y.view(-1) -> 形状 [4]，内容是 [17, 27, 99, 2]

```
我们计算平均损失（怎么感觉这样不精确呢？反向传播的值对每一个词是一样的？ 反向传播调整的是什么参数？）
# 推理
推理仍然要！
一个casusemask（与训练保持一致）
一个attentionmask（掩盖padding的地方）
