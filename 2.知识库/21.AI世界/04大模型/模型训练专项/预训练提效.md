zero1数据并行技术
（数据并行和张量并行的区别）


> “学习率调度啊，我们为了训练初期的稳定，可能加大学习率，叫warmup，再后期我们要精细地收敛就是减小学习率。”

**点评**: 你的大方向是对的，但Warmup的细节正好说反了。这是一个非常常见的口误。

**精确修正**:

- **Warmup (预热)**: 在训练的最开始阶段（比如前1000步），学习率是**从一个非常小的值（甚至是0）慢慢地、线性地增加**到我们设定的初始最大学习率。
    
- **为什么？**
    
    - 模型刚开始时，参数是**随机初始化**的，完全是“一派胡言”的状态。
        
    - 此时如果直接用一个很大的学习率，梯度可能会非常大且不稳定，导致模型参数被“一脚踹飞”，训练过程直接“爆炸”（loss变成NaN）。
        
    - Warmup就像是**“热车”**，用很小的学习率，先让模型**稳定地、小心翼翼地**走出第一步，进入一个比较合理的状态，然后再“挂挡提速”。
        
- **Decay (衰减)**: Warmup之后，学习率再按照一个策略（如余弦函数）**缓慢下降**，直到训练结束时接近0。这部分你的理解是完全正确的——为了在训练后期进行“精细微调”。

**两种最著名的“智能”初始化方法**:

1. **Xavier / Glorot 初始化**:
    
    - **提出者**: Xavier Glorot (发音：格洛肉)
        
    - **核心思想**: 它让权重的初始方差，同时考虑到**输入神经元的数量 (fan_in)**和**输出神经元的数量 (fan_out)**。
        
    - **公式 (近似)**: 权重从一个均值为0，方差为 2 / (fan_in + fan_out) 的均匀分布或正态分布中采样。
        
    - **适用激活函数**: tanh, sigmoid 等以0为中心的激活函数。
        
2. **Kaiming / He 初始化**:
    
    - **提出者**: 何恺明 (Kaiming He)
        
    - **核心思想**: Xavier初始化在配合ReLU激活函数时效果不好，因为ReLU会“杀死”一半的负值神经元，导致信号的方差减半。Kaiming初始化专门针对这一点做了修正。
        
    - **公式 (近似)**: 权重从一个均值为0，方差为 2 / fan_in 的正态分布中采样。它只考虑输入维度，因为ReLU的特性使得它不需要考虑输出维度。
        
    - **适用激活函数**: **ReLU及其变体（如SiLU, GELU）**。这是**现代深度学习**，包括Transformer中**最常用**的初始化方法。