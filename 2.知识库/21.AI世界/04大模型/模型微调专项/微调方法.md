


#  PEFT- **提示式微调**
## **Prompt Tuning (软提示-前缀)**:
在prompt前拼接离散的k个token，只训练这虚拟向量
训练好其实就不变了，比如我们是微调法律领域
接下来用户使用大模型，输入法律领域的prompt时，这个拼接在prompt的向量就好像激活了一般，进入模型，模型就识别到了“哦这是法律领域的问题”
具体代码做法是 ？
从代码中可以看到，这k个虚拟Token在**优化空间**中是**相互独立**的点。它们之间没有平滑的内在联系。反向传播，要独立的传播。
1. 在模型之外，我们创建一个新的、小型的torch.nn.Embedding层。这个Embedding层的大小是 (k, hidden_dim)，其中 k 是你想要的虚拟token数量（比如20个），hidden_dim 是大模型本身的隐藏层维度（比如Llama 7B是4096）。这个Embedding层就是我们唯一要训练的部分。
    
2. **获取“咒语”**：在每次前向传播时，我们从这个Embedding层里，把这 k 个向量（[k, hidden_dim]）全部取出来。
    
3. **拼接**：将用户的输入文本正常通过模型的input_embedding层，得到一个形状为 [batch_size, seq_len, hidden_dim] 的输入向量。然后，我们将上一步取出的“咒语”向量 [k, hidden_dim] 扩展成 [batch_size, k, hidden_dim]，并和输入向量在序列长度维度上拼接起来，变成 [batch_size, k + seq_len, hidden_dim]。
    
4. **后续计算**：这个拼接后的、更长的向量序列，被送入Transformer的第一层，开始后续的计算


## **P-Tuning (v1)**:
 和Prompt Tuning非常相似，也是学习虚拟的Token，也是拼接在prompt
 但是，我们步是独立的token了，我们用小的LSTM生成连续的token，这样更平滑更稳定
 不过有点空，具体代码是？
 1. **创建“咒语生成器”**：我们不再创建一个简单的nn.Embedding，而是创建一个小型的**Prompt Encoder**，它通常由一个nn.Embedding层 + 一个小型的BiLSTM + 一个MLP组成。
    
2. **生成“咒语”**：输入一些“占位符”token到这个Prompt Encoder里，然后由它输出最终的 k 个虚拟token向量。
    
3. **拼接与后续**：和Prompt Tuning一样，将生成的“咒语”向量拼接到真实输入的embedding前面，送入大模型。在训练时，只更新这个小小的Prompt Encoder的参数。
        

## **P-Tuning v2 (更重要)**:
不只是拼接prompt，这样模型层层传递都把拼接的信息传递没了，
于是说，把这个虚拟token每一层都注入
但，这是残差一样的注入吗？就是每一层注入的token应该都一样？还是？
- **每一层注入的token是不同的！** 我们会为模型的每一层都创建一组**独立可学习**的虚拟token。
    
- **具体代码做法是**：
    
    1. 创建一个可训练的参数张量，形状为 [num_layers, k, hidden_dim]。
        
    2. 在模型进行前向传播时，对于第 L 层，我们取出 [L, :, :] 这个切片，得到属于第 L 层的 k 个虚拟token。
        
    3. 将这 k 个虚拟token，拼接到从第 L-1 层传过来的真实token的隐藏状态序列的前面。
        
    4. 这个拼接后的更长的序列，再送入第 L 层的自注意力模块和FFN中进行计算。
        
- **效果**：这样做，相当于在模型的每一层都强行塞入了一个“任务指南”，让模型在每一步计算时都“牢记”当前的任务是什么，控制力远强于只在输入层做文章。

## Prefix-tuning

不搞prompt了
我们直接动刀模型深层的，我们给kvcache加个向量

是在计算完k矩阵，v矩阵后，再拼个向量。
具体训练呢，就单独训练这个向量，其他冻结
- **和P-Tuning v2的细微但重要的区别**：
    
    - P-Tuning v2是把虚拟token和**真实的token隐藏状态**拼接，然后这个拼接后的整体再去计算Q, K, V。
        
    - Prefix-Tuning是先让**真实的token隐藏状态**算出自己的Q, K, V，然后拿出**另外**学习到的“前缀向量”（prefix vectors），直接拼接到K和V序列的前面。
        
- **效果**：这意味着，真实的token（通过它们的Q）可以attend to（关注）这些虚拟的前缀token（通过它们的K和V），但虚拟的前缀token之间以及它们自身不会进行自注意力计算。这是一种更直接地干预注意力计算过程的方式。

|   |   |   |   |
|---|---|---|---|
|方法|“动刀”位置|核心机制|特点|
|**Prompt Tuning**|输入层 (Embedding)|拼接**独立**的虚拟Token|最简单，参数量最少，但有时不稳定|
|**P-Tuning v1**|输入层 (Embedding)|用LSTM生成**连续**的虚拟Token再拼接|试图让提示更平滑，但更复杂，已被v2取代|
|**P-Tuning v2**|**每一层** (Hidden States)|在**每一层**都拼接**不同**的虚拟Token|控制力强，效果稳定，是目前的主流方法之一|
|**Prefix-Tuning**|**每一层** (Attention K,V)|在**每一层**为K和V矩阵拼接**前缀向量**|直接干预注意力计算，效果好，与P-Tuning v2并驾齐驱|


# PEFT-旁路式微调



## Adapter：
在atten输出，和FFN输出的后面，加入Adapter板块，先降维，再激活函数，再升维（恢复）
但在与原始输入的**主残差连接相加之前**


# 其他方法
- 我们创建一个**“学生”小模型**。这个学生模型的结构和大模型**完全一样**，只是**更“瘦”更“矮”**。
    
    - **更矮 (Fewer Layers)**: 比如，老师有12层，学生一开始也可能有12层，或者少一点，比如6层。
        
    - **更瘦 (Smaller Hidden Size)**: 老师的hidden_size是1024，学生的可能是512。
        
- **知识蒸馏 (Knowledge Distillation)**:
    
    - 我们用同样的预训练任务（比如MLM）来训练这个学生模型。
        
    - 但是，损失函数变了！  
        Total Loss = α * Student_Loss_on_Hard_Labels + (1-α) * Distillation_Loss
        
        - Student_Loss_on_Hard_Labels: 和原来一样，学生自己预测的结果和真实标签（Hard Label）之间的交叉熵损失。
            
        - **Distillation_Loss (核心)**: 学生模型的输出Logits，要去模仿**老师模型**输出的**软标签（Soft Label，即概率分布）**。
            
    - **为什么？** 老师的输出[0.1, 0.2, 0.6, 0.1]比真实标签[0, 0, 1, 0]包含了**更丰富的信息**。它不仅告诉学生“正确答案是第3个”，还告诉他“第2个答案也有点可能，但第1和第4个就差远了”。学生通过模仿老师的“思路”，能学得更快更好。
        

**阶段三：“渐进”收缩**

- **核心**: 训练不是一次性完成的。
    
- **做法**:
    
    1. 我们先蒸馏一个12层的学生模型。
        
    2. 训练一段时间后，我们发现模型学得差不多了。
        
    3. 然后，我们**“砍掉”**学生的几层！比如，直接把顶部的6层Transformer Block**扔掉**，只保留底部的6层。
        
    4. 我们用这个**更小**的、6层的学生模型，**继续**进行知识蒸馏训练。
        
- **为什么叫“渐进收缩”？** 因为我们是一步步地、在训练过程中把模型的尺寸“收缩”变小的。


## LoRA

### 成立的条件
“LoRA的核心假设，基于一个在深度学习中被广泛观察到的现象，叫做**‘预训练模型的低内在维度（Low Intrinsic Dimension）’**。”

“这个假设认为：”

- “像GPT-3这样的大型语言模型，虽然它拥有数千亿的参数，处在一个极高维度的空间里，但它们为了适应某个**特定的下游任务**所需要进行的**参数调整**，实际上是发生在一个**非常低的、‘内在’的维度**中的。”
    
- “换句话说，模型为了从一个‘通用知识库’变成一个‘法律专家’，它**不需要**对它所有的亿万个参数都进行剧烈且无关的调整。它只需要在某些**关键的、低维的子空间**里进行微调就足够了。”
    
- “**LoRA正是利用了这一点。** 它不去学习那个巨大的、d x k维度的完整更新矩阵ΔW，而是假设这个ΔW是**低秩**的。通过学习两个小的矩阵A和B，它们的乘积B @ A就是对这个理想的、低秩的ΔW的一个**高效近似**。这使得我们能用极少的参数，来捕捉到任务微调所需的最核心的参数变化。”