# 低秩矩阵的初始化
LoRA要求  或者  其中之一必须使用零矩阵进行初始化，这样当数据第一次通过网络时，它和预训练的结果是一致的，这样便保证了模型在初始阶段便有一个不错的效果。苏剑林老师指出，这种一个全零，一个非全零的方式带来了不对称的问题，其实我们也可以使用两个非全零矩阵进行初始化，但是需要事先将预训练权重减去初始化的值，即
背后体现了深度学习中一个非常重要的工程思想——**保证初始化的稳定性**。
1. **保证稳定的起点 (Stable Starting Point)**:
    
    - 预训练大模型本身是一个经过精心训练、处于**稳定状态**的系统。它已经具备了强大的通用能力。
        
    - 如果我们随机初始化A和B，那么x @ A @ B的输出会是一个**随机的、无意义的噪声**。
        
    - 将这个巨大的随机噪声，直接加到x @ W₀这个有意义的输出上，会**瞬间破坏**掉模型原始的、稳定的输出分布。
        
    - 这会导致训练的**第一步**就产生一个**巨大且混乱的损失和梯度**，就像给一个平稳运行的系统猛踩一脚油门，很容易导致训练**不稳定甚至崩溃**。
        
    - 通过将B初始化为0，我们确保了微调是从**原始模型那个非常好的起点**开始的，然后梯度会**平滑地、逐渐地**让B的权重从0开始“生长”，旁路的作用是慢慢被“开启”的。
        
2. **保留预训练知识 (Preserving Pre-trained Knowledge)**:
    
    - 这个初始化策略，在哲学上体现了对预训练模型能力的**尊重**。
        
    - 它相当于在说：“我默认你（预训练模型）已经很棒了。我的微调，只是在你已有的强大基础上，进行**小范围的、必要的修正**。”
        
    - 训练过程是从一个“零修正”的状态开始，只有当损失函数明确地告诉模型“你需要调整”时，B的权重才会离开0，旁路才会开始产生有意义的输出。
# 应用的位置
1. **LoRA的应用位置**:
    
    - LoRA**最常**被应用在Transformer的**注意力模块**的四个线性层上：q_proj, k_proj, v_proj, o_proj。
        
    - 为什么？因为这些层被认为是**任务适应性最强**的部分。
        
    - 有些实现也会把它应用在**FFN层**的线性层上，或者**Embedding层**和**LM Head**上，但这不那么常见。知道它主要用在Attention层是关键。

# 矩阵的“相加”
1. **“训练时 (Training)”**:
    
    - “我们**不会**去合并矩阵。原始的权重矩阵W₀保持冻结，完全不变。”
        
    - “数据x会走两条并行的‘**旁路**’：
        
        - **主路**: h₁ = x @ W₀ (使用原始的、冻结的权重)
            
        - **旁路**: h₂ = x @ A @ B * alpha (先经过A矩阵降维，再经过B矩阵升维，alpha是一个缩放系数)
    
            
    - “最终的输出是这两条路结果的**直接相加**：output = h₁ + h₂ = x @ W₀ + x @ A @ B * alpha。”
        
    - “这样做的好处是，我们**完全没有修改**原始模型，梯度只会流经A和B这两个小矩阵，训练非常高效。”
        
2. **“推理时 (Inference)”**:
    
    - “为了达到**最高的推理速度**，我们可以将学习到的‘旁路’合并回主路，实现**零额外延迟**。”
        
    - “在部署模型之前，我们可以**一次性地**计算出 ΔW = B @ A * alpha。”
        
    - “然后，我们创建一个**新的权重矩阵 W' = W₀ + ΔW**。”
        
    - “最后，我们用这个**合并后**的矩阵W'来替换掉原始的权重矩阵W₀，并**丢弃**掉矩阵A和B。”
        
    - “这样，在推理时，模型就变回了原始的结构，没有任何额外的计算开销，只是权重被更新了。”

# AdaLoRA
**在 AdaLoRA 中，每一层的 Q, K, V, O, FFN 的 rank 都可以是不同的。** 这是一种极其精细化的参数分配策略。

1. **AdaLoRA 的核心思想**：它不是用固定的 rank，而是用一个总的**参数预算**，并**动态地**将这些预算分配给模型认为最重要的权重更新上。这是一种自适应的 (Adaptive) 方法。
    
2. **AdaLoRA 的实现机理 (加分项)**：可以简单提及它借鉴了 SVD 的思想，通过一种 PΛQ 分解来学习每个更新分量的“重要性分数”，然后根据这个分数进行剪枝，从而实现 rank 的智能分配。

# QLoRA
- **你是否理解硬件的限制？** 你是否知道一个65B的模型在16-bit精度下需要65 * 2 = 130GB的显存，这在单张消费级显卡上是根本不可能的。
    
- **你是否理解常规方案的弊端？** 你是否知道，如果粗暴地把模型量化到4-bit再训练，精度损失会非常大，模型可能就“变笨”了。
这个思想是关键。在任何计算过程中（前向和反向传播），当需要用到基座模型的权重时，**QLoRA会动态地将那一小块4-bit的权重“解压缩”回16-bit**，与16-bit的LoRA矩阵进行计算，算完后立刻丢掉解压后的16-bit权重，显存中始终只保留4-bit的版本

#### **1. 新的数据类型：NF4 (4-bit NormalFloat)**

- **要解决的问题**：标准量化很“傻”，它假设权重是均匀分布的。但实际上，大模型的权重通常是**均值为0的正态分布（钟形曲线）**。这意味着绝大部分权重都挤在0附近，而很大的正数或负数非常少。（何凯明初始化？）
    
- **QLoRA的聪明之处**：NF4是一种“数据感知”的量化方法。它不是均匀地切分区间，而是**根据正态分布的形状来设定量化点**。在0附近，量化点非常密集，可以精确地表示那些微小的权重；在远离0的地方，量化点就比较稀疏。
    
- **一句话理解**：**NF4是一种专门为正态分布的权重“量身定制”的4-bit数据类型，确保了量化后的信息损失最小。**
    

#### **2. 双重量化 (Double Quantization)**

- **要解决的问题**：量化不是只存0-15的整数就完事了。为了能把整数再变回浮点数（反量化），你需要为**每一小块(block)**的权重存储一些元数据，最重要的就是**量化常数（或叫缩放因子）**，比如这一块权重的最大绝对值是多少。这些元数据本身也会占用显存。论文里算了一笔账，这个开销大概是每个参数0.5 bits。对于几十亿参数的模型，这依然是很大的开销。
    
- **QLoRA的聪明之处**：它想出了一个“套娃”式的点子——**对这些量化常数，再进行一次量化！** 这就是“双重量化”。
    
- **一句话理解**：**双重量化就是对“量化的元数据”本身再做一次量化，把元数据的显存开销也极致地压缩了。**
    

#### **3. 分页优化器 (Paged Optimizers)**

- **要解决的问题**：在训练过程中，除了模型参数，**优化器状态**（比如Adam优化器会为每个可训练参数存储动量和方差）也需要占用大量显存。有时候模型本身能装下，但优化器一启动就爆显存了。
    
- **QLoRA的聪明之处**：它利用了NVIDIA的一个技术（统一内存），让**CPU内存成为GPU显存的“虚拟内存”**。当显存不够用时，它会自动把暂时用不上的优化器状态“踢”到CPU内存里；需要用的时候再加载回来。
    
- **一句话理解**：**分页优化器就是给优化器状态也搞了个“虚拟显存”，防止在训练过程中因为梯度更新而导致的瞬间显存溢出。**

“QLoRA并不仅仅是4-bit量化和LoRA的简单相加，它是一套完整的、旨在**在消费级显卡上实现高性能大模型微调**的系统性方案。它的核心思想是**加载一个4-bit的基座模型，但在训练中将梯度只作用于一个16-bit的LoRA适配器上**，从而在节省大量显存的同时，几乎不损失微调性能。

为了实现这一点，它引入了三个关键技术：

1. **NF4数据类型**：这是一种为模型权重（通常呈正态分布）量身定制的4-bit数据格式，相比传统的均匀量化，它的精度损失更小。
    
2. **双重量化**：为了进一步降低量化元数据的存储开销，它对元数据本身又进行了一次量化，极致地压缩了显存。
    
3. **分页优化器**：它利用CPU内存作为GPU的“虚拟内存”，来存储优化器状态，防止了训练过程中可能出现的显存峰值导致的OOM（内存溢出）问题。
    

通过这套组合拳，QLoRA成功地让在单张24G/48G显卡上微调65B甚至更大规模的模型成为了可能。”




# LORA微调优势
对比Adaptor微调，Adaptor其实是改了模型架构，增加了模型深度，势必增加了时间
Prefix微调呢，在模型流动层的kvcache加了虚拟token，相当于增加了序列长度，即增加了时间复杂度和空间复杂度。
而LoRA所谓的“旁路”，就是说计算和原权重是并行的，只是反向传播调整参数而已。
所以没有增加任何复杂度。
在推理时，更是直接和原架构没有任何区别

### **回答面试官追问：为什么Prefix-Tuning和Adapter不行？**

- **Adapter为什么不能合并？**
    
    - Adapter的结构是 Up_proj(Activation(Down_proj(x)))。
        
    - 因为它中间有一个**非线性的激活函数 (Non-linear Activation Function, e.g., ReLU)**！
        
    - 我们无法将一个非线性操作“合并”到一个线性的权重矩阵中。你不能预先计算出它的效果，因为它依赖于**运行时的输入x**。
        
    - 所以，在推理时，数据流必须老老实实地先走完主路，再走一遍Adapter的旁路，最后相加。这个**额外的计算步骤是无法消除的**。
        
- **Prefix-Tuning为什么不能合并？**
    
    - Prefix-Tuning是在**序列维度**上进行**拼接 (cat)**，K_final = torch.cat([K_prefix, K_from_input])。
        
    - K_from_input是**依赖于当前输入**的，每次都不同。
        
    - 你无法把一个固定的、可学习的前缀K_prefix，“合并”到那个负责生成K_from_input的权重矩阵W_k里去。它们是在不同的阶段、以不同的方式作用于数据流的。
        
    - 因此，在推理的每一步，这个**拼接操作**和**更长的注意力计算**都必须被执行，额外的延迟也是无法消除的。