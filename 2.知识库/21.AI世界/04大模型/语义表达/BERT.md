是encoder only
俩大特殊点
1. MASK 填空式预测  通过挖空让模型预测词 ， 提高对语言的理解能力
2. CLS标签 去代表句子向量   ， 

三个输入层：
- token：通过embedding层得到的东西
- segment embedding： 为每个token定义，属于哪个句子
- position embedding：位置编码

三个输入层是相加，为什么选择相加做为信息融合的方式，而不是拼接？
- 相加保证维度不变
- 作者相信，模型能自主分离出这三层信息，并且自己判断哪一层重要，最终相加得到的向量，是处在了这个高维空间中一个**独一无二的点**，这个点的位置**同时编码**了这三种信息。Transformer强大的自注意力机制，有能力在这个高维空间中学会识别和利用这些由不同嵌入源贡献的“细微偏移”，从而在不同的上下文中，关注到不同方面的信息。