# 分词宏观策略

句子要切块处理，就好比cpu处理时的机器字长嘛！

那直觉来看，要么按词分，要么按字分
### 按词分

**例子句子**：  
`"I have two dogs."`

**切分结果**：  
`["I", "have", "two", "dogs", "."]`

需要为所有单词分配ID（如 `dog=1001`，`dogs=1002`），词表可能达到数万甚至百万级
还有dog和dogs完全没关系嘛 模型也不知道他们是单复数关系

### 按字分
**例子句子**：  
`"I have two dogs."`

**切分结果**：  
`["I", " ", "h", "a", "v", "e", " ", "t", "w", "o", " ", "d", "o", "g", "s", "."]`

那更是把语义关联切断了

# 亚词分词策略
## BPE
首先初始分词用空格、标点分，很好理解。
然后我们使用贪心的算法策略，
一遍一遍拼两个相邻的字符，统计频率，
具体咋做？？
[[tokenizer]]




**1. 准备阶段 (Initialization)**:

- **输入**: 一大段文本语料库。
    
- **预分词**: 先把文本按**空格**或**标点**切分成一个个“单词”。为了区分单词的边界，通常会在每个单词的末尾加上一个特殊符号`</w>`。
    
    - "I love hugging face" -> {"I`</w>": 1, "love</w>": 1, "hugging</w>": 1, "face</w>": 1}
        
    - 假设我们有更多数据：`{"low</w>": 5, "lower</w>": 2, "newest</w>": 6, "widest</w>": 3}`
        
- **建立初始词汇表**: 词汇表就是语料库中出现过的所有**单个字符**。
    
    - Vocab = ["<", "/", "w", ">", "l", "o", "e", "r", "n", "s", "t", "i", "d"] (假设就这些)
        

**2. 迭代合并阶段 (Iterative Merging)**:

**第1轮迭代**:

- **寻找最高频的相邻字符对 (Byte Pair)**: 遍历所有单词，数一数哪两个相邻的字符出现次数最多。
    
- 假设我们发现 ("e", "s") 这个组合出现频率最高（在newest和widest中）。
    
- **执行合并**: 将"es"合并成一个新的亚词单元"es"，并把它**加入到我们的词汇表**中。
    
- **当前词汇表**: [..., "es"]
    

**第2轮迭代**:

- 我们再次遍历所有单词（现在"es"被看作一个整体了）。
    
- 寻找新的最高频相邻对。假设这次是("es", "t")（在newest和widest中）。
    
- **执行合并**: 将"est"合并成一个新的亚词单元"est"，并加入词汇表。
    
- **当前词汇表**: [..., "es", "est"]
    

**第3轮迭代**:

- 假设这次最高频的是("l", "o") (在low和lower中)。
    
- **执行合并**: 将"lo"加入词汇表。
    
- **当前词汇表**: [..., "es", "est", "lo"]
    

**...这个过程会一直重复下去...**

**3. 停止阶段 (Termination)**:  
这个合并过程会一直持续，直到满足以下任一条件：

- 词汇表的大小达到了我们预设的**上限**（比如30000）。
    
- 没有更多的字符对可以合并了。
    

**最终我们得到了什么？**  
一个包含了**单个字符**和大量**高频亚词片段**的、大小合-适的最终词汇表。


## 基于Subword的切分

高频词就按完整的样子保留  
低频词就拆分成高频词和其他  
什么叫低频词？比如dogs  
像 `dog` 这样的基词在语料中出现的频率远高于其变体（`dogs`、`doggy`等）

`dogs` 是低频词，被拆分为 `dog` + `##s`（`##`表示子词后缀）  
`["I", "love", "dog", "##s", "and", "hot", "##dog", "##s", "."]`

- `dogs` → `dog` + `##s`
- `hotdogs` → `hot` + `##dog` + `##s`

这是我们人在这说呢，那机器怎么知道他要进行高频词保留低频词拆分？  
模型是一步一步学习的，先全部按字分，统计相邻两个字的词频，就能知道高频是哪两对  
先尝试拼起来，然后看这高频不，高频就说明拼对了，就这么分。循序渐进。

- 初始拆分：`h o t d o g s`
    
- 合并高频对：
    
    - `o` + `g` → `og`
    - `d` + `og` → `dog`
    - `h` + `o` → `ho`
    - `t` + `dog` → `tdog`（频率低，不合并）
    - 最终可能得到：`hot` + `dog` + `s`

开始时将所有单词拆分为 **字符（或字母）**，并对非起始字符添加 `##` 前缀。  
**例如**：  
单词 `"dogs"` 的初始拆分：

- `d` （开头字符，不加 `##`）
- `##o` （非开头字符，加 `##`）
- `##g`
- `##s`  
    `##` 明确表示“这个子词不能作为单词的开头”，帮助模型区分  
    而且有特殊符号标记就大概率是前缀后缀  
    如`##ing` 只能出现在词尾 `playing`  
    非常棒的处理

#### **BPE如何对新句子进行分词？**

当拿到一个新句子时，BPE会用它学到的“合并规则”，贪心地进行分词。

- **句子**: "I am the lowest"
    
- **预处理**: `["I</w>", "am</w>", "the</w>", "lowest</w>"]`
    
- **分词**:
    
    - "`I</w>" -> 在词汇表里，直接匹配。
        
    - `"lowest</w>" -> BPE会发现它可以被拆分成 ["lo", "w", "est", "</w>"]，而这些片段都在它的词汇表里。

## Wordpiece
每一个token都有它的概率呀  count（token）/ total_token
这个句子 最大似然概率越大 说明分词越成功 



<font color="#f79646">这俩者方法都用的自底向上的贪心合并策略</font>