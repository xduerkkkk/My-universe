首先定义好
词向量是比较广泛的概念，只要用向量表示词，这些向量都可以称为词向量
词嵌入是明确的概念
- 可以说是一种思想技术，即将高维稀疏的词表示，嵌入到低维稠密的向量空间
- 这个嵌入得到的具体的、低维稠密的词向量，也可以称为词嵌入
词嵌入就是word embedding
比如说有10维，那这10维每一个维度都有数字 一起表示这个词的含义（在向量空间上）
# Word2Vec
首先，我们未经训练，只有onehot向量。
先定义一下词表，  比如”我“对应”1“ ”爱“对应”2“ 这个对应就是词表
所以显然 词表的维度就是我们所拥有的词
而onehot向量呢，  是词向量 
假如我们现在就是表示“爱：这个词
它的onehot向量  就是 【010000...】 只在第二个位置上有个1，表示”爱“
所以他的维度是1×N 这个N就代表词表维度，就是有多少个词。
我们的目的是什么？  是有一个N×K的矩阵， 每一行代表一个词的词向量！ 这就是词嵌入
这个维度就是`[vocab_size, embedding_dim]`
我们怎么达成目的呢？
分为两个模型
skip-gram
传入一个词，1×N维度， 我们使用N×K矩阵 矩阵乘法得到1×K的词向量
接着，我们借助K×N的权重矩阵， 是矩阵乘法，1×N的词向量  这个词向量就不是hot了，而是稠密的。 我们根据这个向量使用softmax，我们要让真实的上下文所在位置概率大，通过反向传播调整N×K矩阵 
CBOW
传入多个词，每个词都是1×N维度，使用N×K矩阵 下面有很多个1×K向量
然后我们相加或去平均，是聚合。接着去和矩阵K×N做矩阵乘法 1×N向量  我们要让中心词，那个词，概率最高 反向传播调整矩阵


我们的核心都是那个初始的N乘K的矩阵








