解决OOV问题的方法，也经历了一个清晰的技术演进过程。
最简单的就是，标为unk标签
接着进一步发展，就是FastText的技术，拆分字符n-gram，就是用先有分词，现场拼接，弄出一个全新的词向量，现场创造 
到了现代
Subword Tokenization  就彻底解决了  详细来说就是wordpiece，bpe算法

#### **方案一：简单粗暴的 `<UNK>` (Unknown Token)**

- **这是最早期、最基础的处理方式。**
    
- **做法**:
    
    1. 在构建词汇表时，我们预留一个特殊的Token，叫做`<UNK>`（或者`<OOV>`）。
        
    2. 当Tokenizer遇到任何不认识的词时，就**一律**把它映射到`<UNK>`这个Token的ID上（比如ID 0）。
        
- **优点**:
    
    - 实现简单，保证了程序不会因为查不到词而崩溃。
        
- **缺点 (非常致命)**:
    
    - **信息损失严重**: “奥特能”和“擎天柱”这两个不同的、新出现的词，在模型眼里会变成**完全一样**的`<`UNK>`。模型丢失了所有关于这两个词的独特信息。
        
    - **语义理解差**: 模型无法从`<UNK>`这个通用的“未知”符号中，推断出任何有用的语义。
        

#### **方案二：FastText - 利用“构词法”来猜测**

- **这是Facebook在Word2Vec之后提出的一个重大改进。**
    
- **核心思想**: **一个词的含义，不仅由它的上下文决定，也由它内部的字符构成来决定。**
    
- **做法**:
    
    1. FastText在学习词向量的同时，还学习了**字符级n-gram（character n-grams）**的向量。
        
    2. 比如，对于单词apple，除了学习apple这个整体的词向量，它还会学习ap, pp, pl, le, app, ppl, ple... 这些**子词片段**的向量。
        
    3. 当遇到一个未登录词，比如apples时，Word2Vec会直接判为`<UNK>`。
        
    4. 但FastText会把它拆分成字符n-gram，比如 app, ppl, ple, les。然后，它会把这些**已知的、构成它的子词片段的向量加起来**，从而**合成**出一个全新的、专门给apples的词向量！
        
- **优点**:
    
    - 能为未登录词生成一个**有意义的、不是`<UNK>`**的向量。因为apples和apple共享了很多相同的字符n-gram，所以它们合成出的词向量在空间中会非常接近。
        
    - 对于处理拼写错误、词的屈折变化（如run -> running）非常鲁棒。
        
- **缺点**:
    
    - 词典变得非常大（因为要存储海量的n-gram向量），内存占用高。
        

#### **方案三：亚词分词 (Subword Tokenization) - 现代LLM的标配**

- **这是目前最主流、最根本的解决方案。** 我们在讨论vocab_size时已经详细讲过了。
    
- **代表算法**: BPE (Byte-Pair Encoding), WordPiece (BERT使用), SentencePiece (Llama/GPT使用)。
    
- **核心思想**: **放弃“词”是基本单位的想法，转向使用更小的、有意义的“亚词”片段作为基本单位。**
    
- **如何解决OOV**:
    
    - 通过BPE等算法构建的词汇表，本身就包含了大量的高频**亚词**（如"tion", "ing", "pre"）和所有**单个字符**。
        
    - 当遇到任何一个未登录词时，Tokenizer**总能**把它拆分成一个由**已知亚词或单字符**组成的序列。
        
    - 例如，"奥特能"可能会被拆分成 `["奥", "特", "能"]` 这三个单独的汉字token。
        
    - **根本就没有`<UNK>`了！** 因为最坏的情况也能退化成字符级别的表示。
        
- **优点**:
    - **从根本上消除了OOV问题**。
        
    - 词汇表大小可控，且能表示无限的词语。
        
    - 能通过共享的亚词，理解词的形态和语义关联（running和eating都包含ing）。