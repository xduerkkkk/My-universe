#### **. 奠基者与开源标杆**

- **GPT系列 (OpenAI)**
    
    - **角色**: **行业的开创者和引领者**。从GPT-1到GPT-4，每一代都定义了“下一个时代”的可能性。
        
    - **技术贡献**:
        
        - **推广了Decoder-Only架构**: 证明了“文字接龙”这种简单的预训练任务，在足够大的模型和数据下，可以涌现出惊人的能力。
            
        - **开创了指令微调(SFT)和RLHF的范式**: InstructGPT这篇论文，是让模型变得“有用”和“可控”的里程碑。
            
        - **引领多模态**: GPT-4V展示了顶级的多模态理解能力。
            
    - **生态位**: **闭源模型的绝对王者**，是技术能力的天花板和所有其他模型追赶的目标。
        
- **Llama系列 (Meta)**
    
    - **角色**: **开源社区的“真神”和事实标准**。
        
    - **技术贡献**:
        
        - **高质量的开源模型**: Llama 1/2/3的发布，极大地推动了开源社区的发展，让学术界和中小企业也能参与到大模型的浪潮中。
            
        - **架构的“黄金标准”**: 它采用的RMSNorm, SwiGLU, RoPE这套“配方”，被后续几乎所有的开源模型（包括Qwen, Mistral, DeepSeek）所沿用。
            
        - **工程实践的典范**: 提出了**GQA**来优化推理，证明了其在性能和效率上的巨大优势。
            
    - **生态位**: **开源模型的基石**。绝大多数的开源微调工作、研究、应用，都是基于Llama系列展开的。
        

#### **2. 国内的优秀追赶者与创新者**

- **ChatGLM (智谱AI)**
    
    - **角色**: 国内最早、最有影响力的开源对话模型之一。
        
    - **独特改变**:
        
        - **GLM (General Language Model) 架构**: 它在预训练时，没有采用标准的Decoder-Only的Causal Mask，而是采用了一种更灵活的掩码策略，混合了不同类型的任务（有点像T5的思想），理论上能更好地统一理解和生成。
            
        - **多语言原生支持**: 从一开始就对中英双语有很好的支持。
            
        - **自定义Tokenizer**: 拥有自己设计的、对中文更友好的Tokenizer。
            
- **Qwen (通义千问, 阿里巴巴)**
    
    - **角色**: 国内目前综合实力最强、最全面的开源模型家族之一。
        
    - **独特改变**:
        
        - **架构上的“集大成者”**: 它的核心架构非常类似Llama，但也吸收了其他模型的优点，比如可能使用了更优的Norm层或激活函数变体。
            
        - **强大的多模态原生能力 (Qwen-VL)**: 它的视觉语言模型不是简单的“嫁接”，而是在预训练阶段就进行了大量的图文联合训练，使其图文理解能力非常出色。
            
        - **超长上下文**: Qwen很早就推出了支持长上下文的版本，在技术上跟得很紧。
            
- **DeepSeek (深度求索)**
    
    - **角色**: **代码和数学领域的“新贵”**。
        
    - **独特改变**:
        
        - **数据配比的极致优化**: DeepSeek的成功，很大程度上归功于其高质量的**代码和数学数据**的预训练。他们在数据清洗和配比上下了巨大功夫。
            
        - **证明了数据质量 > 数据数量**: DeepSeek用相对更少的参数量（67B），在代码和数学能力上超过了很多更大规模的模型，有力地证明了**高质量、专业化的预训练数据**是通往强大能力的捷径。
            

#### **3. 最新的SOTA竞争者 (2023-2024)**

- **Gemini (Google)**:
    
    - **特点**: **原生的多模态 (Natively Multimodal)**。Google声称Gemini从一开始就是用图文音视频等混合数据进行预训练的，而不是像LLaVA那样先训练一个语言模型再“嫁接”视觉。这使得它的多模态融合能力理论上更强。架构上是标准的Transformer，但训练数据和方式是其核心机密。
        
- **Claude 3 (Anthropic)**:
    
    - **特点**: **超强的长上下文处理能力和“价值观对齐”**。Claude系列一直以其强大的长文本理解和更安全的、更符合“宪法AI”价值观的回答而著称。它的成功更多地体现在**对齐技术**和**高质量的训练数据**上，而不是架构的颠覆。
        
- **Kimi (月之暗面)**:
    
    - **特点**: **将“超长上下文”作为核心卖点的“黑马”**。它在国内率先推出了支持20万汉字（后来扩展到200万）的上下文窗口。
        
    - **技术推测**: 其核心技术很可能是在**注意力优化**（如滑动窗口、稀疏注意力）和**位置编码外推**（如YaRN的变体）上做出了突破。它证明了在特定能力上（长文本处理），一个专注的创业公司也能达到世界顶级水平。