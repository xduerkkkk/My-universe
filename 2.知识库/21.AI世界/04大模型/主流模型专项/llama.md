# 核心
decoder only
和传统transformer相比，有这么几个改动
layer norm 使用的是RMS Norm
激活函数采用SwiGLU
位置编码采用RoPE
tokenizer采用sentence Piece

那llama1、2、3都有哪些改进？
llama这种语料是英文拉丁文训练的模型，如何拓展中文词表的？
是国内谁要用，谁自己去中文sft全量微调吗？ 

# **1. Llama 1, 2, 3 的改进之路**

- **Llama 1 (2023.02)**: **“开源”世界的开创者**
    
    - **核心贡献**: **首次证明了在公开数据集上，用相对“较小”的规模（7B-65B），也能训练出性能逼近闭源巨头（如GPT-3）的模型。** 它的发布，彻底点燃了开源LLM的火焰。
        
    - **技术特点**: 奠定了RMSNorm + SwiGLU + RoPE这套黄金架构。
        
    - **训练数据**: 1.4T tokens，主要是公开的英文数据。
        
- **Llama 2 (2023.07)**: **更强、更安全、更长的上下文**
    
    - **核心贡献**:
        
        1. **性能更强**: 训练数据量增加到了**2T tokens**，质量也更高。
            
        2. **上下文窗口翻倍**: 从Llama 1的2048扩展到了4096。
            
        3. **引入GQA (Grouped-Query Attention)**: 在70B模型中首次使用了GQA，大幅提升了推理效率。
            
        4. **SFT与RLHF的全面对齐**: 发布了Llama-2-chat版本，通过大量的SFT和RLHF（超过100万人类偏好标注），使其对话能力和安全性**远超**第一代。这是它能成为“开源对话模型标杆”的关键。
            
    - **技术趋势**: 体现了行业从“只比预训练性能”转向“**更关注模型对齐和实际应用**”的趋势。
        
- **Llama 3 (2024.04)**: **极致的数据质量 + 更大的词汇表**
    
    - **核心贡献**:
        
        1. **数据为王**: Meta宣称其成功的**最大秘诀**在于数据。他们使用了**超过15T tokens**（是Llama 2的7倍多！）的、经过**极其复杂和严格的数据清洗、过滤、去重**的语料库进行预训练。他们甚至训练了专门的语言模型来帮助筛选高质量数据。
            
        2. **更大的Tokenizer**: vocab_size从32000扩展到了128,000。
            
            - **为什么？** 为了更高效地编码多语言（特别是提升非英语语言的效率）和代码。更大的词表意味着很多常见的词和亚词可以直接用一个token表示，而不是多个，从而**降低了序列长度，提升了处理效率**。
                
        3. **更长的上下文**: 基础模型支持8192的上下文，并且通过微调可以轻松扩展。
            
        4. **架构微调**: 在8B和70B模型中**全面采用了GQA**，进一步提升效率。
            
    - **技术趋势**: 再次印证了“**高质量、大规模的数据是模型能力提升的根本驱动力**”这一黄金法则。

# **2. 如何让Llama说好中文？—— “二次预训练”才是正解**

> “llama这种语料是英文拉丁文训练的模型，如何拓展中文词表的？是国内谁要用，谁自己去中文sft全量微调吗？”

**直接用中文SFT，是一个非常糟糕、效果很差的方法！**

- **为什么不行？**
    
    1. **Tokenizer不匹配 (根本原因)**: Llama的32k词表里，中文token**极少**。当你给它一句中文时，它会被Tokenizer拆成大量**单个的、无意义的字节Token**。比如“你好”可能变成6个Token。这导致输入的序列**极长**且**没有语义单元**。
        
    2. **模型“没见过”**: 模型的“大脑”（权重）在预训练时几乎没见过这些中文的字节组合，它不知道该如何处理它们。
        
    3. **SFT数据量太小**: SFT的数据量通常是百万级别，而预训练是万亿级别。想用百万级别的“家教”，去扭转一个接受了万亿级别“义务教育”的模型的语言基础，是杯水车薪。模型会学得很痛苦，最终效果也是“中式英语”的感觉，无法流畅地生成中文。
        
- **正确的策略：中文词表扩展 + 二次预训练 (Continual Pre-training)**
    
    1. **词表扩展 (Vocabulary Expansion)**:
        
        - 拿Llama原始的32k词表作为基础。
            
        - 准备一个高质量的、巨大的中文语料库。
            
        - 在这个中文语料库上，运行BPE/SentencePiece算法，**学习出**一批新的、高频的中文词元（比如3万个）。
            
        - 将这些新的中文词元**合并**到原始的32k词表中，形成一个新的、比如62k大小的**中英混合词表**。
            
    2. **扩展Embedding和LM Head层**:
        
        - 加载原始的Llama模型。
            
        - 它的embed_tokens和lm_head层的权重矩阵形状是[32000, hidden_size]。
            
        - 我们需要把这两个矩阵的行数**扩展**到[62000, hidden_size]。新增的3万行，可以用随机值或一些更复杂的方法初始化。
            
    3. **二次预训练 (Continual Pre-training)**:
        
        - 用这个修改过的新模型，在**海量的中文语料库**上，**继续进行预训练**（还是做“文字接龙”任务）。
            
        - 这个过程的目的是让模型：
            
            - 学习新增的那些中文词元的嵌入表示。
                
            - 让模型内部的Transformer Block**适应**中文的语法和表达方式。
                
    4. **最后才是SFT**: 在经过了充分的中文二次预训练之后，得到的模型才是一个真正意义上的“中英双语基座模型”。这时，再用中文的SFT数据对它进行微调，效果才会好


# **3. 仿Llama的都有哪些？**

几乎所有你听过的、在Llama之后发布的**开源Decoder-Only模型**，都在架构上大量借鉴了Llama。它们可以被称为“Llama-like”或“Llama-based”架构。

- **Mistral / Mixtral (法国)**: 可以看作是**Llama架构的直接继承者和优化者**。它们使用了同样的RMSNorm, SwiGLU, RoPE，但在Attention层引入了**Sliding Window Attention (SWA)**和**GQA**，并在MLP层引入了**MoE (Mixtral)**。它们在同等规模下，性能通常优于Llama。
    
- **Qwen (中国, 阿里巴巴)**: 核心架构与Llama非常相似，但在多模态能力和Tokenizer上做了很多自己的优化。
    
- **DeepSeek (中国, 深度求索)**: 核心架构也是Llama-like，但其强大的代码和数学能力主要来自于其**独特的、高质量的预训练数据**。
    
- **Baichuan (中国, 百川智能)**: 同样是基于Llama的架构进行改进和中文本地化。
    
- **Gemma (Google)**: Google推出的开源模型，其架构设计也深受Llama影响，采用了类似的组件