#### **RoPE是怎么做的？**

1. **表面上的“绝对”操作**：对于任意一个词向量 x（比如 q 或 k），如果它在第 m 个位置，RoPE会用一个**只跟 m 有关的旋转矩阵 R_m** 去乘以它，得到 R_m * x。这看起来非常“绝对”，因为每个位置 m 都有一个确定的旋转操作 R_m。
    
2. **内在的“相对”魔法**：奇迹发生在计算注意力分数的时候，也就是计算 Query 和 Key 的点积时。  
    假设 Query 在位置 m，Key 在位置 n。我们需要计算的点积是：  
    < R_m * q, R_n * k >
    
    根据旋转矩阵的数学性质，这个点积的结果，经过数学推导，可以被证明**只与 q, k 以及它们的位置差 m-n 有关**，而与 m 和 n 的绝对值无关！
    
    换句话說，pos=3 的token和 pos=5 的token之间的注意力关系，与 pos=10 的token和 pos=12 的token之间的注意力关系，在位置编码层面是**完全一样**的，因为它们的相对距离都是2。
    

**这就是RoPE的精髓：通过对绝对位置进行“旋转”操作，使得向量在进行点积运算（注意力计算的核心）时，其结果只依赖于相对位置。** 它用一种绝对的形式，实现了相对的目的。