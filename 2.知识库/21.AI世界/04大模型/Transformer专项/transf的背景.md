# RNN、LSTM
早期我们怎么进行自然语言理解？
最基本的是，条件概率。下一个词的概率是...
RNN呢 第一次长序列
输入向量  输入到隐藏层 接着 下一个环节，  隐藏层和上面的输入都传入过来
分别乘上权重 
所以核心是利用隐藏层理解信息， 利用“层层传入”的动作传递信息。
那么梯度消失，梯度爆炸都是有可能的。
梯度爆炸可以梯度裁剪，梯度消失没办法了，序列越长，信息传递距离越长，这就是结构决定的必然要梯度消失。
LSTM怎么改进？
首先，LSTM一直保持记录的一个cell state，C 是LSTM单元的内部状态，它从上一个时间步传递到下一个时间步。
那LSTM到底多少输入？ 首先一定是有一个基础的，当前读到的词向量，x，还有上一步的激活值，我们称上一步的知识标签，ht-1向量传入。首先，LSTM会把xₜ和hₜ₋₁**拼接**在一起，形成一个更长的向量 [hₜ₋₁, xₜ]。: 然后，这个混合后的长向量，会**同时**被送去乘以**四个不同**的权重矩阵，从而计算出：
- **遗忘门的信号 fₜ**
- **输入门的信号 iₜ**    
- **输出门的信号 oₜ**
- **候选记忆 gₜ** 

Cₜ = fₜ * Cₜ₋₁ + iₜ * gₜ
- **Cₜ**: 这是我们**当前时刻 t**，马上要写入主笔记本的**新内容**。
    
- **fₜ * Cₜ₋₁**:
    
    - Cₜ₋₁: 是主笔记本上**上一时刻的旧内容**。
        
    - fₜ: 是**遗忘门**的输出（一个0到1之间的向量）。
        
    - *: 是逐元素相乘。
        
    - **翻译**: “看一眼旧的笔记Cₜ₋₁，然后用遗忘门fₜ这支‘修正液笔’，决定**哪些部分要被保留（乘以接近1），哪些部分要被遗忘（乘以接近0）**。” 这就是**选择性遗忘**。
        
- **iₜ * gₜ**:
    
    - gₜ: 是我们根据当前输入新写下的**笔记草稿**（你说的“知识”）。
        
    - iₜ: 是**输入门**的输出（一个0到1之间的向量）。
        
    - **翻译**: “看一眼新的笔记草稿gₜ，然后用输入门iₜ这支‘荧光笔’，决定**草稿里的哪些重点内容，值得被正式写入主笔记本**。” 这就是**选择性写入**。
        
- **+**:
    
    - **翻译**: “最后，把**‘遗忘完旧信息后剩下的部分’**和**‘筛选完新信息后要写入的部分’**，**加在一起**，就形成了我们主笔记本在当前时刻的最终内容Cₜ。”
所以输入应该是4个，三个门控信号与一个知识？

transformer是他们的升级，因为他们的应用一模一样！ 对于一对多，逐步解码， 最最原始的是不是就是ngram（我忘了怎么说），最基础的条件概率
无论是这俩循环神经网络还是transformer，都利用的是它处理每一个词向量的能力， 刚才将的机器翻译，其中的encoder的思想向量，不就是刚才说的所谓“多对一”吗，仍然是用网络生成整个句子的向量，句子的向量是基于对每一个词的词向量的准确表达的！ 然后decoder阶段，就是纯粹的利用对词向量的理解， 因为这些模型对词向量的理解是能照顾到上文的，所以我们利用这个理解， 去经过lmhead 投影到词库维度，去取概率分布，得到下一个词。也就是他们的词向量，还蕴含着“下一个词是什么“的信息，于是我们利用它decoder