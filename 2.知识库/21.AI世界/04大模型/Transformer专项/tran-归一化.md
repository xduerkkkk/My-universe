归一化有三层
- 数据预处理：调整特征关系，避免模型误解
- 网络层：作用在激活值，是为了激活值的稳定，最终目的是稳定训练过程，加速模型收敛
- 输出层：为了转化输出，把输出变成特定形式，以达到某种目的


# RMSNorm
![[归一化-1756356923020.jpeg]]

Meta的LLaMA系列模型（ 7B~65B参数 ）均采用RMSNorm，验证了其在大规模场景下的有效性。

# norm公式
计算均值，计算方差， 使用x-均值 / 根号下方差＋极小数  即标准归一化
缩放平移 

# 归一化为什么使用layernorm而不是batchnorm？

首先明确的，一个维度，如果相对大小有意义，那归一化也是有意义的，反之则无意义。 batchnorm是一批样本的同一维度归一化，
根本在nlp里毫无意义啊 ，举例子一个batch有3个样本
为中华之崛起而读书;我爱中国;母爱最伟大
我们要对”为“，”我“， ”爱“ 做归一化 没有什么意义。
而layernorm，是专门针对”为中华之崛起而读书“这一个长句，做归一化 
### **回答一：为什么BatchNorm在CV中有效，在NLP中无效？—— 特征的“对齐”与“独立”**

我们可以从**特征维度（Channel/Feature Dimension）的意义**来解释。

- **在CV (图像) 中 - 特征是“对齐”的**:
    
    - **一个特征维度代表什么？** 对于一个[Batch, Channel, Height, Width]的图像张量，一个“特征维度”通常指的是**Channel**维度上的一个特定位置。比如，所有图片在(c=0, h=5, w=5)这个位置的值。
        
    
    - **为什么归一化有意义？** 因为对于一个训练良好的数据集（比如ImageNet），一个batch里所有图片在(c=0, h=5, w=5)这个位置的特征，它们**在统计上是相关的**。这个特征可能代表了“物体左上角的边缘”或者“某种纹理的响应”。它们服从一个有意义的统计分布。对这个分布进行归一化（BatchNorm），可以让模型的学习更稳定。**Batch里的样本，共享了空间的结构性。**
        
- **在NLP (文本) 中 - 特征是“独立的”**:
    
    - **一个特征维度代表什么？** 对于一个[Batch, SeqLen, HiddenSize]的文本张量，BatchNorm归一化的维度是Batch维度。这意味着，它会对**所有句子的第i个词的第j个HiddenSize维度**进行归一-化。
        
    - **为什么归一化没有意义？** 正如你所说，第一个样本的第i个词是“为”，第二个样本的第i个词是“我”，第三个是“母”。这三个词在语义上**风马牛不相及**。它们的嵌入向量在第j个维度上的值，没有任何理由要被归一化到一个共同的均值和方差。这样做就像把“姚明的身高”、“马斯克的财富”和“珠峰的氧气含量”这三个完全无关的数字，强行计算一个平均值，这是**毫无意义的统计**，只会破坏每个词独立的语义信息
### **回答二：BatchNorm在NLP中的“工程性”硬伤**

我们seqlen每次都不一样啊。而且在模型推理优化领域，我们常常是使用动态的batching策略，

这是另一个同样重要的原因，特别是在Transformer和LLM的时代。

1. **对Batch Size的依赖**:
    
    - BatchNorm的有效性，**高度依赖于一个足够大的、稳定的Batch Size**来估算出有代表性的均值和方差。
        
    - 在NLP任务中，由于句子长度**变化极大**，为了避免大量的padding浪费显存，我们经常使用**动态的Batching策略**（把长度相近的句子凑成一个batch）。这导致batch size可能时大时小，估算出的统计量**非常不稳定**，反而会损害模型性能。
        
2. **训练和推理的不一致 (The Killer Flaw)**:
    
    - **训练时**: BatchNorm使用当前mini-batch的均值和方差。
        
    - **推理时**: 我们通常一次只处理一个句子 (batch_size=1)。此时**无法计算batch的均值和方差**！
        
    - BatchNorm的解决方案是，在**训练时**，用一个**移动平均（moving average）**来估算一个“全局”的均值和方差，然后在**推理时**使用这个估算出的全局值。
        
    - **问题在于**: 对于语言这种千变万化的东西，用一个固定的、全局的统计量，在推理时去归一化各种各样的新句子，效果通常很差。训练和推理之间的这种**巨大差异**，是它在NLP中表现不佳的一个核心原因。
        



#  Prenorm和Postnorm
![[归一化-1757215916158.jpeg]]
关键区别就是，norm包含的，是纯粹的xn，还是残差+被模型理解过的xn

postnorm，会削弱残差带来的稳定性效果  削弱残差连接的权重   但性能潜能高
prenorm训练稳定性高
