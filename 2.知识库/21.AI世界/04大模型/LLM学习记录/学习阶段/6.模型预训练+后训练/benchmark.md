benchmark，基准测试
相当于专门给模型出的考题
考题在这里就是数据集，得分呢？是通过设计评测指标
比如“Qwen2-7B 在 MMLU 上得分 75.2”，意思就是它在这个涵盖57个学科的综合知识测试中，准确率达到75.2%，说明其知识覆盖面广、表现优秀。
# 常见的评测集
## 综合类
### MMLU (Massive Multitask Language Understanding)
57个学科领域的知识掌握，包括STEM、人文社科、专业领域等
涵盖初高中到专业水平的问题
评估模型的通用知识水平
## 数学推理类
### MATH
- 包含12,500道高中数学竞赛题
- 需要生成解题步骤
- 评估模型的逻辑推理能力
与之类似的还有 GSM8K
## 编程能力类
### APPS
评测算法竞赛级别编程
- **特点**：
    - 包含编程竞赛题目
    - 难度高，需要复杂算法思维
    - 评估模型解决复杂问题的能力
### HumanEval
评测函数级代码生成
## 中文能力类
### C-Eval
- **评测重点**：中文知识与推理能力
- 包含专业领域和基础学科的选择题
### CMMLU
中文语言理解
- 包含多个子任务（分类、匹配、阅读理解等）
# 评测指标
针对不同的数据集应该有不同的评测指标
但是有一组概念是相通的，这组概念在任何形式的模型评测都可能会出现
首先定义清楚，“正“和”负“ 是 真实情况得分两个对立面，是模型给出的结果 ，”真”和“假”都是实际的情况，是我们人类自己判断出来的
- True Positive TP 真正例  机器认为是正，实际上机器认为的没问题
- False Positive FP 假正例， 机器认为是正，但实际上是“负”，也就是这个正是假的
- False Negative FN 假负例，机器认为是负，但实际上是“正” 
- True Negative TN 真假例  机器认为是负的，实际上机器认为的没问题
- 精确率，precision，P    $TP/Tp + FP$  在所有机器判断为正的例子中，判断得没问题的有哪些
- 召回率，recall，R   $TP/Tp + FN$ 在所有实际上为正的例子中，判断的没问题的有哪些
- F1分数    $2*(P*R/P+R)$
举例子
- 测试集 200 封邮件，其中 50 封为垃圾邮件。
- 模型预测结果：
    - 预测为垃圾邮件 60 封，其中 40 封真的是垃圾邮件（TP=40，FP=20）。
    - 漏掉 10 封垃圾邮件（FN=10）。

| 指标  | 计算                        | 结果     |
| --- | ------------------------- | ------ |
| 精确率 | 40 / (40+20)              | 66.7 % |
| 召回率 | 40 / (40+10)              | 80 %   |
| F1  | 2×0.667×0.8 / (0.667+0.8) | 72.7 % |
## 选择题
**不是问模型"答案是什么"，而是问"如果答案是A，可能性多大？"**

```
prompt = "2+2="
options = ["1", "2", "3", "4"]
```
把答案当成填空，计算模型输出每个空的概率是多少。
然后使用accuracy 答对的题/全部的题 当作指标
## 文本生成题
### n-gram
最初是是衡量文本与参考译文的相似度
1-gram就看1个词，匹配几个
2-gram就看连续两个词，匹配几组
- 例如：生成文本"the cat is on the mat" vs 参考文本"the cat sits on the mat"
    - 1-gram匹配：the, cat, on, the, mat → 5个匹配（总6个）→ 5/6
    - 2-gram匹配：the cat, cat is, is on, on the, the mat → 2个匹配（总5个）→ 2/5
我们以精准率，如5个匹配（总6个）→ 5/6这个分数，作为指标。
还要加入短句惩罚（BP），因为这个规则下，模型显然输出越短得分可能越高

```
BP = 1 if c > r else exp(1 - r/c)
```

最终BLEU分数举例

```
生成文本："the cat is on the mat" 
参考文本："the cat sits on the mat"

1-gram精确率：5/6 ≈ 0.83 2-gram精确率：2/5 = 0.40 3-gram精确率：1/4 = 0.25 4-gram精确率：0/3 = 0.00

BP = 1（因为长度刚好一样）

BLEU = 1 * exp((log(0.83) + log(0.40) + log(0.25) + log(0.00))/4) ≈ 0.40（实际计算需处理0值）

```

显然，BLEU对不同词序和同义词很不友好，有点死板
## BERTScore
使用bert识别 能改善同义词问题
计算精确率 召回率 F1

## 推理题
推理步骤、思维链格式、标签的正确存在


# 评测框架
lm-evaluation
指定model='hf' 和模型路径
本地有了加载本地，没有的话去huggingface网站找，和.from_pretrain方法是一样的

 当指定 model='hf' 和模型路径时，框架内部实际上是调用了 transformers 库的 AutoModel.from_pretrained() 和 AutoTokenizer.from_pretrained()。这也是为什么，如果评测自己的模型，注册自定义模型如此重要的原因，因为它就是在这个环节让 transformers 认识我们的模型。
