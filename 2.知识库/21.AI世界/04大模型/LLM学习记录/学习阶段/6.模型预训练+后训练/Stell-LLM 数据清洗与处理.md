# 使用的数据
开源数据，huggingface就有
如中文维基数据、百度百科数据等
# 数据处理
比如我们拿到的数据是中文维基过滤数据、百度百科数据，记作A、B
A与B自己需要进行**清洗与过滤**，此时他们自身是无重复的。
然后我们使用A和B。但A和B两两比较可能也有重复，所以要**二次过滤**
或称全局去重

## 清洗与过滤
#### 质量过滤
- 启发式规则：
- 分类器：用高质量数据源训练一个模型，来筛选数据质量
#### 数据去重
MinHash，与局部敏感哈希（LSH）结合使用，可以高效地处理海量数据，并找出内容相似度高的文档对。LLaMA等知名模型的预训练数据都采用了此方法
#### 过滤敏感内容
类似质量过滤的分类器，使用在特定有毒言论数据集（如Jigsaw提供的评论数据）上训练好的分类器，来识别并剔除含有暴力、色情、仇恨言论的文本。

##  二次过滤
先把A、B的格式统一
最后预训练文本格式为

```
{"text":"电子荒原"}
```

然后进行data juicer数据过滤

算子如这些

|算子|描述|
|:--|:--|
|chinese_convert_mapper|用于在繁体中文、简体中文和日文汉字之间进行转换（借助 opencc）|
|clean_email_mapper|删除邮箱信息|
|clean_html_mapper|删除 HTML 标签并返回所有节点的纯文本|
|clean_ip_mapper|删除 IP 地址|
|clean_links_mapper|删除链接，例如以 http 或 ftp 开头的|
|clean_copyright_mapper|删除代码文件开头的版权声明 (:warning: 必须包含单词 copyright)|
|expand_macro_mapper|扩展通常在 TeX 文档顶部定义的宏|
