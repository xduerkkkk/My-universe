# 1.请简述下PPO算法
强化学习中，我们要优化智能体的决策。ppo就是一种策略梯度算法。这个算法就是一个损失函数，但这个损失函数包括了数据采集、计算奖励和优势。能让智能体更新策略的时候，步子不要迈太大，从而使训练过程更稳定，通过“截断”的技术来实现。具体来说，它会计算新旧策略对同一个动作输出概率的比值。  如果这个比值试图变得太大（说明策略想变得非常激进），PPO就会通过一个‘截断’操作，限制住这个更新的上限，防止步子迈得太大。

## 2.介绍一下基于人类反馈的强化学习流程
首先，能应用RLHF的模型应该是经过预训练和SFT的。接着我们让SFT后的模型，针对同一个问题给出多个不同的答案，人类标注员基于生成的答案进行排序，一个值得注意的点是 是排序不是打分，这样更容易。我们就能得到人类偏好的数据，比如说排序的第一个。然后基于人类偏好数据，训练一个奖励模型，它的输入是问题+回答，它的输出是分数。我们要训练得，让模型识别到人类偏好的回答时给分更高。 最后，我们将奖励模型当作打分专家，让我们的目标模型，针对问题生成回答，然后我们将问题+答案喂给奖励模型打分，使用ppo算法，让算法根据分数，调整语言模型的参数。
# 3.奖励模型的数据收集要满足什么要求?

明确数据集标准，即能准确模型人类复杂偏好。所以我们需要高质量、高一致的人工标注，采用比较数据而非绝对评分，数据需要多样性、足够的数据量。
# 4.奖励模型是如何训练的，它的损失函数是什么?
首先，模型架构是强大的预训练模型，或sft模型为基座，我们去掉原来的语言生成头，换上回归头，即一个简单的线性层，作用是将模型最后一层输出的复杂特征向量压缩成数值，即分数。
也就是，输入qa对，输出一个分数。
训练过程是这样的，取出数据中一个比较对，也就是问题，和两个回答，好回答和差回答，那么一共有两组qa，将输出两个分数，我们希望好回答概率更高，sigmod（好回答分-差回答分）能近似表示成“好回答分数更高的概率”，这个概率在0到1，我们希望概率越大损失越小，概率为1损失为0，概率越小损失越大，-log函数更好符合。于是损失函数就是-log（sigmod（好回答分-差回答分））
![[面试作业-1753593604306.jpeg]]
# 5.目前RLHF 方法有没有什么缺陷?如何改进
在收集奖励模型数据过程中，人类标注仍然面临昂贵、不一致性的风险。在奖励模型使用的阶段，存在**Reward Hacking**，即过度优化。因为当奖励模型被训练时，人类标注者往往倾向于给更详细、更长的回答更高评分，这导致奖励模型错误地将"长度"与"质量"相关联，PPO算法发现增加回答长度是提高奖励的最可靠方式，ppo就倾向于把模型训练啰嗦了。
改进：第一个数据收集缺陷，可以使用ai宪法和RLAIF，这是claude的训练方法。第二个reward hacking问题，可以在奖励函数里加入长度惩罚项，dpo，直接偏好优化，不依赖奖励模型，而是根据偏好数据直接学习，类似sft，避免了奖励模型将长度与质量错误关联的问题，没有“回答长--得分高”这个流程了。
https://zhuanlan.zhihu.com/p/1894073158924993213
# 6.介绍一下LLM的直接偏好优化(DPO)
dpo是强化学习的一个算法，是传统的基于ppo强化学习的一个替代方法。与ppo类似的是，仍然保留一个未训练的sft模型，当作指标，**Reference Model**，不让模型训练地太夸张。接着就是训练过程了。
输入仍然是人类标注/或ai标注的偏好数据对，dpo算法会计算模型输出好回答和坏回答的概率，优化损失函数的过程就是让回答好回答概率更高。所以类似更复杂的sft
<font color="#f79646">PPO的损失函数是为了稳定地“追逐”RM给出的奖励。DPO的损失函数是为了直接“拉开”好坏答案的概率差距</font>
# 7.LLM训练中，近端策略优化包含哪几个模型?
四个模型
两个一直在训练的模型
actor model，生成回答，是优化主体
critic model ， 评估状态价值，帮助计算优势
两个冻结的模型
reward model ，奖励模型，打分
reference model ， 基准模型，是actor最初的样子，作用是不让actor训练地夸张或忘了说人话

# 8.什么是RLAIF?
首先准备一套数据偏好的原则，比如要合法、精简...等，基于这个原则，使用强大的sft模型，训练一个ai裁判，具体方法是，先诱导模型生成有害回答，再让模型根据法则，自己批判和修改刚生成的有害回答，修正成无害回答，重复成千上万次，就有了ai裁判。
下面拿出我们要训练的模型，让模型生成回答对，接着让ai裁判去生成偏好数据，这里就代替了人工标注。偏好数据有了，下面进行ppo或dpo都可以。

# 9.与有监督学习相比，强化学习能够给大语言模型带什么哪些好处?
有监督学习，在训练过程中是给模型了唯一的标准答案，让它去学。所以无法让模型学习某种风格，或性质这种偏感觉的，模糊的东西。模型没有真正学习，比如喂给模型大量“不要说有害内容”的例子，但永远无法穷尽所有可能有害的提问。模型只是在模仿，它不理解“为什么”这是有害的。当遇到一个它没见过的新型有害问题时，它很可能依然会犯错。而RL则通过奖励信号，学会人类偏好，将“感觉”变成可以优化的东西，让模型学会思考“什么样的数据是人类偏好的”，它能面对新情况，而不是sft的死记硬背
# 10.介绍一下RLHF中PPO微调过程
四个模型
两个一直在训练的模型
actor model，生成回答，是优化主体
critic model ， 评估状态价值，帮助计算优势
两个冻结的模型
reward model ，奖励模型，打分
reference model ， 基准模型，是actor最初的样子，作用是不让actor训练地夸张或忘了说人话
准备完模型后正式开始流程
首先，让从问题库里抽取问题，让actor模型生成答案，从而我们有了qa对，接着，我们对数据进行评估，reward model进行给分，critic模型判断这个回答的预期，reference模型体现在kl散度乘法，不让actor离最初的样子太远。最后便是基于现在的状态，和损失函数，进行优化，更新actor和critic。
# 11.你了解DeepMind提出的ReST对齐算法吗?
**Reinforcement Learning from Sequence-level Training**，即**基于序列级训练的强化学习**
核心是让模型反复学习自己最好的输出。
仍然有个奖励模型
流程是这样:
目标模型生成，奖励模型打分，然后我们把好的问答筛选出来，关键来了，传统强化学习此刻就基于损失函数，优化目标模型参数了。但这个方法先不急，而是先把优质问答取出来，创建新的训练数据集，此时这个数据集算是“优质数据集”，接着进行传统的SFT微调，目标模型得到优化。
然后开始迭代，继续回到目标模型生成，奖励模型打分。



# 其他的记录
#### **1. 什么是 On-Policy (同策略)？**

- **定义**：On-Policy 算法指的是**产生数据的策略**和**被评估与改进的策略**是同一个。
    
- **通俗理解**：想象一个学生正在学习如何考试。
    
    - **On-Policy 学生**：他用自己当前的知识水平（策略A）去做一套模拟题（收集数据）。然后，他根据这套模拟题的对错情况，总结经验，提升自己的知识水平，得到一个新的知识水平（策略B）。下一次，他必须用这个新的知识水平（策略B）去做一套**全新的**模拟题，再来学习。他永远是用“现在的自己”去练习，然后变成“更好的自己”。
        
- **核心特点**：**现学现用，用完就扔**。用来学习的数据是当前策略采集的，一旦策略更新了，这些老数据就不能再用来训练了，必须丢弃，然后用新策略去采集新数据。
    

#### **2. 什么是 Off-Policy (异策略)？**

- **定义**：Off-Policy 算法指的是**产生数据的策略**和**被评估与改进的策略**可以是不同的。
    
- **通俗理解**：
    
    - **Off-Policy 学生**：他不仅可以用自己做的模拟题来学习，他还可以拿来**学霸同学做过的模拟题**（甚至是去年高考状元做过的题）来学习。他可以观察学霸在遇到某个难题时是如何选择解法的（学霸的策略），并从中学习，来改进自己的知识水平。
        
- **核心特点**：**经验回放 (Experience Replay)**。算法可以把过去（甚至是很久以前）由各种旧策略采集的数据都存放在一个“经验池”里。在训练时，可以反复从这个池子里拿出数据来学习。
- 





    

#### **3. 什么是 Data Utilization (数据利用率)？**

- **定义**：数据利用率，也叫样本效率 (Sample Efficiency)，衡量的是一个算法从每个数据样本中能学到多少东西。或者说，要达到同样的性能，需要和环境交互多少次。
    
- **On-Policy vs. Off-Policy 的数据利用率**：
    
    - **On-Policy (如 PPO)**：数据利用率**低**。因为它每次更新完策略，之前辛辛苦苦从环境中采集到的一批数据就作废了，必须重新采集。这在真实世界中代价可能很高（比如训练机器人，每次交互都很耗时耗电）。
        
    - **Off-Policy (如 DQN)**：数据利用率**高**。因为它可以反复利用经验池里的旧数据进行训练，一个样本可以被用来更新很多次策略，所以非常节省“和环境交互”的成本。