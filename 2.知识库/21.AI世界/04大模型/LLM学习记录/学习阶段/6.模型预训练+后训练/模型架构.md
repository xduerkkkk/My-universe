# 模型参数
基础自pretrainedconfig  可以无缝衔接transformer的训练架构
## 基础部分
- dropout：防止过拟合
- bos和eos_token_id： 给模型提个醒，这个数字是比较特殊的“开始”与“结束”（为啥要给模型说？）
- hidden_act:激活函数，决定FFN中神经元的激活方式
- hidden_size:即模型训练中的d_model，维度大小代表训练过程中的参数大小
- intermediate_size: 中间层大小
- num_attention_heads：“多头注意力”部分的“头数”
- num_hidden_layers: transformer架构的层数，整个架构！就是重复num_hidden_layer次的架构熏训练
- vocal_size: 使用tokenizer的词汇表的量（为啥要让模型知道？）
- rms_norm_eps: 全新的归一化模式，代替layernorm
## 专家部分
- num_experts_per_tok : 每次分配给专家时，分配给得分前k名的专家
- n_routed_experts: 所有“门诊”的专家数量
- n_shared_experts:全才专家的数量
- scoring_func:  比如softmax，是初始随机化分配专家时使用的，根据概率的topk选取送往的专家
- aux_loss_alpha: 一种权衡，值越大，越能监督专家接任务的均衡程度
- seq_aux:对一个batch中所有Token的专家选择情况，一起计算一个总的负载均衡损失。
- norm_topk_prob:是否对Top-K个专家的概率进行归一化





# 部件


## RMSNorm

用于归一化，相当于layernorm，但是为最新版


## RoPE

位置编码，llama首次用

## Attention

注意力机制
token经过此模块，更能理解语义
### kv_cache
存储之前的kv，加快效率





## FeedForward


多个线性层，以及升维再降维 
token相当于整合思考

## MoEGate
针对输入进来的token，给专家分配权重，从而决定给把问题抛给哪些专家
返回的是就诊的专家

# MOEFeedForward

首先调用MoEgate，确认就诊专家
然后分为训练模型和推理模型，看开的哪种模式

#### **训练模式（repeat_interleave + 布尔掩码）**

- **计算图的样子**: 这条路非常“平坦”和“连续”。
    
    1. x -> repeat_interleave -> x_repeated (可导)
        
    2. x_repeated -> 布尔掩码选择 -> expert_input (可导)
        
    3. expert_input -> expert() -> expert_output (可导)
        
    4. expert_output -> 写入y (可导)
        
    5. y -> reshape -> view -> 加权求和 (全部可导)
整个流程都是由基础、连续、可导的PyTorch操作构成的。Autograd可以毫不费力地沿着这条路反向追溯，计算出损失对于每个专家（expert）和门控（gate）权重的梯度。虽然计算过程有些冗余，但“权责清晰”，梯度路径一目了然。
#### **推理模式（argsort + scatter_add_）**

- **计算图的样子**: 这条路充满了“断崖”和“传送门”。
    
    1. **argsort()**: **这是一个“断崖”**。排序操作本身是**不可导**的。一个值的微小变化，可能会导致它在排序中的位置发生剧烈跳变。例如，[3.0, 3.001] 排序后索引是 [0, 1]，但 [3.0, 2.999] 排序后索引就变成了 [1, 0]。这个剧烈的、不连续的变化使得梯度无法定义。Autograd走到这里，就不知道该如何向后传播梯度了。
        
    2. **bincount()**: 同样是基于整数索引的操作，不可导。
        
    3. **scatter_add_**: 这是一个原地（in-place）操作，它会修改张量的内容。虽然在某些情况下可导，但它的梯度计算比简单的矩阵乘法要复杂得多，并且与argsort结合使用时，梯度路径已经“断裂”了。
        
- **对Autograd的友好度**: **极低**。因为argsort的存在，整个计算图在关键步骤就断开了。PyTorch无法知道，如果专家的权重发生了一点点变化，导致最终损失变小了，这个“功劳”该如何分配给那个导致了排序变化的原始输入。梯度流被阻塞了。
    

---

# MinimindBlock
归一化
attention
加残差连接
前馈网络
归一化
前馈网络
残差连接
其中残差连接把传统的，单纯的feedforward，变成moefeedforward

# MiniMindModel
让输入的token经过n次minimindblock
输出序列化文本
# MiniMindCausaLLM
经过艰难险阻，层层设计，最终的minimindmodel也只是普通的pytorch模型，
我们现在把它封装成huggingface大语言模型

```
class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
```
- **模型加载与下载**: 可以使用.from_pretrained()方法，轻松地从Hugging Face Hub上下载和加载预训练好的模型权重。
    
- **模型保存**: 可以使用.save_pretrained()方法，将模型（包括配置文件和权重）保存成标准格式，方便分享和上传。
- 模型输出：你只需要调用一个简单的.generate()方法，GenerationMixin就会在背后帮你处理所有复杂的解码循环、KV Cache传递等工作

minimindmodel设计的序列化文本，是高维的，
也可以被利用于
MiniMindForTokenClassification
MiniMindForSequenceClassification
本次任务我们是应用于CausaLLM
是将高维的变成低维，再与之前的低维labels计算损失，反向传播