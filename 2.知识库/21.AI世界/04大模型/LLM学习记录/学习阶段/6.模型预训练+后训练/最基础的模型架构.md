我们使用pytorch来搭建模型
其中是4个核心“基石“：
## Tensor
也称张量，就由它存储数据
- 形状：不同的形状代表不同的维度，transformer架构中，维度一般为（batch_size, seq_len, hidden_size)
- 数据类型：float32、bfloat16f等，代表计算精度与效率
## nn.Moudle
任何一个部件都需要继承nn.Moudle
继承后，就能实现
- 参数自动追踪：任何有参数的东西，都能追踪并进行后续操作，如反向传播
- 状态管理：.train和.eval可以切换模块状态
- 标准化：state_dict 将模型权重或其他状态保存下来，load_dict加载模型权重
## Autograd
只要tensor设置了属性 require_grad=True 就能记录前向传播
显式调用autograd的能力，需要写下loss.backward()
接着，pytorch的autograd能力，会沿着记录好的计算图，从后往前，利用**链式法则**，自动计算出损失对于每一个被“监视”的参数的**偏导数（梯度）**。
## Optimizer
Autograd幕后完成了求导
真正的反向传播我们使用optimizer来显式地调用
1. **optimizer.step()**: 当你调用这个方法时，优化器会遍历它所管理的所有参数，然后根据Autograd计算出的梯度，结合自身的更新规则（比如AdamW会考虑动量），对每一个参数的数值进行**真 
2. **optimizer.zero_grad()**: 在完成一次更新后，优化器会把所有参数的梯度清零，为下一次loss.backward()做准备。