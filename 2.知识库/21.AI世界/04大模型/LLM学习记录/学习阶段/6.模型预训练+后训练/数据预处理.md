---

jupyter:

  jupytext:

    text_representation:

      extension: .md

      format_name: markdown

      format_version: '1.3'

      jupytext_version: 1.17.2

  kernelspec:

    display_name: ai_proj

    language: python

    name: python3

---

  

# 预备知识

什么样的数据大模型是看的懂的？

自然语言能看懂吗，肯定不行，我们要翻译成数字嘛！

但是这个数字也是有讲究的。

究竟把这些文本变成怎样的数字，详细看tokenizer那部分的讲解



这里我们简要地说明，tokenizer就是用来把刚才我们能看懂但机器看不懂的文本，变成机器能看懂我们看不懂的数字。

同时，要明白tokenizer处理文本后，所产出数据的结构：

- input_ids

- token_type_ids

- attention_mask

这些都是数字，如input_ids=[101, 7592, 102, 0, 0] 是我们输入给模型文本，翻译成的机器语言

token_type_ids 标识 Token 属于哪个句子（用于区分句子 A 和句子 B，如 BERT 的双句子输入）

attention_mask告诉模型哪些位置是真实Token，哪些是填充符（Padding）

  



  

简单理解，tokenizer把文字转化为数字，转化为的就是token

我们可以查看model文件夹里tokenizer.json文件，

里面有词典，即文字与数字的映射

比如特殊字符<|im_end|>  就被映射成 ‘2’

  

```python

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('../model/')

```

  

```python

text = "Hello<|im_end|>World"

input_ids = tokenizer.encode(text)

input_ids

```

  

```python

encoding  = tokenizer( "Hello<|im_end|>World. I’m man")

encoding

```

  

当然，tokenizer能填很多参数

其中'padding'  使得无论如何，都有预定的长度。就那么多字，你让我那么长？那就补0补齐。

现在我们使用预训练数据举例

  

```python

sample = test_data[1]

sample

```

  

```python

encoding_2 = tokenizer(

                     sample['text'],

                     max_length=512,

                     padding='max_length',

                     truncation=True,

                     return_tensors='pt')

  

```

  

```python

encoding_2

# 有没有注意，往上翻一下encoding，看和这个有什么区别，都是哪些参数造成的？

```

  
  

loss_mask：

  

生成一个布尔掩码，指定哪些位置的预测需要计算损失，哪些位置忽略。

这是可以自定义的，attentionmask是定死的padding位置忽略，

attention_mask 控制模型“看哪里”，但无法阻止损失函数计算某些位置的梯度。

例如，若 pad_token_id=0，则 input_ids=[101, 7592, 102, 0, 0] → loss_mask=[True, True, True, False, False]。

  

```python

  

loss_mask = (input_ids != tokenizer.pad_token_id)

```

  

大模型的核心逻辑是预测！我们要让模型学会预测下一个词

input_ids = [A, B, C, D]  # 原始序列

  

*构造训练对*：

X = [A, B, C]   # 输入序列 (位置 0~2)

Y = [B, C, D]   # 目标序列 (位置 1~3)

  

*模型训练任务*：

- 输入 A → 预测 B ✅

- 输入 A,B → 预测 C ✅

- 输入 A,B,C → 预测 D ✅

  

```python

X = torch.tensor(input_ids[:-1], dtype=torch.long)

Y = torch.tensor(input_ids[1:], dtype=torch.long)

loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)

```

  

```python

X

```

  

```python

Y

```

  

```python

loss_mask

```

  

.squeeze()方法return_tensor这个参数有关
# 预训练

  
  

模型是怎么会说人话的呢？

借助海量数据做预训练而学

这一步，模型学知识。理解语义。

  

未经预训练的模型会输出什么呢？

"奥萨蒂ask的怀旧啊士大夫和"

这种乱打出来的字.

预训练是大批有语法有逻辑的语句，

从中模型可以学到有逻辑的话语

  
  

<!-- #endregion -->
预训练数据来源要尽可能广、多，才能保证这一阶段模型是在学习“通用知识”

预训练数据来自图书、代码、社交平台



预训练所用的数据：

{"text": "这里是第一段文本内容...", "other_key": "其他信息..."}

text中包含的文本可以说，非常多样


我们要构造自己的数据集，就是把text提取出来，很简单


下面我们要把数据转化为模型看得懂的

这次我们先全部加载

  

```python

samples = []

with open('../data/pretrain_hq.jsonl','r',encoding='utf-8') as f:

    for line in f:

        data = json.loads(line.strip())

        samples.append(data)

  

```


  




把上述方法封装成函数如下

  

```python

class PretrainDataset(Dataset):

    def __init__(self, data_path, tokenizer, max_length=512):

        self.tokenizer = tokenizer

        self.max_length = max_length

        self.samples = self.load_data(data_path)

  

    def load_data(self,data_path):

        samples = []

        with open(data_path,'r',encoding='utf-8') as f:

            for line in f:

                data = line.strip()

                samples.append(data)

        return samples

    def __len__(self):

        return len(self.samples)

    def __getitem__(self, index) :

        sample = self.samples[index]

  

        encoding = self.tokenizer(

            sample[index],

            padding = self.max_length,

            max_length = self.max_length,

            truncation = True,

            return_tensor = 'pt'

        )

  

        input_ids = encoding.input_ids.squeeze()

        loss_mask = (input_ids != self.tokenizer.pad_token_id)

  

        X = torch.tensor(input_ids[:-1],dtype=torch.long)

        Y = torch.tensor(input_ids[1:],dtype=torch.long)

  

        return loss_mask,X,Y

  

  

```

  

# SFT流程

SFT 的全称是 Supervised Fine-Tuning，即 监督微调

在预训练阶段，模型在海量的、无标签的文本数据上进行训练，学习的目标通常是“预测下一个词”。这使得模型掌握了语言的语法、事实知识和基本的推理能力。 但此时的模型更像一个知识渊博但不懂得如何交流的“书呆子”，它只会根据上文补全句子，而不会特意地“回答问题”或“遵循指令”。

SFT阶段就是要把这个“书呆子”训练成一个乐于助人、善于沟通的AI助手。 这个过程就像是给模型一本高质量的“问答练习册”（即SFT数据集），其中包含了大量的“指令-回答”范例。

  
  

看看数据长什么样

  

```python

with jsonlines.open('../data/sft_1024.jsonl') as reader:

    data = list(itertools.islice(reader,1))

data

```

  

但这个数据还不是直接输入到模型的！因为SFT格式很多的，

- instruction、input、output

- q、a

等等

那么这个数据要转化成什么样，模型才能很好的学会应答呢？

这跟学的tokenizer有关了

我们找找tokenizer_config文件，发现有如下一段配置

  
  

```

  "additional_special_tokens": [],

  "bos_token": "<|im_start|>",

  "clean_up_tokenization_spaces": false,

  "eos_token": "<|im_end|>",

  "legacy": true,

  "model_max_length": 32768,

  "pad_token": "<|endoftext|>",

  "sp_model_kwargs": {},

  "spaces_between_special_tokens": false,

  "tokenizer_class": "PreTrainedTokenizerFast",

  "unk_token": "<|endoftext|>",

  "chat_template": "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% else %}{{ '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}"

  

```

下面介绍一下

- bos_token (Begin of Sequence): 定义了哪个标记代表“序列开始”。在这里是 <|im_start|>。

- eos_token (End of Sequence): 定义了哪个标记代表“序列结束”。在这里是 <|im_end|>。这是模型知道何时该停止生成文本的关键信号。

- pad_token (Padding Token): 在批处理（batch processing）中，为了让所有序列长度一致，需要用一个特殊标记来填充较短的序列。这里用 <|endoftext|> 来填充。

- unk_token (Unknown Token): 当遇到一个词汇表里没有的词时，用这个标记来代替。

- model_max_length: 模型能处理的最大序列长度（上下文窗口），这里是32768个Token。

- chat_template: 这是最关键的部分。它是一个用 Jinja2 模板语言编写的“小程序”，定义了一个精确的“食谱”，告诉分词器如何将一个对话列表（比如 [{'role': 'user', ...}, ...]）转换成一个最终的、可以被模型处理的字符串。

  
  

## 介绍chat_template

  

检查对话列表的第一个消息是不是system角色。

  

如果是，就把system的内容拿出来，包装成 <|im_start|>system\n...内容...<|im_end|>\n 的格式。

  

如果不是（大部分SFT数据都没有system角色），它会自动添加一个默认的系统提示：<|im_start|>system\nYou are a helpful assistant<|im_end|>\n。

  

这就是为什么很多模型默认表现得像一个“乐于助人的助手”的原因

  

当遇到 user 消息时: 它会生成：

  

起始标记和角色：<|im_start|>user\n

  

用户说的内容：content

  

结束标记：<|im_end|>\n

  

最关键的一步: 它会立刻紧跟着输出一个“提示”，告诉模型该轮到它发言了：<|im_start|>assistant\n

  

当遇到 assistant 消息时: 它会生成：

1. 助手说的内容：content

2. 结束标记：<|im_end|>\n

chat_template` 是一个自动化、标准化的“对话格式化引擎”。它确保了无论原始数据长什么样，只要是`[{'role': ..., 'content': ...}]`的格式，都能被准确无误地转换成模型在训练时所学习和理解的、带有特殊标记的唯一格式

  

简而言之，收集的SFT数据集类似这样

```messages = [

    {"role": "user", "content": "你好"},

    {"role": "assistant", "content": "你好！有什么可以帮助你的吗？"}

]

```

我们转化为如下格式

```

<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\n你好<|im_end|>\n<|im_start|>assistant\n你好！有什么可以帮助你的吗？<|im_end|>\n```

  
  

```python

with jsonlines.open('../data/sft_1024.jsonl') as reader:

    data2 = list(itertools.islice(reader,4))

data2

```

  

```

{'conversations':

   [

    {'role': 'user',

    'content': 'What is the most recent version of the Android operating system?'},

   {'role': 'assistant',

    'content': 'As of my last updaxxxx'}  

    ]

}

```

我们要的是

```

   {'role': 'user',

    'content': 'What is the most recent version of the Android operating system?'},

   {'role': 'assistant',

    'content': 'As of my last updaxxxx'}

```

  

```python

data2[1]['conversations']

```

  

```python

messages = []

for i, turn in enumerate(data2[1]['conversations']):

    role = 'user' if i % 2 == 0 else 'assistant'

    messages.append({"role": role, "content": turn['content']})

```


```

[ {"role": "user", "content": "What is the most recent version of the Android operating system?"}, {"role": "assistant", "content": "As of my last update..."} ]

```

转化为如下格式
```
<|im_start|>user
What is the most recent version of the Android operating system?<|im_end|>
<|im_start|>assistant
As of my last update...<|im_end|>
```

为啥嘞，我们定义好<|im_start|>就是一个编码
见到 "<|im_start|>" 这个完整的字符串，不要拆开它，就把它当作一个**不可分割的整体**，并给它一个独一无二的ID，比如 151644。见到 "<|im_end|>"，就给它ID 151645

同样，user 和 assistant 这两个词，也被当作特殊标记或高频词，在Qwen的模板中紧跟在 <|im_start|> 后面。

1.  在SFT阶段，模型被投喂了数百万条遵循 <|im_start|>user...<|im_end|><|im_start|>assistant... 格式的数据。
    
2. **学习关联**: 它反复观察到一个规律：

    - 跟在 [151644, 872] (ID for <|im_start|>user) 这串数字后面的文本，是“问题”或“指令”。

    - 而它**自己被要求生成**（即labels中不是-100的部分）的文本，**总是**跟在 [151644, 77091] (ID for <|im_start|>assistant) 这串数字后面。
    
3. **形成条件反射**: 经过海量数据的“轰炸”，模型内部的神经网络权重被调整，形成了一种“条件反射”。它学到的不是user的含义，而是这样一个规则：
4. 
    > **“当我的输入是以 [151644, 872, ...] 开头的序列时，我的任务就是续写这个序列。并且，我续写的内容如果能紧跟在 [151644, 77091] 之后，并且续写得和标准答案很像，我就会得到很低的损失（相当于得到了奖励）。**”
  

这里不是多此一举，而是展示如何构造数据集的

如果源数据没有user、assistant这些提示词呢？

  

```python

messages

```

  

```python

prompt = tokenizer.apply_chat_template(

            messages,

            tokenize=False,

            add_generation_prompt=False

        )

```

  

```python

prompt

```

  

```python

input_ids = tokenizer(prompt,

                     max_length=512,

                     padding='max_length',

                     truncation=True,

                     return_tensors='pt').input_ids.squeeze()

```

  

```python

input_ids

```

  

```python

loss_mask = len(input_ids)*[0]

bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids

eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids

  

while i < len(input_ids):

    if input_ids[i:i+len(bos_id)] == bos_id:

        start = i + len(bos_id)

        try:

            # 使用 list.index() 来高效查找 eos_id 的起始位置

            # 我们只在 bos_id 之后的部分查找

            end_token_index = input_ids.index(eos_id[0], start)

            # 确认这确实是一个完整的 eos_id 序列

            if input_ids[end_token_index:end_token_index+len(eos_id)] == eos_id:

                # 标记从回复开始到结束标记之前的所有 token

                # 通常我们希望模型学会预测 eos_id，所以也包含它

                end_of_response = end_token_index + len(eos_id)

                for j in range(start, end_of_response):

                    loss_mask[j] = 1

                # 将主索引 i 直接跳转到处理完的这段序列末尾

                i = end_of_response

            else:

                # 如果 index 找到的不是完整的 eos_id，则正常递增 i

                i += 1

        except ValueError:

            # 如果在 bos_id 之后找不到 eos_id (例如，序列被截断)

            # 那么将从回复开始到序列末尾的所有 token 都进行标记

            for j in range(start, len(input_ids)):

                loss_mask[j] = 1

            # 已经处理到末尾，可以结束循环

            break

    else:

            # 如果当前位置不是起始标记，则索引向后移动一位

        i += 1

  
  

```

  

```python

  

```

  

```python

loss_mask

```

  

对于lossmask函数，就是借助bosid和eosid 找到assisatant输出部分 让他们的lossmask部分等于1.  后面序列化

我们已经稍微熟悉tokenizer了

```input_ids = self.tokenizer(prompt).input_ids[:self.max_length]

input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))

```

和

```

input_ids = tokenizer(prompt,

                     max_length=512,

                     padding='max_length',

                     truncation=True,

                     return_tensors='pt')

```

效果是一样的哦

  

```python

class SFTDataset(Dataset):

    def __init__(self, jsonl_path, tokenizer, max_length=1024):

        super().__init__()

        self.tokenizer = tokenizer

        self.max_length = max_length

        self.samples = self.load_data(jsonl_path)

        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids

        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids

  

    def __len__(self):

        return len(self.samples)

  

    def load_data(self, path):

        samples = []

        with open(path, 'r', encoding='utf-8') as f:

            for line_num, line in enumerate(f, 1):

                data = json.loads(line.strip())

                samples.append(data)

        return samples

  

    def _create_chat_prompt(self, conversations):

        """构建符合ChatML格式的对话"""

        messages = []

        for i, turn in enumerate(conversations):

            role = 'user' if i % 2 == 0 else 'assistant'

            messages.append({"role": role, "content": turn['content']})

        return self.tokenizer.apply_chat_template(

            messages,

            tokenize=False,

            add_generation_prompt=False

        )

  

    def _generate_loss_mask(self, input_ids):

        loss_mask = [0] * len(input_ids)

        i = 0

        while i < len(input_ids):

            if input_ids[i:i + len(self.bos_id)] == self.bos_id:

                start = i + len(self.bos_id)

                end = start

                while end < len(input_ids):

                    if input_ids[end:end + len(self.eos_id)] == self.eos_id:

                        break

                    end += 1

                for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)):

                    loss_mask[j] = 1

                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)

            else:

                i += 1

        return loss_mask

  

    def __getitem__(self, index):

        sample = self.samples[index]

        # 构建对话提示

        prompt = self._create_chat_prompt(sample['conversations'])

        input_ids = self.tokenizer(prompt).input_ids[:self.max_length]

        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))

  

        # 生成动态损失掩码

        loss_mask = self._generate_loss_mask(input_ids)

  

        # 构建训练数据

        X = torch.tensor(input_ids[:-1], dtype=torch.long)

        Y = torch.tensor(input_ids[1:], dtype=torch.long)

        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # 对齐预测位置

  

        return X, Y, loss_mask

```

  

# RLHF

在经历SFT后，模型能回答人话了，但回答的质量嘛...

  

我们为了让他回答质量更好，需要再次进行训练

  

此时我们训练的方法是RLHF (Reinforcement Learning from Human Feedback - 基于人类反馈的强化学习)

  

1. 我们需要训练奖励模型，收集偏好数据，即人类觉得回答很棒的数据

  

2. 我们用这些数据集单独训练一个奖励模型，奖励模型用来给SFT训练好的模型输出打分，以分数作为奖励信号（想想预训练和SFT阶段我们的奖励信号是啥嘞），来更新SFT模型（这一步一般叫PPO算法）

  

-----

  

DPO的全称是：Direct Preference Optimization (直接偏好优化)。

我们不去搞奖励模型那么繁琐的ppo算法流程了，直接利用分割线第一步的偏好数据，直接在原SFT后模型中，直接进行优化

得到更好的DPO模型

  
  
  
  

看看数据长什么样

  

```python

with jsonlines.open('../data/dpo.jsonl') as reader:

    data = list(itertools.islice(reader,1))

data

```

  

chosen意思是人类偏好的回答，是质量好的回答

rejected意思是人类拒绝的回答，质量差的回答

  
  

这里的预处理其实和SFT一样的，提取content和role，应用tokenizer.apply_chat_template

inputids是全部文本都要，lossmask是只将assistance的消息设置为1

  
  

# RLAIF

全称是 Reinforcement Learning from AI Feedback (基于AI反馈的强化学习)

使用的方法称为Distillation，蒸馏

蒸馏的本意是使用已经有的优秀模型，比如说开源的高参数模型，让它当作老师，对一些问题给出参考答案。我们正在训练的小模型，就可以针对参考答案去升级

在得到DPO模型后，可以把这个模型看作“优秀模型”，也就是“老师”

我们把老师生成的参考答案，再给模型学习一次，有监督训练。

也就是 PreTrain：学会通用知识，认识世界---SFT:学会回答人类问题---DPO：基于RLHF，在SFT模型基础上，训练一个优秀的教师模型，它回答人类问题的质量更高---Distillation：将DPO模型视作SFT模型的教师模型，进行蒸馏，所谓蒸馏仍是一种SFT，就是SFT上的再次SFT