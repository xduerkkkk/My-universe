整体流程：
**生成高质量数据 → SFT微调 → GRPO强化学习优化，使用直接定义的奖励函数而非奖励模型。**
# generate
输入一个只有问题/指令的数据集，用一个强大的本地部署LLM，和distilabel库，自动化、并行地让大模型一一回答数据集的问题
输出一个新的数据集，包含问题和高质量答案。
这个大模型，要么是api调用要么是本地部署的
本地部署的，调用方法为
`http://localhost:8000/v1` 就很好理解了：

- `http://`: 表示使用标准的HTTP协议进行通信。
    
- `localhost`: 指向你当前正在使用的这台电脑。
    
- `:8000`: 指定要访问这台电脑上的 **8000** 号端口，也就是 vLLM 正在监听的那个“门”
这个过程就是”标签蒸馏“

# SFT
基于上一步的generate，做有监督训练
这是为了教会模型严格遵守特定的输出格式，比如think标签
# GRPO
grpo的作用只是当奖励给出时，如何更新模型使得他回答高质量答案概率更高， 只是个面对奖励的算法， 奖励函数也可以换成RLHF或RLAIF训练出的奖励模型的打分
同样的，grpo也可以换成ppo算法

1. **收集偏好数据：** 通过人类（RLHF）或AI（RLAIF）来获得成对的偏好数据。
    
2. **训练奖励模型：** 利用这些偏好数据训练一个能够为模型回答打分的奖励模型。
以上步骤也可以变成
**设置奖励函数**，如此，称为纯强化学习
本项目就是如此。

最后一步是
.**优化语言模型：** 使用像**GRPO**或**PPO**这样的优化算法，根据奖励模型提供的分数来微调原始的语言模型，使其更倾向于生成高分（即高质量）的回答。
# 奖励函数
传统SFT到RLHF中，我们要训练奖励模型
那为什么回答给高分还是有点”黑盒“的，也不能清晰解释actor模型的动机
open r1项目使用可计算的奖励函数，叫纯强化学习Zero，好处就在奖励完全透明、可解释性强

奖励函数可以被看作一个**评估委员会**，每个成员从不同角度对模型的回答进行打分。最终的总奖励，通常是这些独立分数的**加权和**。

$$Total_Reward = w1 * accuracy_reward + w2 * format_reward + w3 * reasoning_reward - w4 * repetition_penalty ...$$

这里的权重 w1, w2, w3, w4 是至关重要的超参数，需要精心调整，以平衡不同目标之间的可能存在的冲突。
## 所有函数
accuracy_reward(...): **准确性奖励**
format_reward(...): **格式奖励**。这个函数检查模型的回答是否遵循了特定的格式，即“思考过程”放在 <think>...</think> 标签内，“最终答案”放在 <answer>...</answer> 标签内
tag_count_reward(...): **标签计数奖励**。这是 format_reward 的一个“软性”版本。它不要求格式完全正确，而是根据出现的正确标签数量（如<think>, </think>等）给予部分分数。
reasoning_steps_reward(...): **推理步骤奖励**。通过正则表达式检查回答中是否包含 "Step 1:", "First,", "Next," 等标志着分步推理的关键词，以此来鼓励模型给出清晰、有条理的推理过程
len_reward(...) 和 get_cosine_scaled_reward(...): **长度与效率奖励**。这两个函数旨在鼓励模型在保证正确的前提下，给出更简洁的回答。
    
    - 对于正确的回答，越短，奖励越高。
        
    - 对于错误的回答，惩罚则与长度相关，避免模型为了凑字数而胡言乱语。
        
    - cosine_scaled_reward 使用余弦函数来平滑地调整奖励值，是一种更精细的控制方式。
                                                  
get_repetition_penalty_reward(...): **重复惩罚奖励**。检查回答中是否存在N-gram（连续的N个词）的重复。如果模型不断重复相同的话语，就会被扣分。这能有效提升回答的流畅性和信息密度。
get_soft_overlong_punishment(...): **超长惩罚**。这个函数专门用于惩罚超过预设最大长度的回答，但不会奖励较短的回答。它只在回答“太长”时才施加负向奖励。
用于评估模型生成代码的能力。

- ioi_code_reward(...) 和 cf_code_reward(...): **特定竞赛代码奖励**。这两个函数分别针对IOI（国际信息学奥林匹克竞赛）和Codeforces（一个编程竞赛网站）类型的问题。它们会将模型生成的代码，提交给一个模拟的判题环境（如 **Piston** 或 **Morph**），并根据通过的测试用例（Test Cases）数量来打分。
    
- code_reward(...) 和 binary_code_reward(...): **通用代码奖励**。这两个函数更为通用。它们从数据集中获取测试用例，然后通过一个代码执行提供者（如 **E2B**）来运行模型生成的代码。
    
    - code_reward 会根据通过测试用例的比例给出部分分数（例如，通过8/10个测试用例，得0.8分）。
        
    - binary_code_reward 则是“非黑即白”的奖励，只有当所有测试用例都通过时才给1.0分，否则为0.0。
        
- get_code_format_reward(...): **代码格式奖励**。与 format_reward 类似，但专门针对代码场景，检查代码是否被正确地包裹在 `<think>/<answer> 和代码块标签中

下面我们分类梳理一下

## 内容要对
- accuracy_reward: 答案是否正确？
    
- reasoning_steps_reward: 推理过程是否分步、清晰？
    
- ioi_code_reward, cf_code_reward, code_reward: 代码是否能通过测试用例？
## 格式要对
- format_reward: 是否严格遵守` <think>/<answer> `结构？
- tag_count_reward: 是否包含了所有必要的标签？（一个更宽松的版本）
- get_code_format_reward: 代码是否被正确地包裹在代码块中？
## 效率要高
- len_reward / get_cosine_scaled_reward: 在正确的前提下，鼓励简洁。
    
- get_repetition_penalty_reward: 避免无意义的重复，提升流畅性。
    
- get_soft_overlong_punishment: 对过长的回答进行惩罚。

