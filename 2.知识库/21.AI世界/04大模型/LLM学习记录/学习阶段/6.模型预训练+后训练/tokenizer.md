[(27 封私信 / 2 条消息) 大模型基础组件 - Tokenizer - 知乎](https://zhuanlan.zhihu.com/p/651430181)

- 分词：长段句子需要拆分  组建词库
- 编码：每个词一个数字 特殊符号，比如`<|im_end|>` 
以上流程，构建出`input_ids` 也就是自然语言翻译后，输入给模型的东西
这些流程由tokenizer完成，  而labels、attention_mask呢？是人工构建的吗？

# 分词

句子要切块处理，就好比cpu处理时的机器字长嘛！

那直觉来看，要么按词分，要么按字分
### 按词分

**例子句子**：  
`"I have two dogs."`

**切分结果**：  
`["I", "have", "two", "dogs", "."]`

需要为所有单词分配ID（如 `dog=1001`，`dogs=1002`），词表可能达到数万甚至百万级
还有dog和dogs完全没关系嘛 模型也不知道他们是单复数关系

### 按字分
**例子句子**：  
`"I have two dogs."`

**切分结果**：  
`["I", " ", "h", "a", "v", "e", " ", "t", "w", "o", " ", "d", "o", "g", "s", "."]`

那更是把语义关联切断了

## 基于Subword的切分
高频词就按完整的样子保留
低频词就拆分成高频词和其他
什么叫低频词？比如dogs 
像 `dog` 这样的基词在语料中出现的频率远高于其变体（`dogs`、`doggy`等）

`dogs` 是低频词，被拆分为 `dog` + `##s`（`##`表示子词后缀）
`["I", "love", "dog", "##s", "and", "hot", "##dog", "##s", "."]`
- `dogs` → `dog` + `##s`
- `hotdogs` → `hot` + `##dog` + `##s`

这是我们人在这说呢，那机器怎么知道他要进行高频词保留低频词拆分？
模型是一步一步学习的，先全部按字分，统计相邻两个字的词频，就能知道高频是哪两对
先尝试拼起来，然后看这高频不，高频就说明拼对了，就这么分。循序渐进。
- 初始拆分：`h o t d o g s`

- 合并高频对：
    - `o` + `g` → `og`
    - `d` + `og` → `dog`
    - `h` + `o` → `ho`
    - `t` + `dog` → `tdog`（频率低，不合并）
    - 最终可能得到：`hot` + `dog` + `s`

开始时将所有单词拆分为 **字符（或字母）**，并对非起始字符添加 `##` 前缀。  
**例如**：  
单词 `"dogs"` 的初始拆分：

- `d` （开头字符，不加 `##`）
- `##o` （非开头字符，加 `##`）
- `##g`
- `##s`
`##` 明确表示“这个子词不能作为单词的开头”，帮助模型区分
而且有特殊符号标记就大概率是前缀后缀
如`##ing` 只能出现在词尾 `playing`
非常棒的处理

# tokenizer
## input_ids
怎么分词，我们通过语料都训练好了。
那我们构建个词典不过分吧？
```
{
    "[PAD]": 0,    # 填充符
    "[UNK]": 1,     # 未知词
    "[CLS]": 2,     # 分类标记（BERT）
    "[SEP]": 3,     # 分隔符
    "dog": 4,
    "##s": 5,
    "hot": 6,
    "##dog": 7,
    "I": 8,
    "love": 9,
    ".": 10,
    ...
}
```

我们就可以根据词典token化了
```
tokens = ["I", "love", "hot", "##dog", "##s", "."]
token_ids = [8, 9, 6, 7, 5, 10]  # 根据词典映射
```

# 其他



```
tokens = ["Hello", "<|im_end|>", "World", "[PAD]", "[PAD]"]
```


```
大模型的核心逻辑是预测！我们要让模型学会预测下一个词

input_ids = [A, B, C, D]  # 原始序列

  

# 构造训练对：

X = [A, B, C]   # 输入序列 (位置 0~2)

Y = [B, C, D]   # 目标序列 (位置 1~3)

  

# 模型训练任务：

- 输入 A → 预测 B ✅

- 输入 A,B → 预测 C ✅

- 输入 A,B,C → 预测 D ✅
```

#### **编码后**：

- `input_ids`：`[42, 2, 57, 0, 0]`
    
- `attention_mask`：`[1, 1, 1, 0, 0]`
    
- `labels`（预测目标）：`[2, 57, 0, 0, 0]`
    
- `loss_mask`：`[1, 1, 0, 0, 0]`



`transformers` 库中的 tokenizer 对 `padding` 参数通常支持以下几种字符串值：

- `'longest'`: 填充到批次中最长序列的长度。
    
- `'max_length'`: 填充到 `max_length` 参数指定的长度。
    
- `'do_not_pad'`: 不进行填充。
    
- `True` 或 `False`: 也可以直接传入布尔值，`True` 通常等同于 `'longest'`。
    

因此，`padding='max_length'` 的意思是告诉 tokenizer：**“请将所有序列填充到由 `max_length` 参数所指定的固定长度”**。