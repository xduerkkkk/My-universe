
## 预训练
- 直接拿网上的数据集：
{"text": "这里是第一段文本内容...", "other_key": "其他信息..."}
- 自己收集
最终构造出{'text':...}格式的jsonl
字典的key叫text只是一个约定俗成的习惯，也可以是content或其他。关键在于你的数据加载代码能够正确解析它。
当然，构建一个标准的预训练数据集，仅仅把已有文本变成json格式还不够
还要去重和清洗
- 去重 : 必须对数据进行严格的去重，防止模型在重复的内容上过度拟合，从而降低泛化能力。
    
- **清洗:
    
    - 移除HTML标签、JavaScript代码、模板化的页眉页脚等“噪音”。
        
    - 过滤掉低质量文本（如乱码、过短的句子、无意义的重复字符）。
        
    - 进行**去污（Decontamination）**: 移除所有可能与下游评测任务（如C-Eval）相关的文本，防止“数据泄露”，保证评测的公正性。
        

## SFT
收集的数据类似下面格式
```
[ {"role": "user", "content": "What is the most recent version of the Android operating system?"}, {"role": "assistant", "content": "As of my last update..."} ]

```
有很多开源的数据集，
当然，你也可以自己做
和这些开源数据集制作流程一样，
要经历
**高质量人工标注 **: 
- **Self-Instruct**: 由人类编写少量高质量的“种子”指令，然后利用一个强大的模型（如GPT-4）来生成更多、更多样化的指令，最后由人类对这些生成的指令进行筛选和优化。
- **直接编写**: 雇佣标注员，让他们扮演用户和助手的角色，从头开始创造高质量的对话。


针对不同的tokenizer  进行格式转化
**每种模型家族都有自己独特的对话模板**。在微调时，必须使用与目标模型预训练时或其设计所兼容的模板，否则模型将无法正确理解对话的边界和角色。
qwen类的就转化为：
```
<|im_start|>user
What is the most recent version of the Android operating system?<|im_end|>
<|im_start|>assistant
As of my last update...<|im_end|>
```


    

# DPO、Distillation、Reasoning
DPO、Distillation、Reasoning数据与SFT数据格式大体一致
DPO需要给出两种回答，好的和不好的

```
[
{'chosen': [{'content':'...' , 'role': '....' },{'content':'...','role':'...'} ]

'rejected':[{'content':'...' , 'role': '....' },{'content':'...','role':'...'}]

}

]
```

Distillation数据 中assistant部分的回答，**不是由人类编写的，而是由一个更强大的“教师模型”（如DPO模型或GPT-4）生成的**。

Reasoning数据中，assistant回答多了推理步骤和标签

```
{"conversations": [
    {"from": "human", "content": "问题..."},
    {"from": "gpt", "content": "<think>第一步，我需要分析问题的关键点是A和B。第二步，我需要调用公式C来计算。第三步...</think><answer>最终答案是D。</answer>"}
]}
```
