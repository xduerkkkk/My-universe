

# gpt 历史
gpt1: 大规模文本数据集无监督预训练，可针对特殊任务微调
gpt2：模型参数扩大，数据集扩大
gpt3：参数扩大、零样本学习、改进无监督训策略（精细的正则化、梯度下降）
instructgpt
chatgpt（gpt3.5）





# 再次复习 gpt 流程
## 预训练（pretraining）
先回顾 sec2sec，翻译模型的预训练。句子通过 attention 机制，每个词都能变成与语境相关的词向量（训练后）。训练时给出“i love”和“我爱”，i love 在 encoder，我爱在 decoder，通过 decoder 的掩码机制，让机器学习“i 翻译为我”，但不是直接显式的翻译。
     首先机器会从《start》标签开始生成，此时的 k 就包含着“我需要主语”的信息，经过模型训练，encoder 中的“i”有主语信息的“key”，于是聚焦到 i 上，嗯，接下来我要输出的翻译对应的是“i”，同时我要找“主语”信息的词，i 现在就被榨干了，接着本次生成与“i”就没关系了！模型自己找中文词表，我，你，他，发现“我”的概率比较大，然后输出“我”，并与真实标签“我”作对比。训练阶段与实际生成的阶段区别就是，训练阶段你预测出 end 的下一个为“他”，模型与真实标签对比后惩罚模型自己调整学习参数，接着 1 下一个预测仍然是真实值“我”而不是错误预测“他”。
<font color="#e36c09">那 gpt，以预测下文为目的的模型，具体的训练细节到底是怎样的，如何让模型训练语义呢，让他有预测下文的能力呢，训练时出现的 loss 到底是谁和谁在对比?</font>
这么一看，感觉transformer的架构比gpt复杂？因为transformer使用的是encoder和decoder，训练的时候是两个输入，一个标签（不过是从decoder输入移位得到的），训练时既训练每个词语的attention的各个语义矩阵，也训练如何“翻译”，而gpt仅仅使用decoder，输入是一个完整的文本，标签也是自身，输入用到了掩码。gpt训练的目的只是通过decoder训练文本预测下一个次的能力，
### Tokenizer Training
分词器，指把输入文本拆分成单词级别的 token
- wordpiece：每个词对应一个数字，也就是有个超级大的对应库。- 1. 统计相邻符号对的频率，合并最高频的组合（如 `"un" + "fit"` → `"unfit"`） **迭代优化**：重复合并直到词汇表达到预设大小（如 30k）。**示例**：BERT 的词汇表包含 `"un"`、`"fit"`、`"##ing"`（`##` 表示后缀）。
- bbpe：按照 unicode 编码作为最小粒度
 
## SFT
来自数据集，人工标注“问题+回答”，
当然现在 openai 已经当了人工标准的先锋，我们可以利用 chatgpt 生成“问题+回答”类别
也可以用开源数据集

## RW 训练
## RLHF

| 表头1 | 表头2 | 表头3 |
|-------|-------|-------|
| 内容1 | 内容2 | 内容3 |
| 内容4 | 内容5 | 内容6 |