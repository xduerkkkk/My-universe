# instructgpt 高层方法论
- 先像正常的语言模型一样，进行有监督学习
      那和无监督微调的区别就是，无监督是把所有文本一股脑塞进模型里面让他自己理解语义与文字，而有监督微调是技术人员选取人类的”提示词+回答“，让模型去学习
- 接着来一波人工，让他们对机器的输出进行打分
- 使用 ppo 算法，rlhf，强化学习，使得机器调整参数从而能输出更符合人类偏好的；
- 机器现在知道怎么更符合人类偏好了，<font color="#e36c09">然后机器,,, 不知道了，看不懂原文中的“二三步迭代”是啥意思</font>
      -  其实就是第三步输出的数据，继续回到第二步让人类打分，即用优化后的模型生成新数据，重新让人类标注偏好，更新RM和PPO策略。
## dataset

  这里有点混乱，<font color="#e36c09">本篇论文数据集到底都是从哪来的</font>？
- 数据来源，一些是之前使用 gpt3.. 等等使用模型的网友们，发给模型的 prompt，一些是标注员发的“prompt 和响应”

| **数据集类型**  | **数据内容**           | **人工介入**   | **用途**      |
| ---------- | ------------------ | ---------- | ----------- |
| **SFT数据集** | 标注员的“指令+人工响应”      | 全程人工撰写     | 监督微调（教模型回答） |
| **RM数据集**  | 用户Prompt + 模型输出的排序 | 人工排序（不写答案） | 训练奖励模型      |
| **PPO数据集** | 纯用户Prompt          | 无          | 强化学习环境输入    |

相当于先训练RM模型，这一部分是要有人工参与打分的。RM模型也会学习人类如何打分的。之后呢，就是ppo数据集，此时RM模型就变成了“老师”（RM 模型参数不变了），ppo算法能根据RM模型的打分调整自己的参数。此时就解放人工了，<font color="#e36c09">对吗</font>？<font color="#e36c09">那强化学习这个概念在哪里出现？</font>
         1. 确实不需要人工直接参与了 2强化学习体现的是智能体与环境的交互，有state、action、reward，显然训练RM 模型只是单纯的有监督训练，后面ppo 算法，属于强化学习中的一种方法，无需人工干预的时刻开始就进入了强化学习