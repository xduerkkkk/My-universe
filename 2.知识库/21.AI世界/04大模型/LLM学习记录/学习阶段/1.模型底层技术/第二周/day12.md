
### **词向量（Word Embedding）的定义与阶段**
- **词向量的定义**：词向量是指将离散的词汇（或token）映射到连续的低维向量空间中的表示。
- **具体阶段**：
    - **输入阶段（纯Token ID）**： 模型输入的原始形式是 **Token ID**（整数），例如 `[1, 42, 10]`。此时仅是索引，**不称为词向量**。
    - **经过Embedding层后**： Token ID 通过嵌入层（如 `nn.Embedding`）转换为固定维度的连续向量（如512维），**此时才称为词向量**。 例如：`"apple" → [0.2, -0.5, 0.7, ...]`（形状 `(d_model)`）。

#### **关键区别**

| **阶段**        | 形式            | 是否叫词向量 | 示例                       |
| ------------- | ------------- | ------ | ------------------------ |
| 输入（Token ID）  | 整数（如 `1, 42`） | ❌ 不是   | `[1, 42, 10]`            |
| 输出（Embedding） | 浮点向量          | ✅ 是    | `[[0.1, 0.3, ...], ...]` |
# embedding 层
batch_size, seq_len)）
通过-input embedding 处理，也是nn.Embedding 函数（形状变为 (batch_size, seq_len, d_model)）
具体来说吗，刚传入的词，每个数代表的是它的 id！，emb 函数里有一个词库，每一个词都有 d_model 个信息
#### **(1) 输入张量**

Python

`x = torch.tensor([[1, 2], [3, 4]])  # shape: (2, 2)`

#### **(2) 嵌入层 (`nn.Embedding`)**

- `nn.Embedding(100, 4)` 会初始化一个形状为 `(100, 4)` 的权重矩阵，每一行对应一个 token ID 的嵌入向量。100 是说有多少个词（多少个 id），4 是每个词的维度信息（比如第一列代表形状？第二列代表颜色... 等等）
- 假设权重矩阵的前几行如下（随机初始化值）：
    
     [[0.1, 0.2, 0.3, 0.4],   # ID=0
     [0.5, 0.6, 0.7, 0.8],   # ID=1
     [0.9, 1.0, 1.1, 1.2],   # ID=2
     [1.3, 1.4, 1.5, 1.6],   # ID=3
     [1.7, 1.8, 1.9, 2.0]]   # ID=44
通过 `self.embed(x)`，输入 `[[1, 2], [3, 4]]` 会被替换为对应的嵌入向量：

 output = [
  [[0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]],  # ID=1和ID=2的向量
  [[1.3, 1.4, 1.5, 1.6], [1.7, 1.8, 1.9, 2.0]]   # ID=3和ID=4的向量】
  可见，原本的 (2,2) 变为了 (2,2,4). 这个 4 就是 d_model!!!!
# 位置编码层
maxlen=2，d_model = 4
那么 position 是一个（maxlen，1）的，每一个维度存储的数字就是“位置”的张量
pe 呢，是（maxlen，d_model）的全零的矩阵
pe 
```python
pe[:,0:: 2]  # 表示所有行，所有偶数列。啊对，他们要填充sin函数
```
divterm, 代码是 arange（0，d_model, 2）但是不要在意它是偶数列，而要在意它的维度是 d_model 的一半!
