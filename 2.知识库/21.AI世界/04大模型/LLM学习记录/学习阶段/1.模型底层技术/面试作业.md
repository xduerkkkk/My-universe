##  说一下 chatgpt 的优缺点
chatgpt 是划时代地开创生成式 ai，核心创新在于引入人类反馈的强化学习到大语言模型任务里。再加上大规模预训练，模型表现出很好的对话能力。chatgpt 接着通过 api 服务化，使得各个开发者可基于模型微调适配自己的垂直场景。chatgpt 的缺点，一是消耗大量资源，靠大规模预训练的数据，随着时间的推移，面临数据枯竭的问题，需要不断开展新技术如 deepseekr1。二是推理能力的黑箱与限制，模型生成的本质还是统计概率，有不可解释性。三是数据风险，模型依赖训练数据、人工标注，数据这么多，不能完全把控数据的伦理问题、安全问题，模型还有可能被有心之人用 prompt 生成恶意的代码或信息

## 简述一下 transformer 的基本流程
传统的 transformer 有两部分
encoder：
首先是数据需要进行预处理，通过 tokenizer 将其词语映射成 id，此时向量维度为 batch_size，seq_len。接着，通过词向量层，基于词库（比如 apple 一词有 d_model 个数字），将离散ID到连续向量。此时向量维度应为【batch_size，seq_len，d_model】
接着，还要引入位置编码，已表示词在句子中的位置，也可以加入掩码矩阵
进入多头注意力机制模块，QK 矩阵相乘变为 attention 矩阵，v 与 attention 相乘变为最后输出的向量，在相乘的过程中均把 d_model 拆分成 head 和 head_dim  再合并，还原为【batch_size，seq_len，d_model】，
进入 Add&Norm 板块，先将原来的 x 加入到上一步输出的结果，再层归一化，作为新的输入
进入 FeedForward 板块，先线性，再激活，后再次线性，增加模型表达能力
重复 Add&Norm 和 feedforward

重复 N 次

decoder：
仍然有一个输入序列，这个序列实际上是 decoder 上一次生成的句子。数据处理一样，与 encoder 不同的是要经过两次多头注意力，第一次是基于掩码的，QKV 都来自输入序列，第二次是 KV 来自 encoder，Q 来自 decoder 输入训练的交叉多头注意力机制，两次的结果分别合并，最终一起送进接下来的 Add&Norm 和 FeedForward，仍然重复 N 次
最后，由 softmax 将原本的 d_model 映射为 vocabularry_dim，关注最后一个词，voca_dim 中哪个词的概率最大，生成哪个词

## 为什么基于 Transformer 的架构需要多头注意力机制
一个头容易视角单一。将 d_model 拆开，分别进行矩阵运算，由于是不同的 qkv 矩阵，能让模型理解更多信息，捕捉复杂特征，比如一个头理解了语义，一个头理解了语境等

## 编码器，解码器，编解码 LLM 模型之间的区别
编码器 llm 专注文本理解，输出是与输入长度一致的词向量，直接能运用到下游任务（情感分析、序列标注）
解码器 llm 专注文本生成，输入序列是解码器历史生成序列，基于输入序列，采用基于掩码的自回归注意力机制，生成的向量是信息转化为概率的向量
编解码 LLM 模型就是又有编码器又有解码器，原始的 transformer 架构就是如此，编码器一个输入一个输出，解码器的输入是自己的序列和编码器的输出，采用自回归注意力和交叉注意力机制，解码器的输出是转化为概率的向量

## 你能解释在语言模型中强化学习的概念吗？它是如何应用在 GPT 的
强化学习思想是对象在环境中，通过试错优化策略，以在环境得到好的奖励。语言模型中的强化学习概念全称应该是 RLFH，是基于人类的反馈标注，作为奖励，不断模型输出输出得更好，更对齐人类价值观的过程
在 gpt 中全流程应为：1. 采用无监督预训练后的模型，该模型应理解基本语义，instructgpt 里采用的就是 gpt3  2. 人工编写好的 prompt 和 response，直接有监督学习（SFT），让模型初步对齐人类价值观 3. 模型输出后人类进行排序，让机器学会给优质回答打分，从而训练出奖励模型 4. 通过奖励模型给大模型输出打分，基于打分，模型通过强化学习算法，如 ppo（策略梯度），不断调整输出
## 在 GPT 中，什么是温度系数？
温度系数是一个超参数，影响模型输出的创造性、随机性。体现在最后 softmax 输出概率时，温度系数大于 1，那原本低概率的词概率变大，模型输出就更有创造力，问答系数小于 1，高概率的词概率变大，所以模型输出更保守，比较稳定，甚至有重复。

## 什么是旋转位置编码（ROPE）？
基于复数函数的旋转特性，加入到 QKV矩阵中，相比经典的正余弦绝对位置编码，ROPe 是相对位置编码，更适合长的上下文


## 为什么现在的大模型大多是 decoder-only 模型
生成文本 decoder 就够了，理解语义这一过程，本来说 encoder 的任务，但是通过 chatgpt 的案例发现虽只有decoder ，但通过自回归注意力机制，也能理解文本，那一个 decoder 又能理解又能生成，encoder 所占的资源就有点冗余了

## ChatGPT 的训练步骤有哪些

1. 预训练：基于 decoder-only 架构，通过互联网海量的文本进行无监督学习，理解基本语义架构
2. 指令微调：使用第一步预训练的模型，基于人工标注的 prompt 和 response，进行有监督训练，初步对齐人类价值观
3. 训练奖励模型：模型输出结果，人类给结果排序，训练一个模型学习这个过程，称为奖励模型，奖励模型能做到给其他模型的生成打分
4. PPO 方法的强化学习：以第二步的模型为基础进行输出，通过第三步的奖励模型打分，使用 ppo 策略优化模型参数，使得最大化预期奖励

## 为什么 transformer 需要位置编码
如果没有位置编码，模型无法理解每个字的位置。比如我爱你和你爱我，没有位置编码的话这两种情况每个字的向量都是一模一样的，那带来相同的注意力权重，势必输出结果会是一样的，会造成语义上理解的混乱

## 为什么对于 ChatGPT 而言，提示工程很重要
因为 llm 需要理解语境，是基于输入的语境输出结果的，它不是真正的懂人类意思而是通过概率输出。假如有两套提示词，虽然人类的意思没变，但是模型是根据输入的句子理解的，即同一个含义，不同的提示词可能输出结果差异很大
，故而将提示工程做好，比如结构化割舍，角色设定等，才能保证输出的高质量回答的概率提升。
## 如何缓解 LLM 复读机的问题
训练奖励模型时，将人工生成的重复数据打低分，模型会自动抑制重复数据的生成。
调节温度，将温度系数调高，增强输出随机性