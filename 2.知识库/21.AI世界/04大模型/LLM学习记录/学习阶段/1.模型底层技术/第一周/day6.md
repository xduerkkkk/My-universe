# 模型预训练
## 语言建模
让模型学习句子中每个字的联系
预测文本的概率分布，
传统就是马尔可夫，进阶一点是 rnn、lstm、transformer，再进阶是 gpt、bert
预测句子前缀“小明有三块糖，给了小红两块糖，还剩下一块糖”中的“一块糖”时，并不是大模型脑海中转化成 3-2=1，而是单纯的学“三块糖，少两块，剩一块”有点像突击期末时看答案看出题感的感觉
## 去噪自编码
如 bert，t5 预训练语言模型。输入文本经过随机替换、删除，形成损坏文本，模型根据损坏的文本恢复成正常的情况。通过这一手段训练模型
# 优化参数设置
这里和就是神经网络的经典用法，梯度下降，反向传播，作为程序员要调节学习率以及优化器的梯度修正策略，
### 批次数据
较小批次和较大批次都会带来不同的效果。现在大模型都是个位数 M 单位为一个批次。
<font color="#e36c09">为啥批次会影响效果呢</font>？这应该是深度学习的知识点，我不太懂

<font color="#e36c09">就是假如第一个批次有10个，，模型会根据这10个数据一直连续地梯度下降，调整参数。对吗？那如果是100个，就会连续100次？那10次10个数据批次和一次性100个数据，1个批次，为什么有不同效果呢</font>
    答：模型更新参数是一次性更新的，也就是读取 10 个，那就计算这 10 个的平均损失，然后更新参数，读取一百个，那就计算 100 个的平均损失。这样的话，预训练中大批次的好处就显现出来了。
    损失下降地更加稳定，使模型更好地收敛。

### 学习率
- 在大语言模型预训练中，学习率先小，后线性增加至一个高阈值，再慢慢变小。<font color="#e36c09">为什么这么做？这是所有深度学习领域的做法吗？还是针对llm的</font>
     答：因为大语言参数巨大，所以最开始要“预热”，学习率低一点，让模型稳定，然后再慢慢高起来。也是效率最高的时候，就是阈值部分。但后面要收敛嘛，于是再慢慢变小
### 优化器
Adam，用来自己优化更新学习率
### 稳定优化技术
- 梯度裁剪：把梯度限定到一个范围。超出了就按阈值算。大模型领域常设之为 1
- 训练恢复：就跟游戏存档一样，设置模型存档点
- 权重衰减：正则化，即引入衰减系数。常设置为 0.1
- dropout：随机把一些正训练的神经元置为 0. 不过因为归一化，这种方法不用了。为啥