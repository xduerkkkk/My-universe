# 强化学习
强化学习，与有监督学习的区别是，更加明显的惩罚机制，而不仅仅是交叉熵函数损失？然后强化学习并不给模型一个确切的标签学习？而是设立惩罚标准，比如说，现在目的是让机器找到好吃的餐馆。有监督就是把好餐馆罗列出来，把机器找到的餐馆与我们罗列的餐馆进行比较，若不同则调整参数，以让机器更大概率选到我们的好餐馆。强化学习就是给机器说，你自己找餐馆吧。反正如果找到的餐馆，饭菜很难吃我就惩罚你。很好吃就奖励你。这样让机器自己摸索好餐馆在哪，于是本来人也不知道好餐馆在哪，但机器通过我们设立的标准帮人摸索出来了？？？这种对强化学习的理解正确吗
	    ####  **误解1：强化学习仅依赖“惩罚”**
	
	- **修正**：RL更依赖**正向奖励**设计，惩罚（负奖励）可能导致训练不稳定。例如，推荐系统中更关注“点击奖励”而非“不点击惩罚”。
	
	#### **误解2：强化学习不需要先验知识**
	
	- **修正**：RL通常需要预训练或结合监督学习（如模仿学习）加速收敛。例如，餐馆推荐可先用监督学习初始化策略，再通过RL优化。
	
	#### **误解3：强化学习完全替代监督学习**
	
	- **修正**：两者常结合使用。例如，RL的奖励预测模块可能依赖监督学习模型（如预测用户评分）。
	
	### **总结**
	
	- **你的理解正确**：监督学习依赖明确标签，强化学习通过奖励信号探索策略。
	- **核心补充**：
	    1. 强化学习关注**长期累积奖励**，而非单步反馈。

## 策略梯度算法
#### SFT通常指的是 **监督微调（Supervised Fine-Tuning）**
### RLFH（**Reinforcement Learning from Human Feedback**）

## PPO（Proximal Policy Optimization）

先再次定义一下强化学习里面的一些符号。首先强化学习是放在 agent 与 environment 的交互里讨论的。在交互过程中，S ，R，A。S 为机器当前的状态，A 为机器做出的行为，R 为身处 s 能带来的即时收益。对应 state、action、reward
不过，让机器有一个量化收益的公式，一定不仅有当前即时收益 $R$ ，还有未来收益 V。暂时乘一个折扣因子 $\gamma$
$V_t=R_t+\gamma V_{t+1}$
### **在RLHF-PPO阶段**
#### 有四个模型。
 - Actor Model，就是我们要训练的语言模型
     - SFT 得到初始模型，我们给他投喂 prompt，然后我们把他的 prompt 与 respond 都传到我们的奖励惩罚体系中
 - Critical Model 预估总收益 $V_t$
      -  就是输出后，用一个简单的线性层，预估未来总收益。这个模型是可训练的
 - Reward Model 计算即时收益，也就是 $R_t$
     - 这个模型参数是冻结的，也就是我们无法改变它！它是训练好后的模型。虽然 c 模型能预估总收益，但我们用 c 模型预估的未来收益和 r 模型的当前收益加起来，一定是更加准确的
 - Reference Model 参考模型
     - SFT 得到初始模型. 然后不动他了，我们去训练我们 actor，保证 actor 不会偏离太远。好比我们在培训演员，reference model 就是正常人，我们培训的演员不能出现跟正常人偏差太大的行为嘛

#### Actor-loss 
 - 基本公式  $actor\_loss = -\sum_{t \in \text{response\_timestep}} V_t \cdot \log P(A_t \mid S_t)$
	注意 $-V_t \cdot \log P$
	是损失。优化器的目的是降低 loss，那如果 V 很大，显然 loss 现在就很大！（注意 logp 是个负数，因为 p 是小数）自然而然地，我们要损失最小。那么p 的概率就要大。靠近 1！所以，这一过程，就体现了当收益很大时，模型会调整地让收益大的行为对应的 p，变大
 - 引入优势
    - 其实就是“甜头”，本来预估的未来收益是 10，结果未来真正的收益是 25，我们把多出来的 15 定义为  $$Adv_t = R_t + \gamma * V_{t+1} - V_t $$
- deepspeed-chat的RLHF实践中，对 Rt 的定义
![[day7-1744815159157.jpeg]]
即，考虑是否输出与“正常人”差太多
- 对于优势的再次改造：
     把优势再加上个未来优势。未来优势就是下一步的优势，我们从最后一步的优势反推，把每一步未来优势推出
## 理清 RLHF-PPO 流程
准备数据，STF 构造 actor 和 reference 两个初始模型，再训练 reward 模型，用人类偏好数据标注，损失函数是对比损失。接着就是 ppo，首先，准备一个 batch 的提示词，然后喂给 actormodel，生成 response，接着将提示词与 actormodel 的全部训练 c、reward 模型和 reference 模型，也就是根据 loss，更新参数