# gpt 系列
## gpt1
transformer 架构为基础，无监督预训练和有监督微调相结合
1. **无监督预训练（自学菜谱）**
    
    - 老师扔给你一本**没有标注的菜谱大全**（比如全网文本），让你自己反复看。
    - 你通过观察句子中词语的搭配（比如“西红柿炒鸡蛋”常一起出现），**猜词游戏**（遮住一个词猜它是什么），慢慢学会了语法、常识和做菜的套路。
    - _相当于模型从海量文本中自学语言规律。_
2. **有监督微调（师傅手把手教）**
    
    - 现在你要专攻某道菜（比如“宫保鸡丁”）。师傅给你**标注好的步骤**（输入句子→对应答案），纠正你的细节。
    - 你调整之前自学的知识，专注解决具体任务（比如问答、分类）。
    - _相当于用少量标注数据让模型适应特定场景。_
同时期发布的预训练语言模型是 bert，对比 gpt 只保留了 encoder，偏向理解任务
此时 bert 是比 gpt1“火”的
## gpt2
注重参数量，加大无监督预训练，认为大规模无监督训练后，模型能理解世界的文本，那微调也很简单了，因为模型更像人了
## gpt3
研究拓展法则，进一步加大参数规模
## codex
gpt3 复杂推理能力弱，ioenai 以代码数据集为微调，gpt3.5 就基于 codex
## PPO算法（ProximalPolicyOptimiza tion, PPO）
人类对齐？
## RLHF

## chatgpt
22 年 11 月，沿用 instructGPt 的训练技术，对对话能力针对性优化，用大量对话数据！
## gpt4
拓展图文双模态应用了一些干预策略来缓解大语言模型可能出现的问题—— 幻觉、隐私泄露等。例如，研究人员引入了“红队攻击”（RedTeaming）机制 30 2.4 GPT 系列模型的技术演变来减少生成有害或有毒的内容
## GPT-4V、GPT-4Turbo以及多模态支持模型
进一步提升功能，提供 api

# 大模型预训练的归一化
## layernorm
## deepnorm

# 一些问题
#### **缩放点积（除以√D）：稳定梯度传播**

- **点积问题**：当向量维度D较大时，点积结果方差随D增长（假设Q/K元素独立且方差1，则点积方差为D），导致softmax输入过大，梯度趋近于0（ 
    
- **缩放作用**：通过除以√D将方差归一化为1，使softmax梯度保持在合理范围内（实验证明不缩放时模型难收敛