# 位置编码
之前的 attention 机制中，每个词最终的 z 词向量，是和所有词有关的。但如果打乱顺序呢？比如 abce 变成 ceba。显然，a<font color="#e36c09"> 的词向量还是一样的</font>？
那现在除了基础的 attention 的方法，再给某个词加一个位置编码。
通过正余弦函数实现，**某个单词的位置信息是其他单词位置信息的线性组合，这种线性组合就意味着位置向量中蕴含了相对位置信息。**

# transformer

这里讲的是 transformer，这个时间点下大语言模型还没出现
那 transformer 是干嘛呢？
翻译！一句话进入译码器，骚操作生成词向量
词向量也要让机器看懂啊，那就塞进解码器

### encoder
一句话中的一个词，定义为 x 吧。用位置编码+selfattention，此时词向量 z，又有语义特征又有位置信息。然后再加残差，也就是 x 加 z！这个变换后加 layernorm，LayerNorm 是对相加后的结果做标准化。
这一步是为了防止丢失原本 x 的信息，最后在进入 feed forward 这个环节，额，就是深度学习中的激活函数概念。transformer 的 encoder 用的是 relu
**FFN 不仅是 ReLU**：
- 它是一个小型神经网络（含两层线性变换 + 激活函数
### decoder
我的理解是，decoder 是一个训练机器。在训练阶段，假如说已知的词是 a，翻译的为 b，那么 b会进入 decoder，首先仍然会做像 encoder 里面的操作，生成一个 b 的词向量，不过多了个掩码过程。然后，decoder 比 encoder 多的一步就是，a 词的词向量会和 b 词的词向量做 attention 计算。softmax 输入匹配概率，通过训练，能让机器知道 a 真的能翻译成 b
不过这里的 attention，是由 a 提供 k，v 矩阵，b 提供 q 矩阵。为啥？因为一个时序的问题：a 有五个字，b 有 4 个字，
还有的细节就是

1. **训练后的矩阵是否不再是“瞎的”？** ✅ **是的！** 训练后，WKWK​、WQWQ​、WVWV​ 会学习到有意义的模式，不再是随机初始化的“瞎矩阵”。
2. **每个词是否都有自己独立的矩阵？** ❌ **不是！** 所有词**共享同一组** WKWK​、WQWQ​、WVWV​ 矩阵，但通过词嵌入和位置编码，模型能动态生成每个词独特的 Key、Query、Value 向量。

|每个词是否有独立的矩阵？|否，所有词共享矩阵，但通过词嵌入和位置编码生成独特的向量。|