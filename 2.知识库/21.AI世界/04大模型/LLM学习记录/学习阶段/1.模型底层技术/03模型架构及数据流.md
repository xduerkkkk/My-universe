用户输入的自然语言，经过tokenizer后 
维度为`bsz,seq_len`
经过embedding层，选出每个词对应的向量，向量长度是hidden_size
所以当前维度是`bsz,seq_len,hidden_size`

此时还要做个两个外部的准备动作
一个是构造出位置编码表。
seq的不同位置，旋转长度都不一样。
我们准备max_seq_len长度的角度，这样seq_len多长我们都能兼顾到了。
另一个是检测kvcache的长度。同样方便我们的位置编码。取出`past_key_value`的长度。
取出长度，去确定接下来要注入的位置。

现在就正式进入transformerblock了。
送往block的forward方法的参数，有流动向量x，位置编码，pastkeyvalue，还有attentionmask。
这些参数有什么用呢？ 其实这些参数，都是送往block里的attn方法和mlp方法的。

block很简单，
记录残差， 归一化，  数据送往attn  ， 加残差。   attention返回新的hidden_state和key_value
记录残差， 归一化，  数据送往mlp ， 加残差。   mlp返回更新的hidden_states

送往block维度是`bsz,seq_len,hidden_size`  ，无论attn还是mlp，返回的维度也都是`bsz,seq_len,hidden_size`

下面就看看`bsz,seq_len,hidden_size` 在atten和mlp都发生什么了
送往block的forward方法的参数，有流动向量x，位置编码，pastkeyvalue，还有attentionmask。

其中，mlp只送进去了x。维度是`bsz,seq_len,hidden_size`
mlp中，采用了Swiglu。
传统的就是升维度，激活函数，再降维度。
本方法，升维时比较特殊，x放进两个升维的linear
一个，去乘门控激活函数。我们称作activated_gata
另一个升维后的linear，我们去乘刚才的activated_gata。
不是矩阵相乘，而是用符号`*` 意为逐元素相乘。
然后我们再下采样，linear，恢复维度。

下面关键的是attn
生成qkv向量，我们就用`linear(hidden_size,hiddensize)` 生成。
生成的向量，维度仍然是`bsz,seq_len,hidden_size`
我们手动拆分，拆分成`bsz,num_heads,seq_len,head_size`。

基于这个向量，我们先注入事先传入好的位置编码
然后提取事先传入好的past_key_value ，k提取出来cat到k向量，v提取出来cat到v向量。
即将**当前经过位置编码的**k和v，与**历史的**k和v进行**拼接 (torch.cat)**，得到**完整长度**的k和v。
这个拼接后的完整k和v，就是 present_key_value现在命名为present_key_value
接着就进行注意力点积计算。
先是qk矩阵矩阵乘法与缩放，成attn_weights，再是就应用attentionmask
再训练阶段无比重要，将mask`扩展 [bsz, seq_len] -> [bsz, 1, 1, seq_len]`
然后将attn_weights使用masked_fill_方法，把该遮住的地方遮住。
接着就是softmax，dropout，@v， 恢复维度（拼接多头）`bsz,seq_len,hidden_size`，
然后在最后整合的linear（hiddensize，hiddensize）完成最后的输出
`bsz,seq_len,hidden_size`


那么在model中，pastkeyvalue仍然非常重要
结构为二维，第一维代表层数，第二维只有两个，代表k和v
每一层循环
都取出相对应的层数的
`past_key_value[i]` 进行使用

```
all_presents = [] if use_cache else None

        for i,layers in enumerate(self.layers):

            layer_past_key_value = past_key_value[i] if past_key_value is not None else None

  

            hidden_states, present_key_value = layers(x=hidden_states,position_embeddings=position_embeddings,past_key_value=layer_past_key_value,use_cache =use_cache, attention_mask=attention_mask)

  

            if use_cache:

                all_presents.append(present_key_value)
```



那更新呢？
更新位于generate方法，
我们把all__present,即每一层记录的keyvalue，都替换pastkeyvalue，即可。
