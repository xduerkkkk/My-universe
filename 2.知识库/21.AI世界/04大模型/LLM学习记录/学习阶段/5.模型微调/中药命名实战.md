 ​**BIO（Begin, Inside, Outside）标注格式**，每行包含“词语 + 空格 + 标签”，空行分隔句子。

# 数据预处理
先看看原始数据
```
现 O
头 O
昏 O
口 B-临床表现
苦 I-临床表现

目 O
的 O
观 O
察 O
复 B-中医治疗
方 I-中医治疗
丁 I-中医治疗
香 I-中医治疗
开 I-中医治疗
胃 I-中医治疗
贴 I-中医治疗

...
```
BIO都是什么，上面表现的很清晰了，值得注意的是，似乎一段话完毕之后，有一个空行，我们使用`readlines` 函数的话，这个空行就是`'\n',` 显然，我们可以利用这个空行对数据集进行分割。

## 格式转换
我们转为json进行处理，

```python
def bio_to_jsonl(input_path,output_path):

    samples = []

    current_tokens = []

    current_labels = []

    with open(input_path, 'r', encoding='utf-8') as f:

            for line in f:

                line = line.strip()

                if not line:  # 遇到空行，保存当前样本

                    if current_tokens:

                        samples.append({

                            "tokens": current_tokens,

                            "labels": current_labels

                        })

                        current_tokens = []

                        current_labels = []

                    continue

                parts = line.split()

  

                current_tokens.append(parts[0])

                current_labels.append(parts[1:])

  

            if current_tokens:

                samples.append({"tokens": current_tokens, "labels": current_labels})

  

            # 写入JSONL文件（每行一个JSON）

            with open(output_path, 'w', encoding='utf-8') as out:

                for sample in samples:

                    json.dump(sample, out, ensure_ascii=False)

                    out.write('\n')

```

此时数据为dataset格式，我们想在数据基础上处理的话，想象针对dataset的一行，即一个字典处理
然后使用.map 就可以修改dataset格式了
这里我们剔除一些空列表

处理后的数据如下

```python
{'tokens': ['现', '头', '昏', '口', '苦'], 'labels': ['O', 'O', 'O', 'B-临床表现', 'I-临床表现']}
```


这个BIO命名实体中，我们统计一下实体

```
所有实体类型及出现次数: {'临床表现': 10030, '中医治疗': 4419, '西医诊断': 17175, '方剂': 5179, '中药': 6441, '中医诊断': 571, '西医治疗': 2111, '中医证候': 4650, '中医治则': 1011, '其他治疗': 283}
```

我们要进行适合sft格式微调的数据处理


```python
def format_ner_example(example):

    # tokens拼接为输入文本

    text = "".join(example["tokens"])

    # labels用空格拼接为输出标签序列

    labels_str = " ".join(example["labels"])

    return {

        "instruction": "你是一个中医药领域的命名实体识别专家。请对输入的中医药文本进行序列标注，识别以下实体类型：临床表现、中医治疗、西医诊断、方剂、中药、西医治疗、中医诊断、中医证候、中医治则、其他治疗。输出要求：1. 使用BIO标注体系 2. 每个字符对应一个标签 3. 标签之间用单个空格分隔 4. 若无非实体，使用'O'标签；例如：{ \"input\": \"桂枝加龙骨牡蛎汤合小陷胸汤治疗功能性消化不良临床观察\", \"output\": \"O O O O O O O O O B-方剂 I-方剂 I-方剂 I-方剂 O O B-西医诊断 I-西医诊断 I-西医诊断 I-西医诊断 I-西医诊断 I-西医诊断 I-西医诊断 O O O O\"}",

        "input": text,

        "output": labels_str

    }

  

# 应用转换

formatted_dataset = dataset.map(format_ner_example, remove_columns=dataset.column_names)
```
处理后的数据如下
```python
{'instruction': '你是一个中医药领域的命名实体识别专家。请对输入的中医药文本进行序列标注，识别以下实体类型：临床表现、中医治疗、西医诊断、方剂、中药、西医治疗、中医诊断、中医证候、中医治则、其他治疗。输出要求：1. 使用BIO标注体系 2. 每个字符对应一个标签 3. 标签之间用单个空格分隔 4. 若无非实体，使用\'O\'标签；例如：{ "input": "桂枝加龙骨牡蛎汤合小陷胸汤治疗功能性消化不良临床观察", "output": "O O O O O O O O O B-方剂 I-方剂 I-方剂 I-方剂 O O B-西医诊断 I-西医诊断 I-西医诊断 I-西医诊断 I-西医诊断 I-西医诊断 I-西医诊断 O O O O"}', 
'input': '方以茵陈蒿汤加味',
'output': 'O O B-方剂 I-方剂 I-方剂 I-方剂 O O'}
```

## 序列化
我们给模型训练的数据的格式是什么样的？
是序列化的input_ids 和 labels  
而序列化函数，返回字典`{input_ids:...,attention_mask:...,labels:..}`   是map在dataset格式的数据的

现在就应该处理这个数据集。将其序列化


**`input`**: 拼接系统指令（`system_prompt`）、用户输入（`example['input']`），并以 `<|im_start|>assistant\n` 提示模型开始生成
**`response`**: 对真实标签（`example['output']`）进行分词，作为生成目标

```python 
input = tokenizer(f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{example['input']}<|im_end|>\n<|im_start|>assistant\n")

response = tokenizer(f"{example['output']}")

```


最终输入的`input_ids、attention_mask` 是集提示词、用户输入、模型输出为一体
模型学习的标签labels，是模型输出
**自回归生成（Autoregressive Generation）** 训练中（如GPT、T5等），模型的输入需要包含完整的上下文    
在计算损失时，`-100` 会被忽略（不参与梯度更新），借此性质控制模型输入


```python
 input_ids = input['input_ids'] + response['input_ids'] + [tokenizer.pad_token_id]

    labels = [-100]*len(input['input_ids']) + response['input_ids'] + [tokenizer.pad_token_id]

    attention_mask = input['attention_mask'] + response['attention_mask'] + [1]
```

完整处理代码：

```python
def process(example):

    input_ids, attention_mask, labels, =[], [], []

    MAX_LENGTH = 500

    system_prompt = example['instruction']

    input = tokenizer(f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{example['input']}<|im_end|>\n<|im_start|>assistant\n")

    response = tokenizer(f"{example['output']}")

  

    input_ids = input['input_ids'] + response['input_ids'] + [tokenizer.pad_token_id]

    labels = [-100]*len(input['input_ids']) + response['input_ids'] + [tokenizer.pad_token_id]

    attention_mask = input['attention_mask'] + response['attention_mask'] + [1]

  

    if len(input_ids) > MAX_LENGTH:  # 做一个截断

        input_ids = input_ids[:MAX_LENGTH]

        attention_mask = attention_mask[:MAX_LENGTH]

        labels = labels[:MAX_LENGTH]

  
  

    return {

        "input_ids": input_ids,

        "attention_mask": attention_mask,

        "labels": labels

    }

```


之前提到过，针对dataset格式数据，只需要使用map函数就能处理了

```
final_data = train_data.map(process,remove_columns=train_data.column_names)
```

# 开始训练

本机只有8GB的gpu显存，要训练一定要用QLoRA
其中“量化：在于初始化模型时的

```
quantization_config=bnb_config 
```

## 初始化
下面一口气初始化模型和tokenizer   

```python
model_id = "qwen/Qwen3-4B"

bnb_config = BitsAndBytesConfig(

    load_in_4bit=True,                  

    bnb_4bit_use_double_quant=True,    

    bnb_4bit_quant_type="nf4",          

    bnb_4bit_compute_dtype=torch.float16

)
 
model = AutoModelForCausalLM.from_pretrained(

    model_id,  

    device_map="auto",

    trust_remote_code=True,

    quantization_config=bnb_config  

)


tokenizer = AutoTokenizer.from_pretrained(

    model_id,

    trust_remote_code=True,

    padding_side="left",

    truncation_side="right"

)


tokenizer.pad_token = tokenizer.eos_token


```


- **`pad_token`**：用于将不同长度的序列填充到相同长度的特殊令牌（如 `<pad>`）。
    
- **`eos_token`**：序列结束符（如 `<eos>`、`</s>`）。
 将填充令牌设为与结束符相同，**避免引入新的特殊令牌**（某些模型如 LLaMA 没有预定义的 `pad_token`）。确保模型在训练/推理时能正确处理填充位置

- **`padding_side="left"`**：指定在填充（padding）时，在序列的 **左侧** 添加填充符（`pad_token`） 生成式模型（如 GPT、LLaMA）通常从左到右生成文本，左侧填充能保持生成方向的一致性 若设为 `"right"`，可能干扰自回归生成。
            
- **`truncation_side="right"：`指定在截断（truncation）时，从序列的 右侧** 移除超长部分。优先保留左侧的提示词和关键输入，右侧截断对生成任务影响较小

下面进行lora
先配置lora参数，再对模型进行应用
其中，target_modules需要自己打印出模型，选择
**注意力层**  **前馈网络**
```python
from peft import LoraConfig, get_peft_model


lora_config = LoraConfig(

    r=8,  # LoRA注意力维度（8足够， smaller=更省显存）

    lora_alpha=16,  # 缩放参数（16比32更省显存）

    target_modules=["q_proj", "k_proj", "v_proj", "o_proj","gate_proj", "up_proj", "down_proj"],  

    lora_dropout=0.05,

    inference_mode=False,

    bias="none",

    task_type="CAUSAL_LM"

)

  

# 应用QLoRA

model = get_peft_model(model, lora_config)

model.print_trainable_parameters()
```


## 配置训练
我们使用transformer的trainer训练

定义训练参数

```python
training_args = TrainingArguments(
    # === 显存优化 ===
    per_device_train_batch_size=1,          # 单设备批次大小（根据显存调整）
    gradient_accumulation_steps=16,         # 梯度累积步数（等效批次大小=16）
    fp16=True,                              # 混合精度训练（A100/V100建议用bf16）
    gradient_checkpointing=True,            # 梯度检查点（节省显存）

    # === 优化器 ===
    optim="adamw_bnb_8bit",                 # 8-bit AdamW（需bitsandbytes库）
    max_grad_norm=0.5,                      # 梯度裁剪阈值

    # === 训练调度 ===
    learning_rate=1e-4,                     # 初始学习率
    lr_scheduler_type="cosine",             # 余弦退火学习率
    warmup_ratio=0.03,                      # 预热步数比例（3%总步数）

    # === 训练控制 ===
    num_train_epochs=2,                     # 训练轮次
    logging_steps=10,                       # 每10步打印一次日志（显示步数！）
    save_steps=200,                         # 每200步保存一次模型
    evaluation_strategy="steps",            # 按步数评估（需提供验证集）
    eval_steps=200,                         # 每200步评估一次

    # === 输出与日志 ===
    output_dir="./qlora_output",             # 模型保存路径
    report_to="swanlab",                    # 日志工具（可选"tensorboard"）
    remove_unused_columns=False,            # 保留无用列（避免数据错误）
    dataloader_pin_memory=False,            # 禁用内存锁定（可能提升速度）

    # === 其他 ===
    label_names=["labels"],                 # 标签字段名
    save_on_each_node=True                  # 分布式训练时每个节点保存
)
```


定义数据收集器

```python
from transformers import DataCollatorForSeq2Seq

data_collator=DataCollatorForSeq2Seq(

    tokenizer,

    padding=True,

    return_tensors="pt",

    label_pad_token_id=-100  # 忽略填充部分的损失计算

)
```

开始训练

```python
from transformers import Trainer, DataCollatorForSeq2Seq

trainer = Trainer(
    model=model
    args=training_args,  
    train_dataset=final_data,
    eval_dataset=None,    # 无验证集时设为 None
    data_collator=DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        padding=True,
        pad_to_multiple_of=8  # 可选：按8的倍数填充（加速GPU计算）
    )
   
)

trainer.train()

```
注意trainning_args里logging_steps设置小一点，比如设置成10.如果忘了设置，会默认500
这样如果步数小于1000 就只有1个点，无法绘图
结果就只能如下
![[中药命名实战-1751949055140.jpeg]]
# 评估数据集

## F1 score
- 精确率：模型预测正确的实体占所有预测实体的比例
- 召回率：模型预测正确的实体占所有真实实体的比例
TP（True Positive）：模型正确预测的实体数量。 FP（False Positive）：模型误判为非实体的数量。 FN（False Negative）：模型漏检的真实实体数量

## 开始评估
对于训练好的模型，我们使用它，最好封装一个predict函数
流程是`tokenizer.apply_chat_template 后tokenizer([text], return_tensors="pt").to(device)` 序列化文本  
model.generate生成序列
截取序列的input_ids后·
tokenizer.batch_decode解码
返回人类语言

```python
def predict(messages, model, tokenizer):

    device = "cuda"

    text = tokenizer.apply_chat_template(

        messages,

        tokenize=False,

        add_generation_prompt=True

    )

    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(

        model_inputs.input_ids,

        max_new_tokens=512,

        pad_token_id=tokenizer.eos_token_id  # 避免注意力掩码警告

    )

    generated_ids = [

        output_ids[len(input_ids):]

        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)

    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return extract_labels(response)

```

下面实现f1指标评估函数
核心在于关键指标的计算


```
for ent_type in entity_types:

            true_set = {e for e in true_entities if e[0] == ent_type}

            pred_set = {e for e in pred_entities if e[0] == ent_type}

            stats[ent_type]['TP'] += len(true_set & pred_set)

            stats[ent_type]['FP'] += len(pred_set - true_set)

            stats[ent_type]['FN'] += len(true_set - pred_set)
```
完整：
```python
def evaluate_ner_predictions(predictions, entity_types):

    """

    评估NER预测结果，计算每个实体类别的F1分数

    :param predictions: 预测结果列表 [{'input':, 'true_output':, 'pred_output':}]

    :param entity_types: 实体类别列表

    :return: 评估结果字典

    """

    from collections import defaultdict

    import re

    # 初始化统计字典

    stats = {ent: {'TP': 0, 'FP': 0, 'FN': 0} for ent in entity_types}

    def extract_entities(tag_sequence):

        """从BIO标签序列中提取实体"""

        entities = []

        if not isinstance(tag_sequence, list):

            tag_sequence = tag_sequence.split()

        current_entity = None

        for i, tag in enumerate(tag_sequence):

            if tag.startswith('B-'):

                if current_entity: entities.append(current_entity)

                entity_type = tag[2:]

                current_entity = (entity_type, i, i)

            elif tag.startswith('I-'):

                entity_type = tag[2:]

                if current_entity and current_entity[0] == entity_type:

                    current_entity = (entity_type, current_entity[1], i)

                else:

                    if current_entity: entities.append(current_entity)

                    current_entity = (entity_type, i, i) if entity_type in entity_types else None

            else:

                if current_entity:

                    entities.append(current_entity)

                    current_entity = None

        if current_entity: entities.append(current_entity)

        return set(entities)  # 去重

    # 遍历样本

    for pred in predictions:

        true_tags = pred['true_output'].split()

        pred_tags = pred['pred_output'].split()

        # 确保长度一致

        min_len = min(len(true_tags), len(pred_tags))

        true_tags = true_tags[:min_len]

        pred_tags = pred_tags[:min_len]

        # 提取实体

        true_entities = extract_entities(true_tags)

        pred_entities = extract_entities(pred_tags)

        # 统计各类别

        for ent_type in entity_types:

            true_set = {e for e in true_entities if e[0] == ent_type}

            pred_set = {e for e in pred_entities if e[0] == ent_type}

            stats[ent_type]['TP'] += len(true_set & pred_set)

            stats[ent_type]['FP'] += len(pred_set - true_set)

            stats[ent_type]['FN'] += len(true_set - pred_set)

    # 计算指标

    results = {}

    for ent_type in entity_types:

        TP = stats[ent_type]['TP']

        FP = stats[ent_type]['FP']

        FN = stats[ent_type]['FN']

        precision = TP / (TP + FP) if (TP + FP) > 0 else 0

        recall = TP / (TP + FN) if (TP + FN) > 0 else 0

        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        results[ent_type] = {

            'precision': round(precision, 4),

            'recall': round(recall, 4),

            'f1': round(f1, 4)

        }

    return results
```
最后集中使用predict函数和evaluate函数

```python
entity_types = ['中医治则', '中医治疗', '中医证候', '中医诊断', '中药',

                '临床表现', '其他治疗', '方剂', '西医治疗', '西医诊断']

  

predictions = []

for sample in test_samples:  

    messages = [

        {"role": "system", "content": sample["instruction"]},

        {"role": "user", "content": sample["input"]}

    ]

    pred_output = predict(messages, model, tokenizer)

    predictions.append({

        'input': sample["input"],

        'true_output': sample["output"],

        'pred_output': pred_output

    })

  

# 3. 评估各类别F1

results = evaluate_ner_predictions(predictions, entity_types)

  

# 4. 可视化结果

print("实体类别\tF1分数\t精确率\t召回率")

for ent, metrics in results.items():

    print(f"{ent}\t{metrics['f1']:.4f}\t{metrics['precision']:.4f}\t{metrics['recall']:.4f}")
```



最终效果如图，因为只搞了5个测试样本 加上用的笔记本，效果没体现
但流程很清晰了


![[中药命名实战-1751952750268.jpeg]]


# 改进 

```
def process(example):
    # 1. 更安全的模板拼接（避免额外空格）
    messages = [
        {"role": "system", "content": example['instruction']},
        {"role": "user", "content": example['input']},
        {"role": "assistant", "content": example['output']}
    ]
    tokenized = tokenizer.apply_chat_template(
        messages,
        truncation=True,
        max_length=500,
        padding="max_length"
    )
    
    # 2. 直接返回（HuggingFace的apply_chat_template已处理labels）
    return tokenized

```
