# 有监督微调基本概念
指令，其实就是一组问答
构建方法
- 手动构建：网上收集大量问答数据，然后人工筛选过滤
- 现有数据集转换：可以简单理解为，再原有问答数据基础上，这里添一添，那里改一改，然后视作“新的数据集”。当然这样描述不严谨，应该称“结构化重构”，只是这样说便于理解。
- 让模型自己生成指令
我们有指令了，然后呢？
其实就跟当初模型预训练（模型预训练是给原来说话随机吐字的模型，喂海量的互联网文本语料，让模型理解人类语言，从而能有基本的“说话”能力）一样，给模型这些问答数据，让模型“悟”这些问答传递出来的知识。
这个过程会直接反向传播，修改模型参数。

因为前面基础知识有可能不太够，你可能会有的问题是：这又是拿数据训练模型，这不和最初的预训练没区别吗？不应该叫训练新模型吗，怎么叫微调
预训练时模型的参数是随机初始化的，好比生一个娃，而微调时，模型参数的初始状态是预训练好的参数。好比让这个娃学习某个专业技能。如果说“重新训练模型”的话，就得让模型重新随机初始化。
# 高效模型微调
即使模型参数的初始状态是预训练好的参数，我们进行微调时，所有参数都有可能进行修改。
那我们能不能在仅训练少量参数就使模型适应下游任务（下游任务比如医疗领域，法律领域，指想让模型学习哪方面知识）

PEFT的全名是**Parameter-Efficient Fine-Tuning**，中文译为**参数高效微调**

研究人员就思考啊，他们经过实验发现
那么大规模的参数，微调后参数的有效改变量其实很小
那我能不能保留绝大部分参数，训练时根本不让他们参与，而只是训练很少的参数，把他们的矩阵拿出来训练，这样效果怎样？
诶，发现还真的非常有效，和初始的“全微调”效果是完全接近的！
当然，问题也来了。是选一点参数微调就够了，但到底选哪些参数进行微调呢？
最后这些参数微调后，如何合并到原来的模型里呢？真的光修改这一小块地方就可以吗？

诶，这就和transformer架构有关了
大模型的核心，或大脑，就在Q/V权重中，所以我们是基于结构特性精准打击的。而且，这些部分原始参数也是不动的，我们只是在这些部分的基础上，加可训练的新的小参数矩阵，视作新的参数。合并矩阵加法，还真是光修改这一小块地方就够了。

厘一下思路
模型有很多大矩阵，大A，大B，大C.....大Q
我们选择大Q矩阵进行微调，给大Q矩阵的基础上加可训练的新的参数矩阵，相当于改变大Q矩阵参数。

再多讲一点，这个可训练的新的参数矩阵也有技巧的，
让ai举例了，
4096×11008个参数，和4096×16 + 16×11006个参数谁多，不用我说吧？
这里也是利用矩阵乘法的性质在参数训练上减小了很多负担。
![[指令微调入门-1749306387930.jpeg]]

$Output = x · W_Q + x · (LoRA_B · LoRA_A)$
原始 WQ **纹丝不动**，新增的**低秩矩阵** ΔW=$LoRA_B · LoRA_A$学习任务知识

# 其他高效微调
刚才介绍的最主流的lora，其效果还是“改变参数”，只不过改变得有点假哈，好比给参数加了件衣服，但是这个衣服你想拿掉也可以。
下面介绍一些微调方法，可以说模型的参数一点没改变！
## p-tuning
修改prompt。在用户的prompt加上虚拟的向量，诶，在喂指令数据集的过程中，就把这个向量训练了。面对实际的下游任务时，模型实际接受的输入和用户的prompt是有区别的，那个向量让模型对prompt的回应更精确了。

## iA3
缩放向量，但不是缩放prompt向量，
而是缩放transformer架构（模型内部）中的部分向量
比如是情感下游任务，那就把跟情绪有关的向量放大
所以没有改变模型参数，只是加了个放大器