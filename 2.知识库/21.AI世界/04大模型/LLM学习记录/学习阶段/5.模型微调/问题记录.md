```python
target_modules=[  # 覆盖所有关键模块
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention 层
        "gate_proj", "up_proj", "down_proj"       # MLP 层（必须添加！）
```  
<font color="#f79646">是先启用量化再lora 还是先lora再4bit量化</font>


**`per_device_train_batch_size=1` + 增大 `gradient_accumulation_steps`** 是显存优化的黄金组合。
**显存占用对比**：

|策略|单步显存占用|显存优化效果|
|---|---|---|
|`batch_size=8`|高（直接处理8个样本）|❌ 显存需求大|
|`batch_size=1` + `grad_accum=8`|低（每次处理1个样本，累计8次）|✅ 显存降低8倍|为啥为

<font color="#f79646">为啥呀/？/</font>

- 如果数据集极大（如数TB），且每个 epoch 需要加载全部数据，可能触发内存溢出（OOM）。
    
- **解决**：
    
    - 使用流式加载（如 `datasets` 库的 `load_from_disk` 或迭代器）

<font color="#f79646">model.enable_input_require_grads()是干嘛的 </font>

<font color="#f79646">我已经训练好了 在之前trainargs设置的outdir 下面出现了chekpoint文件 我是不是用这个文件加载训练好的模型？如何快速加载</font>



```
instruction = tokenizer( f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{example['input']}<|im_end|>\n<|im_start|>assistant\n" ) 
与 
input = tokenizer(f"<system>{example['instruction']}</system>\n<user>{example['input']}</user>\n")
```
<font color="#f79646"> 对于命名实体任务来说 哪个好</font>

- ✅ **生成式NER**：模型直接生成实体文本和类型（如T5、GPT风格的模型）。
    
- ❌ **传统Token分类NER**：如果任务是给每个Token打标签（如BERT的BIO标注）

序列化完 最终输入的`input_ids、attention_mask` 是集提示词、用户输入、模型输出为一体
模型学习的标签labels，是模型输出 为什么？ 输入为什么还有模型输出