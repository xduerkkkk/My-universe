# 1.Prompt design, Prompt tuning 还有finetuning的区别是什么?
答：首先要搞清楚这三个东西的语境都是大模型微调，第一个不改变模型权重，后两者改变模型参数
prompt design是提示词设计，修改提示词来优化模型的输出。也就是训练时就用设计好的提示词训练，最后用模型的时候，用户也用设计好的提示词模板
prompt tuning：用户的prompt本身不用改，我们给用户prompt内置一个向量，这个向量是可以不断被训练的，面对实际的下游任务时，模型实际接受的输入和用户的prompt是有区别的，那个向量让模型对prompt的回应更精确了。
finetuning，就真实地改变模型参数了，对模型参数进行改动，有全参数微调和参数高效微调两种方式       

# 2.参数高效的fine-tuning(PEFT)是什么?
仅训练少量参数，保留原本的大部分参数，即保留预训练时的学习的通用知识
如LoRA 冻结原先参数，在一些特定的层，加入训练的低秩矩阵，最终训练的参数可能只占总参数的百分之0.5 但是效果是全参数微调的百分之90以上
因为通用知识是已训练好的，如何全参数微调会重新学一次通用知识，下游任务仅仅对特定任务敏感   数学角度来讲，整体参数大矩阵的增量可以分解为两个低秩矩阵的乘积     小空间参数的敏感性实验表面也很高
# 3.介绍下prompt-tuning
可输入文本前加入一段可被训练的向量，这个向量实际上可以引导模型有着不同的输出


# 4.什么是Prefix tuning?
prompt tuning是把虚拟向量加到提示词前，prefix tuning是在每一层kv value矩阵前都加入一个虚拟向量 训练时他们都训练上。使用模型时，可以选择是否启用这些修饰向量，如果启用，针对特定领域的问题回答就会如微调时一样，如果不启用就仍然是初始预训练一样，有“ **动态拔插**”的效果


| ​**步骤**​      | Key矩阵形状             | Value矩阵形状           |
| ------------- | ------------------- | ------------------- |
| 原始输入          | `[1, 8, 768]`       | `[1, 8, 768]`       |
| 添加Prefix（训练时） | `[1, 10+8=18, 768]` | `[1, 10+8=18, 768]` |

# 5.介绍-下LORA微调
选取特定的层，比如kv层，其余层参数都进行冻结。大模型权重矩阵是高维低秩的，所以我们使用两个低秩矩阵相乘，作为原始矩阵的增量，比起直接在原始矩阵上加一个同样形状的矩阵，这个操作使得训练的参数非常少，还达到全微调效果的百分之90。

# 6.相比LORA，AdaLORA的改进点是什么?

lora设定好低秩矩阵的秩的大小后，就固定了，每一个模块都是一样的低秩矩阵。
AdaLoRA针对秩进行调整吗，重要的模块用高秩，次要模块用低秩，使资源用到刀刃上
还有LoRA只针对attention模块，AdaLoRA还覆盖了FFN模块
# 7.QLORA模型有什么创新点?
在进行传统的微调前，先进行量化即Quantization   
先是4bit量化，因为权重服从正态分布，将权重分为一些区间 在相同区间的权重干脆就当成一个4bit什么意思呢？  分的区间是**16个区间**，每个区间用1个数表示。那所有数是不是就4bit范围（2的4次方）
接着进行双重量化，
# 8.稀疏微调是怎么工作的，有哪几个步骤?
稀疏微调是低资源部署需求，同时追求推理效果时使用
首先物理剪枝参数，即大的参数块之间去掉，置零或整行/列删除，这些参数不再参与训练
对于剩余参数，用蒸馏损失来继承预训练知识的同时，更新新任务的学习能力。
齐大模型和小模型最后一层的输出


# 9.监督微调SFT后LLM表现下降的原因
首先数据要量够，且标注正确。假设数据没问题，llm出现的问题可能是灾难性遗忘，即过度专注新任务，权重被大幅更新，导致预训练时学到的通用能力被破坏，知识蒸馏、增加正则约束可以缓解这个问题
也有可能模型过拟合，泛化能力不足所以能力下降，这样的化采用正则化，dropout，或者早停法。
# 10什么是P-Tuning?
可以跟prompt-tuning对比着看，prompt-tuning是在输入文本的前面添加可训练的向量，p-tuning的意思是在输入的任意一个位置都可以添加向量，也可以添加多个

# 11.多轮对话任务如何微调模型?
主要是数据集的处理，要设置好上下文格式，拼接多轮对话，并且用特殊符合，如`<user> <assistant>`  选择自回归语言模型， 使用peft微调

