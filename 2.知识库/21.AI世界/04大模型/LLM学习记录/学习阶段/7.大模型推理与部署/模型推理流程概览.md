### 从输出向量到概率分布：采样前的准备

你说得完全正确！经过了N层Decoder Block的处理后，我们得到的最终输出是一个形状为 (batch_size, seq_len, d_model) 的张量。

在Prefill阶段，我们只关心**最后一个词**的预测，所以我们取出这个张量的最后一个时间步：  
last_hidden_state = final_output[:, -1, :]  
这个张量的形状是 (batch_size, d_model)。

**但是，这还不是我们可以直接用来采样的东西！** 它只是一个包含了丰富上下文信息的d_model维特征向量。我们需要把它转换成对**整个词汇表**的预测。

这一步通过一个**最终的线性层**来完成，这个层有时被称为lm_head（Language Model Head）：

1. 这个lm_head是一个权重矩阵，形状为 (d_model, vocab_size)。
    
2. 我们用它和last_hidden_state进行矩阵乘法：  
    logits = last_hidden_state @ lm_head.T (或者 lm_head(last_hidden_state))
    
3. 得到的 logits 张量，形状是 (batch_size, vocab_size)。这个logits就是模型对词汇表中每一个词的原始打分，分数越高，代表模型认为这个词是下一个词的可能性越大。
    
4. **最后，通过Softmax函数**，将这些原始分数转换成一个**概率分布**：  
    probabilities = softmax(logits, dim=-1)  
    这个probabilities张量的形状还是(batch_size, vocab_size)，但它里面的每一个值都在0到1之间，并且每一行的和都等于1。
    

**现在，我们终于拿到了可以进行采样的“原材料”——一个代表了下一个词所有可能性的概率分布。
**
**Top-P (Nucleus) 采样 (Top-P Sampling)**

- **方法**：这是目前**最主流、效果最好**的采样策略之一，它比Top-K更灵活。
    
    - **第一步：排序**。把所有词按概率从高到低排序。
        
    - **第二步：累加筛选**。从概率最高的词开始，依次往下加，直到它们的**概率之和超过一个阈值 P**（比如P=0.95）。我们把这些词构成一个**动态的候选集**（这个集合被称为“核心/Nucleus”）。
        
    - **第三步：重新计算概率和采样**。和Top-K一样，对这个“核心”候选集里的词重新计算概率，然后进行加权随机抽样。
        
- **优点**：候选集的大小是**动态变化**的！
    
    - 当模型对下一个词非常确定时（比如“中国的首都是”后面，“北京”的概率可能高达0.98），那么候选集里可能就只有“北京”这一个词，保证了答案的准确性。
        
    - 当模型不太确定，有很多可能的选择时（比如写小说的开头），它可能会包含几十上百个词，保证了生成的多样性。

# 大模型推理存在的瓶颈
- 模型响应时间，给用户体感带来的延迟
- 多个用户多个请求，模型的高并发能力
- 模型容量所占的存储空间
- 模型对硬件的适配
- 上述的解决方法都是提升模型精度和效率，但二者一定是相对对立的，如何权衡的模型精度与效率
# 解决方案
算法层面
- decoding算法优化
- 架构优化
- 模型压缩：蒸馏、剪枝
系统层面
- 模型量化：AWQ，GPTQ
- 并行计算：gpu的使用方式升级，张量并行和流水线并行。DeepSpeed，T
- 内存管理：VLLM， TensorRT-LLM等框架
- 请求调度：VLLM， TensorRT-LLM等框架
- kernel优化： **FlashAttention** (优化Prefill), **FlashDecoding** (优化Decode)

# 最先进的开源基于 GPU 的 LLM 服务系统
VLLM，TensorRT-LLM， MLC-LLM， DeepSpeed
# 显存管理
pytorch  pageattention       VM Allocator
我的理解是，pytorch是一次性提供最大空间，任你使用 Vm AI locator是训练后的产物，能预测提供空间，是pytorch版的进化。 pageattention是随便用，把显存以快的单位分出来，当你需要的时候再给你分配一块，你还需要继续给你分配。pageattention不能预测用户请求的多少，用户请求也无上限，但存在显存占用过高时，新用户来了没地方分配的情况。

# 量化
在服务端推理时，矩阵乘法所需时间占推理百分之70以上，使用int8量化加速矩阵乘法
weight only不适于服务端？解量化？
### 1. “什么是服务端？” & “矩阵乘法占比70%”

- **什么是服务端？**
    
    - 在这里，“服务端”就是指**提供大模型推理服务的后端服务器**，通常由一堆高性能GPU组成。它和你的“客户端”（比如你的网页、手机App）相对。
        
    - 当你向ChatGPT提问时，你的问题被发送到OpenAI的服务器上，模型在这些服务器的GPU上运行，生成答案，再把答案返回给你的浏览器。这个过程就叫**服务端推理**。
        
- **“矩阵乘法所需时间占推理百分之70以上”**
    
    - 这是一个非常关键的性能剖析（Profiling）结论。它告诉我们，一个Transformer模型在GPU上进行推理时，**绝大部分时间都花在了执行矩阵乘法上**。
        
    - 哪些地方有矩阵乘法？我们已经很熟悉了：
        
        1. Q = x @ W_Q，K = x @ W_K，V = x @ W_V (QKV投射)
            
        2. Scores = Q @ K.T (注意力分数计算)
            
        3. Output = Scores @ V (注意力值加权)
            
        4. FFN层里的两个线性层。
            
    - **结论**：既然70%的时间都花在这上面了，那么，**加速矩阵乘法，就是加速整个推理过程最有效的手段**。
        

---

### 2. “量化不只是改变矩阵权重吗，为什么还能加速？”

你这个问题问到了点子上！量化能加速，主要有两大原因：

**A. 更少的数据传输量（内存带宽优化）**

- 我们之前讨论过，推理，尤其是Decoding阶段，是**内存带宽密集型（Memory-Bound）**的。瓶颈在于把数据（权重、KV Cache）从慢速的显存（HBM）搬到飞快的计算核心（SM）上。
    
- 假设一个权重原来是FP16（16位）。现在你把它量化成了INT8（8位）。
    
- 这意味着，在同样的时间内（比如1纳秒），GPU能从显存里**多搬一倍**的数据到计算核心！
    
- 数据搬得快了，计算核心就不会“饿肚子”等着，整体的计算速度自然就提升了。
    

**B. 更快的硬件原生计算（计算本身优化）**

- 这才是最主要的加速来源！
    
- 现代NVIDIA GPU（从Volta架构开始）内部有专门为低精度计算设计的硬件单元，叫做**Tensor Cores**。
    
- 这些Tensor Cores执行**INT8整数矩阵乘法**的速度，理论峰值（TOPS, Trillions of Operations Per Second）可以达到执行**FP16浮点数矩阵乘法**的**2倍**！
    
- 所以，当你把权重和激活值都量化成INT8后，你就可以调用这些“涡轮增压”的INT8计算单元，从而实现矩阵乘法本身的加速。
    

**总结**：量化通过**“减少搬运量”**和**“使用更快的专属计算车道”**这两个途径，实现了对矩阵乘法的显著加速。

---

### 3. “Weight-Only是什么意思？” & “为什么不适于服务端？”

这里我们需要对比两种主流的量化方式：

**A. Weight-Only Quantization (仅权重量化)**

- **是什么？**：顾名思义，它**只对模型的权重（Weights）**进行量化（比如转成INT8或INT4）。而在进行矩阵乘法时，**激活值（Activations）**，也就是我们流动的输入数据x，**依然保持为FP16格式**。
    
- **计算过程**：在做y = W_q @ x_fp16这个运算时，需要先把量化的权重W_q**动态地反量化（Dequantize）**回FP16，然后再进行一次**FP16的矩阵乘法**。
    
- **优点**：
    
    - 实现简单，对精度的影响相对较小，因为激活值没有被量化。
        
    - 能享受到量化的**第一个好处**：减小了模型存储和内存带宽压力。
        
- **缺点**：
    
    - **无法享受量化的第二个好处！** 因为最终执行的还是FP16矩阵乘法，所以它**不能利用GPU硬件的INT8加速能力**。
        
    - 动态反量化的过程本身也带来了一些额外的计算开销。
        

**B. W8A8 Quantization (权重和激活值都是8位)**

- **是什么？**：这是一种更彻底的量化。它不仅量化权重，也**动态地量化激活值**。
    
- **计算过程**：在做y = W_q @ x_q时，它会先把FP16的激活值x也动态地量化成INT8，然后执行一次**纯粹的、高速的INT8矩阵乘法**，最后再把INT8的结果反量化回FP16。
    
- **优点**：
    
    - 能**完全利用**GPU的INT8 Tensor Cores，获得巨大的速度提升。
        
- **缺点**：
    
    - 对激活值进行量化，引入了更多的量化误差，对模型精度的挑战更大。
        
    - 实现更复杂。
        

**现在，我们回到那个结论：“Weight-Only不适于服务端？”**

这句话的意思是：对于追求**极致吞吐量和低延迟**的服务端推理场景，我们最看重的是**计算速度**。Weight-Only量化虽然节省了内存，但它**没有利用到硬件的INT8计算加速能力**，这对于性能攸关的服务端来说，是不可接受的。