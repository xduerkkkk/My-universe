量化基本操作

# 数据类型
fp32 fp16 bf16 fp8
**. FP32 (单精度浮点数 / float)**

- **比特数**：32位。这是传统的、标准的浮点数表示。
    
- **结构**：1位符号，8位指数，23位尾数。
    
- **特点**：精度高，数值范围广。是模型训练和科学计算的基准。但也是最占空间和内存带宽的。
    

**2. FP16 (半精度浮点数 / half)**

- **比特数**：16位。
    
- **结构**：1位符号，5位指数，10位尾数。
    
- **特点**：
    
    - **优点**：占用空间和带宽是FP32的一半，计算速度在支持的硬件（NVIDIA Volta架构及之后）上快得多。
        
    - **缺点**：**数值范围（range）非常小**。由于指数位只有5位，它能表示的最大数和最小数都很有限。在训练非常深的网络时，梯度可能会变得非常小，超出FP16的表示范围，变成0，导致训练不稳定（梯度消失）。
        
    - **现状**：是当今LLM**推理**和**存储权重**的主流格式。
        

**3. BF16 (BFloat16 / Brain Floating Point)**

- **比特数**：16位。这是Google为了解决FP16的缺点而设计的。
    
- **结构**：1位符号，**8位指数**，7位尾数。
    
- **特点**：
    
    - **优点**：它的**指数位数和FP32一样**！这意味着它的**数值范围和FP32几乎一样广**。因此，在训练时用BF16，基本不会遇到梯度消失的问题，训练过程非常稳定。
        
    - **缺点**：它的**精度（precision）比FP16低**，因为尾数位更少。
        
    - **现状**：是当今LLM**训练**的主流格式。NVIDIA Ampere架构及之后的GPU都对它有很好的支持。
        

**一句话总结FP16和BF16**：

- **FP16**：精度高，但范围小，适合推理。
    
- **BF16**：范围广，但精度略低，适合训练。
    

**4. FP8 (8位浮点数)**

- **比特数**：8位。这是最新的、更极致的浮点数格式，在NVIDIA Hopper架构（如H100）上被引入。
    
- **结构**：它也有两种变体（E4M3和E5M2），分别侧重于精度和范围。
    
- **特点**：比16位浮点数更快、更省内存。它被认为是未来进一步提升LLM训练和推理效率的关键技术之一。
    
- **现状**：还比较新，主要在最顶级的硬件上使用，生态还在发展中。
# 训练后量化
所有PTQ的核心，都是找到一个映射关系，把一个范围内的浮点数，映射到整数上。比如，把 -1.0 到 1.0 之间的浮点数，映射到 -127 到 127 之间的整数。  
这个映射关系就是： 浮点数 ≈ 缩放因子 (scale) * （整数 + 零点 (zero-point)）
![[模型量化-1754379395265.jpeg]]
# 量化感知训练
**. 遇到的问题是什么？**

- 训练后量化（PTQ）虽然方便，但把一个FP16模型直接硬转成INT8，精度损失还是比较明显的。因为模型在训练时，完全不知道自己将来会被“压缩”，它没有为此做任何准备。
    

**2. 核心思想是什么？**

- 在**训练或微调的过程中**，就让模型**“感知”**到量化操作会带来的误差，并学着去**“适应”**和**“补偿”**这种误差。
    

**3. 具体流程是怎样的？**

- 在训练的前向传播过程中，我们会插入一些**伪量化节点 (Fake Quantization Nodes)**。
    
- 一个典型的伪量化流程是：
    
    1. 拿到一个正常的、高精度的权重或激活值（FP32）。
        
    2. **模拟量化**：把它按照我们之前讨论的scale和zero-point的方法，转换成一个低比特的整数（比如INT8）。
        
    3. **立刻模拟反量化**：再把这个INT8的整数，用同样的scale和zero-point，转换回FP32的浮点数。
        
- 这个经过“量化->反量化”过程后的浮点数，和原始的浮点数相比，已经带上了**量化误差**。
    
- 然后，我们用这个**带有误差的权重/激活值**，去继续进行后续的计算。
    
- 在反向传播时，梯度会正常地流过这些伪量化节点。模型为了让最终的损失变小，就会在更新权重时，**自动地调整自己的参数分布**，使得这些参数在经过量化后，产生的误差尽可能小。
# 混合精度训练
**. 遇到的问题是什么？**

- 用FP32训练，太慢了，显存也吃不消。
    
- 如果直接暴力地把所有东西都换成FP16，虽然快了，但由于FP16的数值范围太小，在反向传播时，很多微小的梯度会变成0（下溢/underflow），导致模型不收敛，训练失败。
    

**2. 核心思想是什么？**

- “混合精度”这个名字起得非常好。它的思想是：**我们不搞一刀切，而是在训练的不同环节，聪明地使用不同精度的浮点数，取其长，避其短。**
    

**3. 具体流程是怎样的？(以FP16混合精度为例)**  
一个典型的混合精度训练迭代步骤如下：

- **a. 维护一份FP32的“主权重” (Master Weights)**：在内存中，模型的权重始终以高精度的FP32格式存储一份。这是为了保证权重更新的精确性，防止微小梯度更新的累积误差。
    
- **b. 前向传播 (Forward Pass) 使用FP16**：
    
    - 在每次前向传播开始时，把FP32的主权重**转换**成FP16。
        
    - 然后，用这个FP16的权重和FP16的输入数据，在GPU上进行**高速的矩阵运算**。这是加速的主要来源。
        
    - 计算出模型的输出，并与标签计算损失（Loss）。
        
- **c. 损失缩放 (Loss Scaling)**：
    
    - 在进行反向传播**之前**，这是一个至关重要的步骤！
        
    - 我们把计算出的损失值，乘以一个很大的系数（比如1024），这个系数叫**缩放因子 (scale factor)**。
        
    - **为什么这么做？** 根据链式法则，损失被放大了，它在反向传播时产生的梯度也会被同等放大。这就巧妙地把那些原本可能因为太小而在FP16下变成0的梯度，“抬”进了FP16能够表示的有效范围内，防止了梯度下溢。
        
- **d. 反向传播 (Backward Pass) 使用FP16**：
    
    - 用这个被放大了的损失，进行反向传播。整个过程中的梯度计算也是在FP16下高速进行的。
        
- **e. 更新主权重 (在FP32下进行)**：
    
    - 得到的FP16梯度，在更新主权重之前，要先**除以**之前乘的那个缩放因子，把它恢复到正确的量级。
        
    - 然后，把这个恢复后的、可能很小的梯度，**转换回FP32格式**。
        
    - 最后，用这个FP32的梯度，去更新我们维护的那份FP32的主权重。
        

**总结**：混合精度训练，就是在内存中保留一份精确的FP32权重，但在计算密集的前向和反向传播中，聪明地使用FP16加速，并通过损失缩放来避免数值问题。它完美地平衡了**速度、内存和稳定性**
# 框架
### AWQ (Activation-aware Weight Quantization) 

```python
quant_config = { 
    "zero_point": True,      # 1
    "q_group_size": 128,     # 2
    "w_bit": 4,              # 3
    "version": "GEMM"        # 4
}
```
- **因 (Why)**：AWQ的研究者们发现了一个现象 -> **权重的重要性是不平等的**。
    
- **据 (How to judge)**：如何判断哪些权重更重要？-> 他们发现，那些**与大幅度激活值相乘的权重**，对模型的最终输出影响巨大。我们称之为“显著权重”。
    
- **果 (How to do)**：既然找到了这些重要的权重，我们该怎么办？-> **我们必须在量化时重点保护它们**。
    
- **术 (The trick)**：如何保护？-> 通过一个**保护性的缩放操作**，在量化前，先系统性地减小这些显著权重的数值，让它们在后续被量化到有限的16个“坑位”时，产生的误差更小。
    

所以，整个逻辑是：**因为**权重不平等，**所以**要保护重要的，**而**AWQ的核心创新就是找到了一种**通过分析激活值来保护它们**的有效方法。
### AutoGPTQ 框架解析

```python
quantize_config = BaseQuantizeConfig(
    bits=4,                  # 1
    group_size=128,          # 2
    damp_percent=0.01,       # 3
    desc_act=False,          # 4
    sym=True,                # 5
    # ...其他参数...
)
```
**GPTQ 的方法像一个“聪明的修复师”**：

- 他拿起画笔，看着第一个像素点，说：“嗯，这个像素的颜色，最接近我们调色盘里的‘深红色’。”
    
- 在把它涂成深红色之后，他**不会**立刻去看下一个像素。他会退后一步，审视整幅画，心想：“我把这里涂成深红色后，它旁边的那个像素，如果再涂成它最接近的‘天蓝色’，整体看起来就不协调了。为了补偿我刚才的操作，我应该把旁边的像素涂成稍微深一点的‘靛蓝色’，这样整体的观感才最好。”
    
- **核心**：GPTQ在量化一个值时，会**迭代地、联动地调整**其他未量化的值，目的是让**最终的整幅画（层的输出）**和原作的误差最小。它是一种**全局补偿**的思路

**AWQ 的方法像一个“精明的预处理器”**：
- 在动笔之前，他先不看画本身，而是拿出了一张“观众视线热力图”（激活值）。他发现，观众的目光主要都集中在画的中央——蒙娜丽莎的微笑上。
    
- 他想：“既然微笑是重点，那么我在压缩这部分区域时，必须格外小心。而边缘那些背景山水的颜色，错一点也无所谓。”
    
- 于是，他先对整幅画做了一个“预处理”：他用一种技术（缩放），把微笑区域的色彩对比度稍微降低了一点，让这些关键颜色更容易被我们有限的16色调色盘所表示。而对背景，他可能就处理得粗糙一些。
    
- 做完这个预处理后，他再开始逐个像素地进行简单的“找最近颜色”的涂色工作。
    
- **核心**：AWQ的核心工作在“动笔之前”，它通过**保护关键区域（重要权重）**，使得后续简单的量化操作也能得到很好的结果。它是一种**重点保护**的思路。



**sym (对称/非对称)**: **影响精度的微调参数**。

- sym=True (对称): 计算更简单，速度可能略快，但对于分布不均匀的权重，精度损失可能更大。
    
- sym=False (非对称): 需要额外计算和存储zero_point，理论上能更好地适应各种数据分布，通常精度更高。
# weightonly对比 W8A8
这张图是在解释，为什么weightonly量化与batchsize有关
描述的是，weightonly的解量化必须在计算时解，把权重w 从int4变为fp16 在循环内

那为什么不每次在计算前，在循环外，把权重w解为fp16

如果计算前解量化，那此时显卡要同时存在fp16的权重和fp16的激活值，那显存空间此时就跟不量化一样了，量化就没有意义。   这个weghtonly的计算就是，计算的瞬间，抽取需要的数据解量化，其他的保持int4. 

那我感觉int4weightonly完全不如 W8A8     图片左边解释weightonly适用访存密集型，意思是权重很大的时候？ 那模型很大的时候，这种模型用户肯定也多吧，所以也很计算密集型，大厂商都用的int8量化吧？？  我不太理解weightonly的适用场景