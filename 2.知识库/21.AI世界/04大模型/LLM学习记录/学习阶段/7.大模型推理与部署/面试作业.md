# 1.说说你知道的大模型训练or推理的常用优化手段
==面试官角度分析：按大模型训练/推理的瓶颈分类，有层次的回答，而不是一口气把脑袋里的名词全说完==
首先是大模型训练，优化计算有混合精度训练方法，降低矩阵运算的精度，通过损失缩放保证数值的稳定性。优化内存，有数据并行、张量并行、流水线并行的做法
然后是大模型推理， 首先是针对模型压缩的，量化、剪枝、蒸馏
其次是硬件框架上的，kvcache，算子融合，连续批处理技术，pagedattention，flashattention



# 2.一般会对哪些大模型里面的算子做算子融合，说说你知道的
### ==面试官问题分析：他想听到什么？==

==这个问题有两层：==

1. ==**“哪些算子”**：考察你的**知识广度**。你是否知道常见的融合模式？==
    
2. ==**“为什么是这些算子”**：考察你的**深度理解**。你是否明白算子融合的**根本动机**？==
layernorm本身的融合，还有残差连接这个加法，可以和layernorm一起融合位一个算子
**Bias + Activation**，加偏置和激活函数，两个操作本身就是连续的，激活函数也是典型的访存密集型操作。
**注意力机制内部的融合**，以flashattention为代表，把transformer计算中最复杂的qkv矩阵运算融合，减少io




# 3.什么是KV Cache技术，它具体是如何实现的?
kvcache是缓存k矩阵和v矩阵来减小计算开销的，k矩阵是当前时间步为止所有tokend1key向量，v矩阵是所有tokenvalue向量。在大模型推理阶段，WkWv矩阵都已固定。每个词的KV向量也是固定的，所以可以缓存。在prefill阶段读取矩阵后，会将他们的KV矩阵都缓存下来，这样新的词来decoding时，就不用重复得再从头到尾计算kv矩阵了，直接用刚才的缓存。然后新词得到kv向量后，直接追加到原矩阵中就可以了。
不适用kvcache，计算时间复杂度是n方，使用后是n
# 4.Paged Attention的原理是什么，它解决了大模型推理中的什么问题?
==面试官问题分析：他想听到什么？==
==这个问题有三层，你需要层层递进地回答：==
==它解决了什么问题？(The Problem)：不能只说“内存占用过大”，要具体指出是哪种内存问题。==
==它的原理是什么？(The Principle)：清晰地解释“分页”和“映射”的核心思想。==
==它带来了什么好处？(The Benefit)：量化地、具体地说明它带来的收益。==
传统大模型推理，管理kvcache时会为每一个请求预留一块巨大而连续的物理空间，这样一有可能会浪费，二会导致显存存在碎片，显存存在很多不连续的空间，再找不到连续空间服务新请求了。paged attention原理是使用连续逻辑块和离散物理内存块一一对应，来高效利用显存的。 具体来讲，将每个kvcache分块，维护一个逻辑块到物理块的映射表，多个逻辑块可以对应一个物理块。，每次推理需要新空间时，再给分配token块。  解决的是大模型推理中，显存占用过大的问题

# 5.DeepSpeed 推理对算子融合做了哪些优化?
将三次qkv的操作合并为一个算子，与前面的归一化算子融合
自注意力计算融合，flashattention
残差连接，归一化，全连接，激活层融合
偏置加法、残差连接
# 6.FlashAttention的空间复杂度和对HBM的访问次数是多少?
标准的空间复杂度，由于kv矩阵的存在，是n方
访问次数，也是n方
但flashattention，由于只用存储最终结果那个nd的矩阵  所以空间复杂度是n
访问次数，也是n，因为是线性次数的读写


# 7.FlashDecoding在FlashAttention2上做了哪些改进?
FlashAttention2是假设Q, K, V的序列长度是相似的，并采用相对均衡的分块策略。这在Prefill（N x N的注意力）中非常有效。但在Decoding时，Q的序列长度永远是**1**，而K和V的序列长度N却非常长。这是一个1 x N问题。直接用FlashAttention2的Kernel来处理，并行度很低，很多线程组会被浪费
FlashDecoding设计了专门为q_len=1场景优化的CUDA Kernel 它将K和V这两个长序列，切分成非均衡的块。并行地分配给GPU上更多的线程组和流处理器
    
## 8.FlashDecoding++做了什么优化?

# 9.什么是子图融合优化技术，为什么他可以提升推理速度?
子图融合是深度学习编译器或推理引擎自动执行的，关键的计算图优化技术
它的核心思想，是将计算图中多个连续的、可以被合并的节点（算子），融合成一个单一的、由底层高性能库（如cuDNN, cuBLAS）或者手写CUDA Kernel实现的“超级节点”

# 10.MHA，GQA，MQA推理优化技术的区别是什么?
MHA就是原始的多头注意力，N个query头，也配对了N个k和v，每个Q头都有自己的信息，能学到自己的注意力特征
MQA仍然是N个Q头，但是所有头都共享一份k头和v头，这样显存占用小，内存带宽压力小
GQA，是MHA和MQA的折中。它将N个Query头分成G组，每组内的Q头，共享一份K/V头。
# 11.Paged Attention是如何有效管理具有分页的KV缓存的?
### ==面试官问题分析：他想听到什么？==

==这个问题有两个关键词：“如何有效管理”和“具有分页的KV缓存”。==

1. ==**“具有分页的KV缓存”**: 这要求你首先要解释清楚，这个“分页”到底是什么意思，即物理块的概念。==
    
2. ==**“如何有效管理”**: 这要求你详细描述vLLM是如何**追踪、分配、共享和释放**这些分页的物理块的。这就要引出逻辑块和块表的概念。==
首先vllm引擎初始化时，会给kvcache预留显存，然后切分成多个固定大小、较小的单元，称作物理块，当新请求进来时，我们通过prompt的长度，在逻辑上将token序列切分为同样大小的块，这在逻辑层面上的块，叫逻辑块。块表将他们连接起来。一个逻辑块定义一个物理块。
这样的话，就能做到<font color="#f79646">按需分配</font>，也能做到利用起来显存，不连续有没关系，<font color="#f79646">消除内存碎片</font>。还能做到<font color="#f79646">内存共享</font>
# 12.介绍一下动态批处理技术(Dynamic Batching)
静态批处理就是所有请求一起凑齐batchsize后，一起进，所有请求的结果生成出来后，一起出。这样的问题是前端、后端堵塞。动态批处理就是设置一个时间窗口或一个最大批次大小。当时间到了或者批次满了，就立即将已有的请求打包送入模型。解决了前端堵塞
# 13.请问什么是猜测推理技术?请举例说明
# 14.什么是continuous batching技术，为什么他的效率比动态batching效率高?
动态批处理解决了前端堵塞，未解决后端堵塞
它将推理过程看作一个永不停歇的循环。在每一个迭代步（step），调度器都会重新评估整个系统的状态。1. 没有固定的“批次”概念**: 请求不是被凑成一个“命运共同体”然后被执行。相反，请求可以在任何时间点加入或离开正在运行的计算集合。 一个请求一旦完成，它占用的资源会在下一个迭代步立刻被释放，并且这个空位在同一个迭代步就有可能被新请求填充。
# 15.优化CUDA程序的访存效率，你可以想到哪些?
# 16.优化CUDA程序的计算效率，你又可以想到哪些?