
**任务卡 4/7：反向传播 (Backward Propagation)**

**目标**: 编写函数 `backward_propagation(parameters, cache, X, Y, lambd)`。它接收参数、前向传播的缓存、输入和标签，计算并返回所有参数的**梯度**。

**引导性思考与问题（我们将严格按照“追责”的链条来推导）：**

1.  **从哪里开始？**
    反向传播从**最后**开始。我们的起点是**损失 `L`**。我们要计算的第一个梯度，是损失 `L` 对**最后一层的线性输出 `Z2`** 的偏导数 `dZ2 = ∂L / ∂Z2`。
    *   对于交叉熵损失 + Sigmoid输出层，有一个非常优美的结论：`dZ2 = A2 - Y`。这个简单的结果大大简化了我们的计算。（推导过程略，我们可以把它当作一个已知结论来使用）。`dZ2` 的形状将是 `(1, m)`。

2.  **计算第二层（输出层）的梯度 (`dW2`, `db2`)**:
    *   **`dW2 = ∂L / ∂W2`**: 根据我们之前推导的 `∂L/∂w = (∂L/∂z) * (∂z/∂w)`，可以得到向量化的公式：`dW2 = (1/m) * dZ2 @ A1.T`。
        *   思考维度：`(1, m) @ (m, n_h)` -> `(1, n_h)`。这和 `W2` `(1, n_h)` 的形状一致。`.T` 是Numpy中获取矩阵转置的便捷方法。
        *   别忘了加上**正则化项的梯度**！正则化项 `(λ / (2*m)) * Σ(W2^2)` 对 `W2` 求导，结果是 `(λ / m) * W2`。所以最终 `dW2 = (1/m) * dZ2 @ A1.T + (lambd / m) * W2`。
    *   **`db2 = ∂L / ∂b2`**: `db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)`。
        *   `np.sum` 的 `axis=1` 参数表示沿着“行”的方向求和。`keepdims=True` 保证了即使求和后，结果的形状依然是 `(1, 1)` 而不是 `(1,)`，这在后续计算中能避免很多维度问题。

3.  **将“责任”传给第一层**:
    *   我们需要计算 `dA1 = ∂L / ∂A1`，即损失对第一层**激活值**的敏感度。
    *   公式：`dA1 = W2.T @ dZ2`。
        *   思考维度：`(n_h, 1) @ (1, m)` -> `(n_h, m)`。

4.  **计算第一层（隐藏层）的 `dZ1`**:
    *   `dZ1 = ∂L / ∂Z1 = (∂L / ∂A1) * (∂A1 / ∂Z1) = dA1 * g'(Z1)`，其中 `g` 是 `tanh` 函数。
    *   `tanh` 的导数有一个很好的性质：`tanh'(x) = 1 - tanh(x)^2`。因为 `A1 = tanh(Z1)`，所以 `g'(Z1) = 1 - A1^2`。
    *   所以 `dZ1 = dA1 * (1 - np.power(A1, 2))`。`np.power(A1, 2)` 计算 `A1` 的逐元素平方。

5.  **计算第一层（隐藏层）的梯度 (`dW1`, `db1`)**:
    *   **`dW1 = ∂L / ∂W1`**: `dW1 = (1/m) * dZ1 @ X.T + (lambd / m) * W1`。（和 `dW2` 的推导完全类似）。
    *   **`db1 = ∂L / ∂b1`**: `db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)`。

**你的任务**:
根据以上5个步骤，编写 `backward_propagation` 函数。
1.  从 `cache` 和 `parameters` 中取出需要的变量（`A1`, `W2`, `A2`, ...）。
2.  严格按照顺序，依次计算出 `dZ2`, `dW2`, `db2`, `dA1`, `dZ1`, `dW1`, `db1`。
3.  将所有计算出的梯度存入一个名为 `grads` 的字典中，并返回它。

这是最有挑战性，也是最有收获的一步。仔细思考每一步的维度变化，这将是对你理解的终极考验。祝你好运！