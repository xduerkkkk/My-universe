如果已经搭建好了神经网络，接下来就是喂数据，然后根据数据调整权重。
不同的训练目标调整方法是相同的，只是“评分标准”不同
- 回归：均方误差作为损失函数。 因为只是预测数值
- 分类：交叉熵函数。 我们以概率的差衡量
训练过程，仍然是梯度下降。
我们以最小化损失函数做为目标，因为损失函数的计算里面，就蕴含着各个环节的权重。
最小化损失函数怎么做？若把横坐标（也有可能多维）设为权重，纵坐标为损失函数得到的值大小，那我们想做的，就是图中最低点，所对应的权重大小。因为我们损失函数设计的都是光滑，尽管可能有很多“低谷” 但我们总能去到那些地方。

- 对权重求梯度，梯度体现了此时这个权重，在刚才描述的图中的陡峭程度。而我们的目标状态下，权重陡峭程度应该是0
- 沿着陡峭方向的反方向进行更新 ，更新的过程叫梯度下降。梯度怎么来的？反向传播计算出来的。
- batch gradient descent 是看完所有训练数据后，计算总的梯度更新权重，比如我们暂时确定了w1 w2  用w1和w2跑所有数据，每一个数据，都会和正确数据有偏差，我们可以理由这个偏差求梯度？  然后我们把梯度加起来求平均，去决定新的w1，w2。 mini-batch gd是 有batchsize 一批一批地数据后分别更新
我们使用**梯度下降**策略来更新权重，而更新时所需要的那个**梯度**，是通过**反向传播**算法高效计算出来的。