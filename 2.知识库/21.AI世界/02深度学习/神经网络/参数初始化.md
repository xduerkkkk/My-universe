backpropagation
gradient descent
To evaluate the derivatives of the error function w.r.t. weights.
The first stage, namely the propagation of errors backwards through the network in order to evaluate derivatives, can be applied to many other kinds of network and not just the multilayer perceptron.
It can also be applied to error functions other that just the simple sum-of-squares, and to the evaluation of other derivatives such as the Jacobian and Hessian matrices

The second stage of weight adjustment using the calculated derivatives can be tackled using a variety of optimization schemes, many of which are substantially more powerful than simple gradient descent.
we have supplied the corresponding input vector to the network
successive

如果隐藏层神经元初始化完全一样，那么接下来梯度计算也一定一样，因为他们都收到的一样的数据，一样的损失，那这样根本无法分别学到各自把握的特征。就没用了。
然后反向传播，是计算梯度的手段，因为梯度在数学上是一直嵌套的，所以我们需要把每一个位置的梯度都纪录上，这个记录动作，就是反向传播的核心。记录好了，想算任何一个神经元的梯度都容易。  拿着梯度，去更新权重参数的动作，叫梯度下降。
**数值方法**？ 我不知道要干嘛，我的理解就是不记录梯度，每当视角转到一个神经元时，他只能估算前面的神经元的梯度。这样当然效率低。 
只不过可以用来检查。
我真正的疑惑还在这
- PPT引入了一个关键符号 δ_j = ∂E / ∂a_j （损失对**激活前**线性输出的偏导，注意：PPT这里用a代表激活前的z），并推导出了δ的**反向传播公式** δ_j = h'(a_j) * Σ(w_kj * δ_k)。
    
- 这个公式的含义是：**一个神经元的误差 δ_j，等于它后面一层所有神经元误差δ_k的加权和，再乘以它自己的激活函数的导数。** 这完美体现了“误差逐层向后传播”的思想。
    
- 一旦算出了所有层的 δ，计算最终的权重梯度就非常简单了：∂E / ∂w_ji = δ_j * z_i (误差乘以输入)。
感觉只凭直觉理解了，反正感觉没掌握