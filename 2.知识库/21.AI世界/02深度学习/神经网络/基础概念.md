机器学习模型的任务，尤其是像神经网络这样的模型，其本质目标之一就是**自动地去发现和学习这些嵌入在高维空间中的低维流形结构**。它不需要手动旋转坐标轴，而是通过学习权重，自动地对数据进行变换，使得数据在新空间里变得更容易被分开。这呼应了我们之前讨论的，神经网络是一种“自动的特征工程”


x1 XOR x2 等价于 (x1 OR x2) AND (NOT(x1 AND x2))
**x1和x2中，有一个是1，但不能两个都是**


1. **高维空间是“空旷”的**: 数据点在高维空间中会变得非常稀疏。就像把100个点撒在一个房间里很密集，但把它们撒在整个地球上就非常稀疏了。
    
2. **“距离”的概念变得奇怪**: 在高维空间中，所有数据点到中心点的距离都差不多，而且任意两个数据点之间的距离也可能都差不多。这使得基于“距离”的算法（比如K-近邻）效果变差。
    
3. **我们的直觉不可靠**: 这正是这几页PPT的核心。它在提醒我们，当处理高维数据（如图像、文本）时，不要想当然地用二维或三维的几何图形去类比，因为高维空间的性质是反直觉的。


就是线性不可分问题 数学上仍是可以分解成n多个线性可分问题。 当然，这n个问题还要恰当的组合 不能随笔分 n适当的话 效果会很好，也许还能完美拟合 神经网络就是不断调整组合过程