
# 从线性模型到多层感知器（MLP）的演进、

首先默认阅读者明白“线性模型”这一知识点
## 故事的起点
线性回归，做简单数据的二分类 也许很同意 只需要一条直线。 wx+b
做复杂的 如圆心是原点，半径一个大一个小的同心圆数据 我们就需要线性模型拟合出一条介于两个圆之间的似圆曲线。
那我们不能简单的wx+b 我们也许要引入x平方 x三次方 x1乘x2等等
到底引入多少特征？ 不知道，还得自己调。
所以痛点就是特征工程。
## 第一次进化
我们的目的，是看着手上的数据中一个点，判断到底是这俩个同心圆的哪一个
而判断过程，仍然是用线来分， 然后看这个点在这个线的哪一侧
直觉上：
看着这个数据
我们仍然画直线，笨拙地试图划分。 
嗯...可能我们在整个图片右侧画了个线，稍微分类了右侧的同心圆。
我们再加！
左侧、右侧、上侧、上测偏下。。。
我们同时画很多条直线   虽然不能拟合曲线，但也勉强看起来分类了。
接下来，保留这些直线（保留上一层输出的特征)，我们基于这些直线再次试图拟合， 但此时，我们会把直线变“曲”。
直觉来看，理解为每次试图拟合都稍稍复杂了一次。
就这样不断拟合 不断变复杂一点点
原先那个偏左侧的 已经变成一个能差不多拟合的圆形了
右侧那个也是
但他们总有不同
最后我们综合一下，得到最终的拟合曲线


数理逻辑上：
首先定义每次我们到底想画多少条直线“笨拙分类”。
假如4，隐藏层大小是4.
输入层只跟数据有关。
假如二维平面同心圆数据共30个点
送进去的数据向量是X，维度30，2  
接下来进入隐藏层，隐藏层是执行“画线”的地方。
最终画4条线
，但我们把视角放到其中一条，比如隐藏层的第一个神经元
他会把30个数据看完， 他也能感受到自己最开始分类的效果，然后“反向传播”使得他调整他定的w和b。  这就是他第一次画的笨拙的线。   w，蕴含着他对维度一和维度二的重视程度。 可能一样可能偏心...等等
ok 我们有4个神经元，都是这样调整的。

## 第二次进化
假如说 第一层都已经调整过了w和b。
下面有我们把视角从神经元转到数据本身。假如这个数据就是第21个数据。
他会被4个神经元都计算一遍。  嗯得出的结果是完全不同的
得出的值我们叫激活值，a1，a2，a3，a4.
嗯...
然后再经过第二层！ 第二层和第一层是一样的神经元，我们假设也是4个  唯一区别就是，他们拿到的数据，是经过第一层考量后的数据，第二层会拿第一层考量的数据分...
嗯仍然会考量出一个w和b  仍然是4条线。第二层得到的结果，其实已经可以理解为，对“特征”的组合了。 比如a1是w重视一维的数据，  第二层，a11，是w重视二维的数据， 这俩就组合了。
可能a2是重视一维，a22也是重视一维，这个神经元就完全重视一维数据...
反正这就是组合特征的体现

但单纯这样组合，其实最后的结果还是直线 没有直觉里讲的变复杂的曲线 
所以要引入激活函数了。 
.激活函数 把wx+b的激活值，压缩到了0-1的数字。 是非线性的。
视角来到第二层神经元
第一层神经元输出的a1 a2 a3 a4 变成了0-1的数字，
第二层神经元接受的数据是这么些样子。 
曲线一定要画到图上，我们图像的横坐标，是最开始的输入数据x！
 嗯 第二层接受第一层的f(a1) f(a2) ...   （我们设f就是激活函数） 输出wf(a)+b 是直线 但横坐标啊f(a)
 第一层接受数据输出a1 a2... 即wx+b 也是直线 但横坐标是x
 现在我们横坐标是x 纵坐标是wf(a)+b  不是简单的粘合俩个直线模型 而引入了非线性。所以最后的图像是曲线！

ok 每一个神经元 都在努力拟合
可能经过很多层...  每个神经元都有自己的见解。
比如最后一层隐藏层的第一个神经元，其输出的方程别看是wx+b 但这个x包含了许多sigmoid非线性函数 让整个x蛮复杂的
最后以及可以很好拟合了。

最后一层隐藏层后，就是输出层，他可以综合最后一层所有隐藏层的神经元， 听取他们意见
进行加权求和，得到最终的拟合方程



我们继续讲上次的a1
1. **第一次进化**: 当我们把多个神经元“并排”组成一个隐藏层时，从“画线”的角度来看，这一层在做什么？
    
2. **第二次进化**: 当我们把隐藏层的输出，喂给输出层时，输出层又在做什么？它是如何在隐藏层画出的那些“直线”的基础上，创造出“曲线”的？





**让我们回到单个神经元的计算公式：** a = sigmoid(z)，其中 z = w*x + b。

- **决策边界**: 一个神经元真正用来“划分”空间的，是它的**决策边界**。在决策边界上，神经元处于“纠结”的状态，不知道该判断为0还是1。对于Sigmoid函数来说，这个边界就是 z=0 的地方。
    
- **边界的形状**: z = w*x + b = 0 这个方程，在 (x1, x2) 空间里，画出来是一条什么形状的线？**它本身就是一条不折不扣的直线！**
    

**那么，非线性的激活函数（Sigmoid）到底起了什么作用？**

它的作用**不是**把这条直线边界“掰弯”成曲线。

它的作用是，定义了**远离**这条直线边界时，输出信号 a 的**变化方式**。它提供了一个从0到1的**平滑、柔和的过渡区域**。所以我们之前称之为“软”直线。但无论多“软”，它用来划分空间的“分界线”依然是**直的**。

**所以，正确的逻辑顺序是：**

1. 隐藏层的**每一个**神经元，都在原始的 (x1, x2) 空间里，独立地、各自为政地画出了自己的一条**线性**分界线。
    
2. 它们各自输出了一个“软”信号 a1, a2, a3, ...。
    
3. **输出层**接收这些信号，然后通过对这些 a 进行加权求和 (v1*a1 + v2*a2 + ...)，才最终“拼接”出了一个**非线性**的决策边界。
    

**换句话说，非线性不是在单个神经元内部创造的，而是在层与层之间的“组合”中涌现出来的。**
 







隐藏层是用来不断思考数据，从数据中提取当前特征。 一层接一层，接力思考，每一层提取的特征就都不一样，但又都基于前面，这就是多层的意义  
激活函数是引入非线性项，以便能拟合复杂曲线。




1. **隐藏层**的每个神经元都会对原始数据进行一次简单的**线性**切分，从而提取出一组基础特征。
    
2. **激活函数**的作用是在层与层之间进行一次**非线性**变换，这打破了线性叠加的局限，使得网络能够学习“曲线”的组合，而不是“直线”的组合。
    
3. **多层结构**让网络能够逐层地将前一层提取出的简单特征，组合成更复杂、更抽象的特征，最终拟合出任意复杂的非线性决策边界





**Multilayer Perceptron**

a = sigmoid(w_h * x + b_h)


- 神经网络的思想和传统的map_feature（这里称为fixed basis functions）做了对比。
    
    - 传统方法：φ(x) (基函数，比如x^2) 是**固定的、人定的**，模型只学习如何对这些固定的特征进行线性组合（学习权重 w）。
        
    - 神经网络：基函数 φ(x) 本身也变成了**可学习的**！我们隐藏层的每个神经元，a = sigmoid(w_h * x + b_h)，就是一个可学习的基函数。我们不仅要学习输出层如何组合它们，还要学习这些基函数本身应该是什么样子（通过学习隐藏层的权重 w_h, b_h）。
        

**2. 引入标准化的数学符号 (**