
**第一站：神经网络的基石 (Foundations of Neural Networks)**

- **1. 多层感知器 (MLP)**:
    
    - **定位**: 深度学习的“创世模型”。
        
    - **内容**: 我们已经详细探讨并实践过的所有内容（从线性到非线性、前向/反向传播、正则化等）。这应该是你这个星域的第一篇基石文档。
        

**第二站：处理特定结构数据的两大经典范式 (Classic Paradigms)**

- **2. 卷积神经网络 (CNN): 洞悉空间**
    
    - **动机**: MLP处理图像时的“位置敏感”缺陷与参数爆炸问题。
        
    - **核心思想**: 局部感受野、**权重共享**、平移不变性。
        
    - **核心组件**: 卷积层、池化层、全连接层。
        
    - **经典架构演进**: LeNet -> AlexNet -> VGG -> GoogLeNet (Inception) -> ResNet (残差连接)。理解架构的演进如何解决了“深度网络难以训练”的问题。
        
- **3. 循环神经网络 (RNN): 拥抱序列**
    
    - **动机**: MLP/CNN处理序列数据时无法“记忆”历史信息。
        
    - **核心思想**: 循环的隐藏状态。
        
    - **核心问题**: 长期依赖问题与**梯度消失/爆炸**。
        
    - **解决方案 (现代RNN的基石)**: **LSTM** 和 **GRU**，理解其内部精巧的“门控机制”（遗忘门、输入门、输出门）。
        
    - **架构**: Encoder-Decoder (Seq2Seq) 模型及其在机器翻译等领域的应用。
        

**第三站：注意力革命与Transformer的崛起 (The Attention Revolution)**

- **4. 注意力机制 (Attention Mechanism)**
    
    - **动机**: Seq2Seq模型的“信息瓶颈”问题。
        
    - **核心思想**: 打破固定长度编码的束缚，允许模型在生成输出时，“动态地关注”输入的不同部分。
        
    - **学习重点**: 从第一性原理理解Attention分数的计算过程（Query, Key, Value）。
        
- **5. Transformer: 注意力即全部**
    
    - **定位**: 现代大模型（无论是NLP的GPT，还是CV的ViT）的**共同基础架构**。
        
    - **核心组件**:
        
        - **自注意力 (Self-Attention)**: 序列内部的相互“审视”。
            
        - **多头注意力 (Multi-Head Attention)**: 从不同“子空间”进行审视。
            
        - **位置编码 (Positional Encoding)**: 重建被抛弃的顺序信息。
            
        - **残差连接 & 层归一化**: 训练深度Transformer的“两大护法”。
            

**第四站：生成模型与其他前沿 (Generative Models & Frontiers)**

- **6. 生成对抗网络 (GAN)**: 两名玩家（生成器与判别器）的博弈游戏，在图像生成领域影响深远。
    
- **7. 变分自编码器 (VAE)**: 另一种强大的生成模型，结合了概率图模型的思想。
    
- **8. 图神经网络 (GNN)**: 将深度学习的能力扩展到非欧几里得的图结构数据。