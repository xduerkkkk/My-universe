---
share_link: https://share.note.sx/ovdp02x0#dOpBo7aptHd0UECWLe3wm3H/E3TKyUBfsQt5nAvHORE
share_updated: 2025-08-20T17:03:57+08:00
---
### ReLU函数

  

最受欢迎的激活函数是*修正线性单元*（Rectified linear unit，*ReLU*），

因为它实现简单，同时在各种预测任务中表现良好。


给定元素$x$，ReLU函数被定义为该元素与$0$的最大值：

  

(**$$\operatorname{ReLU}(x) = \max(x, 0).$$**)

  




### sigmoid函数

  

**对于一个定义域在$\mathbb{R}$中的输入，

*sigmoid函数*将输入变换为区间(0, 1)上的输出**]。

因此，sigmoid通常称为*挤压函数*（squashing function）：

它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：

  

(**$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$**)

  

在最早的神经网络中，科学家们感兴趣的是对“激发”或“不激发”的生物神经元进行建模。

因此，这一领域的先驱可以一直追溯到人工神经元的发明者麦卡洛克和皮茨，他们专注于阈值单元。

阈值单元在其输入低于某个阈值时取值0，当输入超过阈值时取值1。

  

当人们逐渐关注到到基于梯度的学习时，

sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。

当我们想要将输出视作二元分类问题的概率时，

sigmoid仍然被广泛用作输出单元上的激活函数

（sigmoid可以视为softmax的特例）。

然而，sigmoid在隐藏层中已经较少使用，

它在大部分时候被更简单、更容易训练的ReLU所取代。

在后面关于循环神经网络的章节中，我们将描述利用sigmoid单元来控制时序信息流的架构。

![[激活函数-1741792682856.jpeg]]

# SiLu函数
SiLU（Sigmoid Linear Unit）是一种激活函数，常用于神经网络中，尤其是在深度学习模型里。它结合了线性函数和Sigmoid函数的特点，公式一般表示为：

SiLU(x) = x * sigmoid(x) = x / (1 + e^(-x))

SiLU的优点包括：

1. **平滑且非单调**：相比ReLU，SiLU在负区间不是简单截断，而是平滑过渡，能帮助模型更好地捕捉复杂特征。
2. **自门控特性**：SiLU的输出值部分依赖于Sigmoid，具有一定的自调节能力，有助于梯度流动。
3. **性能提升**：在某些任务和模型中，使用SiLU激活函数能提升模型的表现。