### 均方损失函数

  
即 MSE Loss
在我们开始考虑如何用模型*拟合*数据之前，我们需要确定一个拟合程度的度量。

*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。

通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。

回归问题中最常用的损失函数是平方误差函数。

当样本$i$的预测值为$\hat{y}^{(i)}$，其相应的真实标签为$y^{(i)}$时，

平方误差可以定义为以下公式：

  

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$


  

常数$\frac{1}{2}$不会带来本质的差别，但这样在形式上稍微简单一些

（因为当我们对损失函数求导后常数系数为1）。


## 交叉熵损失函数
好的！以下是关于交叉熵损失函数的详细讲解，包含未渲染的 LaTeX 公式代码，方便你直接复制和使用。

---

### **1. 交叉熵损失函数的数学原理**

#### **1.1 信息量（Information Content）**
在信息论中，信息量 \( I (x) \) 是衡量一个事件发生时所携带的信息量。对于一个随机事件 \( x \)，其发生的概率为 \( P (x) \)，则该事件的信息量定义为：
```latex
I(x) = -\log P(x)
```

- 如果一个事件发生的概率很高（接近 1），那么它携带的信息量就很少（接近 0）。
- 如果一个事件发生的概率很低（接近 0），那么它携带的信息量就很大。

#### **1.2 熵（Entropy）**
熵是衡量一个随机变量不确定性的度量。对于一个离散随机变量 \( X \)，其概率分布为 \( P (X) \)，熵定义为：
```latex
H(X) = -\sum_{i} P(x_i) \log P(x_i)
```

- 熵越大，表示随机变量的不确定性越高。
- 熵越小，表示不确定性越低。

#### **1.3 交叉熵（Cross-Entropy）**
交叉熵是衡量两个概率分布之间的差异。假设有两个概率分布 \( P \) 和 \( Q \)，它们定义在同一个随机变量 \( X \) 上，交叉熵定义为：

$$H(P, Q) = -\sum_{i} P(x_i) \log Q(x_i)$$


- \( P \) 是真实分布。
- \( Q \) 是模型预测的分布。
- 交叉熵越小，表示两个分布越接近。

---

### **2. 交叉熵损失函数的公式**
在分类问题中，交叉熵损失函数用于衡量模型的预测概率分布与真实标签的概率分布之间的差异。

#### **2.1 单个样本的交叉熵损失**
对于单个样本，假设：
- \( y \) 是真实标签的索引。
- \( $$\hat{y} $$\) 是模型的预测概率分布

交叉熵损失函数定义为：

$$\text{Loss}(y, \hat{y}) = -\log \hat{y}_y$$


#### **2.2 多分类问题中的交叉熵损失**
在多分类问题中，每个样本的真实标签 \( y \) 是一个整数索引，表示正确类别。模型的预测输出 \( \hat{y} \) 是一个概率分布（通常通过 Softmax 函数得到）。交叉熵损失函数可以写为：

$$\text{Loss}(y, \hat{y}) = -\sum_{i} \mathbb{1}(y = i) \log \hat{y}$$




由于每个样本只有一个真实类别，因此上式可以简化为：

$$\text{Loss}(y, \hat{y}) = -\log \hat{y}_y$$


---

### **3. 交叉熵损失函数的实现**
在 PyTorch 中，交叉熵损失函数可以通过 `torch.nn.CrossEntropyLoss` 或手动实现。

#### **3.1 手动实现**
```python
def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)), y])
```

- **`y_hat`**：模型的预测概率分布，形状为 `(batch_size, num_classes)`。
- **`y`**：真实标签的索引，形状为 `(batch_size,)`。
- **`y_hat[range(len(y_hat)), y]`**：选择每个样本的预测概率中与真实标签对应的元素。
- **`-torch.log(...)`**：计算交叉熵损失。

#### **3.2 使用 PyTorch 的 `torch.nn.CrossEntropyLoss`**
PyTorch 提供了一个内置的交叉熵损失函数 `torch.nn.CrossEntropyLoss`，它结合了 Softmax 和交叉熵损失的计算。

```python
import torch
import torch.nn as nn

# 模型的原始输出（未经 Softmax）
logits = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
# 真实标签
y = torch.tensor([2, 1])

# 创建交叉熵损失函数
criterion = nn.CrossEntropyLoss()

# 计算损失
loss = criterion(logits, y)
print("Loss:", loss.item())
```

#### **3.3 手动实现与 PyTorch 的对比**
假设我们手动计算 Softmax 和交叉熵损失：
```python
import torch

# 模型的原始输出
logits = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
# 真实标签
y = torch.tensor([2, 1])

# 手动实现 Softmax
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition

# 手动实现交叉熵损失
def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)), y])

# 计算 Softmax
y_hat = softmax(logits)

# 计算交叉熵损失
loss = cross_entropy(y_hat, y)
print("Manual Loss:", loss)
```

---

### **4. 交叉熵损失函数的性质**
1. **非负性**：交叉熵损失总是非负的，因为对数函数的输入是概率值（在 0 到 1 之间），取负对数后结果为正。
 好**：交叉熵损失越小，表示模型的预测分布与真实分布越接近。
2. **对数惩罚**：对数函数对低概率值的惩罚很大，因此模型会尽量提高正确类别的预测概率。

---

### **5. 为什么使用交叉熵损失函数？**
1. **数学性质良好**：交叉熵损失函数具有良好的数学性质，便于优化。
2. **概率解释**：交叉熵损失函数基于概率分布，符合分类问题的直观理解。
3. **避免梯度消失**：对数函数可以避免梯度消失问题，因为对数函数的导数在概率值较小时会变得较大。

---

### **6. 总结**
交叉熵损失函数是分类问题中常用的损失函数，用于衡量模型的预测概率分布与真实标签的概率分布之间的差异。它的数学基础来源于信息论，具有良好的数学性质和直观的概率解释。在实际应用中，交叉熵损失函数可以通过手动实现或使用 PyTorch 提供的 `torch.nn.CrossEntropyLoss` 来计算。

希望这些内容能帮助你更好地理解交叉熵损失函数！如果有更多问题，欢迎继续提问。
对数均方根误差（Logarithmic Root Mean Squared Error, Log RMSE）是一种用于评估回归模型预测性能的误差度量。它通过计算预测值和真实值的对数之间的均方根误差来衡量模型的预测误差。这种误差度量特别适用于目标值范围跨度较大的回归任务，如房价预测、收入预测等。

# Log RMSE 的定义
Log RMSE 的计算公式如下：![[损失函数-1742127566696.jpeg]]

### Log RMSE 的特点
1. **对数变换**：通过取对数，可以将目标值的范围缩小，从而减少大值和小值之间的差异。这有助于稳定误差，减少异常值的影响。
2. **均方根误差**：计算对数预测值和对数真实值之间的均方根误差，可以衡量模型的预测误差。
3. **适用于范围跨度大的目标值**：特别适用于目标值范围跨度较大的回归任务，如房价预测、收入预测等。

### Log RMSE 的适用场景
- **目标值范围跨度大**：当目标值的范围跨度较大时，Log RMSE 可以更好地衡量模型的预测误差。
- **减少异常值影响**：对数变换可以减少异常值的影响，使模型更加稳健。

### Log RMSE 的局限性
- **对负值和零的处理**：由于对数函数对负值和零是未定义的，因此在使用 Log RMSE 时需要确保预测值和真实值都大于零。可以通过对预测值进行裁剪来避免这个问题。
- **解释性**：与均方根误差（RMSE）相比，Log RMSE 的解释性可能较差，因为它涉及对数变换。

### 总结
对数均方根误差（Log RMSE）是一种适用于目标值范围跨度较大的回归任务的误差度量。通过对数变换，可以稳定误差，减少异常值的影响。在 PyTorch 中，可以通过自定义函数来计算 Log RMSE，如上文所示。这种误差度量特别适用于房价预测、收入预测等场景。

# KL散度