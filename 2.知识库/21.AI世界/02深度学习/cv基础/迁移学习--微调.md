1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即_源模型_。
    
2. 创建一个新的神经网络模型，即_目标模型_。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。
    
3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
    
4. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。
# 迁移学习
想象你是一名学生，之前学过数学和物理，现在要学一门新课《机器人控制》。老师不会让你从头开始学，而是会利用你已有的数学知识（比如微积分）来辅助理解机器人的运动学。 **迁移学习**就是这个逻辑
利用在大型数据集（如ImageNet）上训练好的模型，将其学到的通用知识（如边缘、纹理特征）迁移到新任务（如识别医疗影像中的肿瘤）

# 微调
微调是迁移学习的一种具体技术，可以理解为“二次加工”：

- **步骤拆解**：
1. **预训练模型**：从大厂（比如谷歌）拿一个现成的模型（如ResNet），它已能识别猫狗、汽车等常见物体 

2. **替换输出层**：原模型的输出层是1000类（ImageNet），而你的任务可能只有2类（比如“瘤和“正常”）。删掉原输出层，新建一个随机初始化的输出层 
。
3. **选择性训练**：
 -  **参数微调**：让模型在新数据上“复习旧知识+学习新知识”，即同时调整原有层和新输出层的参数
- **特征提取**：冻结原有层（仅用它们提取特征），只训练新输出层（适合数据极少的情况） 


**为什么有效？**

- 底层特征（如边缘、颜色）通用性强，高层特征（如物体部件）可针对性调整 
- 相当于用大模型的“基本功”帮你解决小任务，省时省力