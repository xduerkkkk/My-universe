
# 全连接层 （Fully Connected Layer）
是神经网络中的一种经典结构，它的作用是将所有输入特征综合起来，进行全局分析和分类决策。我们可以通过一个生活中的比喻和具体例子来理解它的作用。

### **1. 核心作用**

#### **类比解释**

假设你要识别一只猫，之前的卷积层和池化层已经帮你完成了以下工作：

- **卷积层**：像“显微镜”一样，提取了局部特征（如猫耳、胡须、毛发纹理）。
- **池化层**：像“压缩工具”，保留了显著特征并降低数据量（例如知道“这里有圆形物体”但忽略具体位置）。

**全连接层的作用**：像一个“侦探”，把所有这些线索（特征）综合起来，判断这些特征组合起来是否符合“猫”的定义。

## 卷积核运算
```python
def corr2d(X, K):  #@save

    """计算二维互相关运算"""

    h, w = K.shape

    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))

    for i in range(Y.shape[0]):

        for j in range(Y.shape[1]):

            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()

    return Y
```

![[卷积神经网络-1742825949231.jpeg]]

## 训练卷积核
```python
# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核

conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

  

# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），

# 其中批量大小和通道数都为1

X = X.reshape((1, 1, 6, 8))

Y = Y.reshape((1, 1, 6, 7))

lr = 3e-2  # 学习率

  

for i in range(10):

    Y_hat = conv2d(X)

    l = (Y_hat - Y) ** 2

    conv2d.zero_grad()

    l.sum().backward()

    # 迭代卷积核

    conv2d.weight.data[:] -= lr * conv2d.weight.grad

    if (i + 1) % 2 == 0:

        print(f'epoch {i+1}, loss {l.sum():.3f}')
```

## 1x1 卷积
### 代码
```python
def corr2d_multi_in_out_1x1(X, K):

    c_i, h, w = X.shape

    c_o = K.shape[0]

    X = X.reshape((c_i, h * w))

    K = K.reshape((c_o, c_i))

    # 全连接层中的矩阵乘法

    Y = torch.matmul(K, X)

    return Y.reshape((c_o, h, w))

```
### 功能



#### **1. \(1 \times 1\) 卷积的功能**

#### **1.1 通道变换**
\(1 \times 1\) 卷积的核心功能是**通道变换**。它可以将输入数据从 \(C_{\text{in}}\) 个通道转换为 \(C_{\text{out}}\) 个通道，同时保持空间维度（高度和宽度）不变。

#### **1.2 特征融合**
通过 \(1 \times 1\) 卷积，可以对输入特征图的不同通道进行加权求和，从而实现特征的融合。这在处理多通道输入（如 RGB 图像或深层网络中的特征图）时非常有用。

#### **1.3 参数减少**
\(1 \times 1\) 卷积可以减少模型的参数数量，尤其是在处理高维特征图时。它通过线性组合的方式，将多个通道的信息压缩到更少的通道中。

---

#### **2. 应用例子：改变通道数**

假设我们有一个输入特征图 \(X\)，其形状为 \((C_{\text{in}}, H, W)\)，我们希望将其通道数从 \(C_{\text{in}}\) 改变为 \(C_{\text{out}}\)，同时保持空间维度 \(H\) 和 \(W\) 不变。这可以通过 \(1 \times 1\) 卷积实现。

#### **例子：将输入通道数从 3 改为 6**

假设输入特征图 \(X\) 的形状为 \((3, 5, 5)\)，表示有 3 个输入通道，高度和宽度均为 5。我们希望将其通道数改为 6。

```python
import torch

# 输入特征图 X，形状为 (C_in, H, W)
X = torch.randn(3, 5, 5)  # 3 个输入通道，高度和宽度均为 5

# 卷积核 K，形状为 (C_out, C_in, 1, 1)
K = torch.randn(6, 3, 1, 1)  # 6 个输出通道，每个输出通道对应 3 个输入通道，卷积核大小为 1x1

# 使用 1x1 卷积
output = torch.nn.functional.conv2d(X.unsqueeze(0), K).squeeze(0)  # 添加批量维度，执行卷积后移除批量维度
print(output.shape)  # 输出形状为 (6, 5, 5)
```

#### **解释**
1. **输入特征图 \(X\)**：形状为 \((3, 5, 5)\)，表示 3 个输入通道，高度和宽度均为 5。
2. **卷积核 \(K\)**：形状为 \((6, 3, 1, 1)\)，表示 6 个输出通道，每个输出通道对应 3 个输入通道，卷积核大小为 \(1 \times 1\)。
3. **卷积操作**：
   - 使用 `torch.nn.functional.conv2d` 执行卷积操作。
   - 输入 \(X\) 的形状为 \((1, 3, 5, 5)\)（添加了批量维度）。
   - 卷积核 \(K\) 的形状为 \((6, 3, 1, 1)\)。
   - 输出的形状为 \((1, 6, 5, 5)\)，表示 6 个输出通道，高度和宽度保持为 5。
   - 使用 `.squeeze(0)` 移除批量维度，最终输出形状为 \((6, 5, 5)\)。

---

#### **3. 应用例子：特征融合**

假设我们有一个输入特征图 \(X\)，其形状为 \((C_{\text{in}}, H, W)\)，我们希望对不同通道的特征进行融合，以提取更有用的特征。

#### **例子：融合 8 个通道的特征**

假设输入特征图 \(X\) 的形状为 \((8, 10, 10)\)，表示有 8 个输入通道，高度和宽度均为 10。我们希望将其通道数减少到 4，同时融合不同通道的特征。

```python
import torch

# 输入特征图 X，形状为 (C_in, H, W)
X = torch.randn(8, 10, 10)  # 8 个输入通道，高度和宽度均为 10

# 卷积核 K，形状为 (C_out, C_in, 1, 1)
K = torch.randn(4, 8, 1, 1)  # 4 个输出通道，每个输出通道对应 8 个输入通道，卷积核大小为 1x1

# 使用 1x1 卷积
output = torch.nn.functional.conv2d(X.unsqueeze(0), K).squeeze(0)  # 添加批量维度，执行卷积后移除批量维度
print(output.shape)  # 输出形状为 (4, 10, 10)
```

#### **解释**
1. **输入特征图 \(X\)**：形状为 \((8, 10, 10)\)，表示 8 个输入通道，高度和宽度均为 10。
2. **卷积核 \(K\)**：形状为 \((4, 8, 1, 1)\)，表示 4 个输出通道，每个输出通道对应 8 个输入通道，卷积核大小为 \(1 \times 1\)。
3. **卷积操作**：
   - 使用 `torch.nn.functional.conv2d` 执行卷积操作。
   - 输入 \(X\) 的形状为 \((1, 8, 10, 10)\)（添加了批量维度）。
   - 卷积核 \(K\) 的形状为 \((4, 8, 1, 1)\)。
   - 输出的形状为 \((1, 4, 10, 10)\)，表示 4 个输出通道，高度和宽度保持为 10。
   - 使用 `.squeeze(0)` 移除批量维度，最终输出形状为 \((4, 10, 10)\)。

---



# **输出通道数的核心概念**

#### **类比解释**

想象你有一张黑白照片（**1个输入通道**），现在你需要用不同的“放大镜”（卷积核）去观察这张照片的不同特征：

- **放大镜1**：专门观察**垂直边缘**（如树干、门框）。
- **放大镜2**：专门观察**水平边缘**（如地平线、桌面）。
- **放大镜3**：专门观察**圆形区域**（如纽扣、车轮）。
- ……其他放大镜各司其职。

**每个放大镜（卷积核）会生成一张新的“特征图”**，记录它关注的模式。如果有6个放大镜，就会生成6张特征图，即**输出通道数=6**。