
七大题
简答、论述、设计（内容和之前类似）、前沿探索（也与之前类似）
答题时，建议用“**英文术语 + 中文解释**”的方式，既显得专业又能看懂题目。

*   **Inputs (输入层, $x$):**
    *   **作用：** 接收来自外部的数据或者上一层神经元的输出。
*   **Weights (权重, $w$):**
    *   **作用：** 调节每个输入信号的重要性（强度）。
    *   **原理：** 类似于线性方程 $y=ax+b$ 中的斜率 $a$。权重越大，该输入对结果的影响越大。它是网络通过训练主要学习的参数。
*   **Bias (偏置, $b$):**
    *   **作用：** 调整激活的阈值。
    *   **原理：** 类似于线性方程中的截距 $b$。它允许激活函数左右平移，即使输入为 0，神经元也能被激活（或保持不激活）。
*   **Weighted Sum / Linear Combination (线性加权求和, $z = \sum w_i x_i + b$):**
    *   **作用：** 将所有输入信号进行汇总。
*   **Activation Function (激活函数, $\sigma(\cdot)$):**
    *   **作用：** **引入非线性因素 (Non-linearity)**。这是最重要的部分！
    *   **原理：** 如果没有它，无论网络有多少层，最终都只是一个线性模型，无法解决复杂问题（如异或问题 XOR）。它决定了神经元是否“被点燃”（fire）。

---

### 二、 常见的激活函数 (Common Activation Functions)

这部分通常要求列举 3-4 个，并简述特点。

#### 1. Sigmoid
*   **公式：** $f(x) = \frac{1}{1+e^{-x}}$
*   **特点：** 将输出压缩到 **(0, 1)** 之间。
*   **缺点：** 容易导致**梯度消失 (Gradient Vanishing)**；输出不是以 0 为中心的。
*   **适用：** 二分类问题的输出层（表示概率）。

#### 2. Tanh (双曲正切)
*   **公式：** (形状类似 Sigmoid，但是拉长了)
*   **特点：** 将输出压缩到 **(-1, 1)** 之间。
*   **优点：** 输出是以 0 为中心的 (Zero-centered)，通常比 Sigmoid 收敛快。
*   **缺点：** 依然存在梯度消失问题。

#### 3. ReLU (Rectified Linear Unit, 线性整流单元) —— **必写，最常用！**
*   **公式：** $f(x) = \max(0, x)$
*   **特点：** $x>0$ 时直接输出 $x$，$x\le0$ 时输出 0。
*   **优点：** 计算非常简单（速度快）；有效缓解了**梯度消失**问题。
*   **缺点：** Dead ReLU 问题（某些神经元可能永远不会被激活）。

#### 4. Softmax
*   **作用：** 通常用于**多分类问题的输出层**。
*   **特点：** 将一组输出数值转换为**概率分布**，所有概率之和为 1。

---

### 三、 满分论述模板 (可以直接背诵思路)

**Question:** *Explain the structure of a Perceptron and the role of Activation Functions.*

**Answer Structure (中文作答思路):**

1.  **定义 Perceptron：** 感知机是神经网络的基本构建单元，它模拟了生物神经元的工作方式。
2.  **描述结构：** 它由输入 ($x$)、权重 ($w$)、偏置 ($b$)、加权求和 ($\sum$) 和激活函数 ($f$) 组成。（此处可画图）。
3.  **解释线性部分：** 权重代表输入的强度，偏置代表阈值的调整。它们组合形成线性变换 $z = Wx + b$。
4.  **解释非线性部分（核心）：** 激活函数的作用是引入**非线性 (Non-linearity)**。
    *   *Why?* 如果没有激活函数，多层神经网络退化为单层线性网络，无法拟合复杂的非线性数据。
5.  **举例：**
    *   **Sigmoid:** 适用于二分类输出，但有梯度消失问题。
    *   **ReLU:** 现代深度学习最常用的函数，计算快且解决了梯度消失，常用于隐藏层。
    *   **Softmax:** 专门用于多分类的输出层，将输出转化为概率。

# 神经网络
神经网络的结构   画下来、讲清楚 其中部分的作用、原理、名字
常见的激活函数  


perceptron 神经元，是神经网络的基本单位
- inputs:接收来自外部的数据或上一层神经元的数据
- weights：
	- 作用：调节每个输入信号的重要性
	- 原理：输入的权重越大，该输入对结果的影响越大。它是网络通过训练主要学习的参数。
- bias：
	- 作用：调整激活的阈值
	- 原理：类似线性方程中的结局b，它允许激活函数左右平移，即使输入为0，神经元也能被激活（或保持不激活）
- Weighted Sum,线性加权求和, $z = \sum w_i x_i + b$ 
	- **作用：** 将所有输入信号进行汇总。
 - **Activation Function (激活函数, $\sigma(\cdot)$):**
    *   **作用：** **引入非线性因素**。
    *   **原理：** 如果没有它，无论网络有多少层，最终都只是一个线性模型，无法解决复杂问题（如异或问题 XOR）。


常见的激活函数：
 1. Sigmoid
*   **公式：** $f(x) = \frac{1}{1+e^{-x}}$
*   **特点：** 将输出压缩到 **(0, 1)** 之间。
*   **缺点：** 容易导致**梯度消失 (Gradient Vanishing)**；输出不是以 0 为中心的。
*   **适用：** 二分类问题的输出层（表示概率）。

 2. ReLU 
*   **公式：** $f(x) = \max(0, x)$
 $x>0$ 时直接输出 $x$，$x\le0$ 时输出 0。现代深度学习最常用的函数，计算快且解决了梯度消失，常用于隐藏层。


3. Softmax
*   **作用：** 通常用于**多分类问题的输出层**。
*   **特点：** 将一组输出数值转换为**概率分布**，所有概率之和为 1。





记忆相关的 记忆怎么一步步构建、实现的  基础的
序列 RNN LSTM  注意力  前沿的大模型  
LSTM！！！  清楚了解一切细节  PPT有 （结构、为什么）


注意力机制 类型、如何起作用、工作   侧重于自注意力机制

大模型智能体，设计类   给定一个背景（问题） 设计一套xxx系统 逐项作答
落实到案例？

多模态信息融合  什么叫多模态、融合时常用的方法，融合时会有什么问题？   如何做好    也是设计类题目（提供一个背景，比如教育相关、心理健康相关，比如什么十天以后的天气？考虑数据特点？）


为了实现记忆，现在有哪些前沿的方法，这些方法的特点、技术要点，相互作用是？
长上下文的理解，与rag的对比


关于贝叶斯理论的讨论，与具体背景结合
什么是？贝叶斯分类、网络； 公式各个部分的关系，模拟了怎样的人类认知。


前沿探索题目： 
智能决策  与具身智能小小结合  对强化学习的理解   马尔科夫的理解（为什么跟这个有关？）

