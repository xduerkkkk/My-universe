
七大题
简答、论述、设计（内容和之前类似）、前沿探索（也与之前类似）
答题时，建议用“**英文术语 + 中文解释**”的方式，既显得专业又能看懂题目。

*   **Inputs (输入层, $x$):**
    *   **作用：** 接收来自外部的数据或者上一层神经元的输出。
*   **Weights (权重, $w$):**
    *   **作用：** 调节每个输入信号的重要性（强度）。
    *   **原理：** 类似于线性方程 $y=ax+b$ 中的斜率 $a$。权重越大，该输入对结果的影响越大。它是网络通过训练主要学习的参数。
*   **Bias (偏置, $b$):**
    *   **作用：** 调整激活的阈值。
    *   **原理：** 类似于线性方程中的截距 $b$。它允许激活函数左右平移，即使输入为 0，神经元也能被激活（或保持不激活）。
*   **Weighted Sum / Linear Combination (线性加权求和, $z = \sum w_i x_i + b$):**
    *   **作用：** 将所有输入信号进行汇总。
*   **Activation Function (激活函数, $\sigma(\cdot)$):**
    *   **作用：** **引入非线性因素 (Non-linearity)**。这是最重要的部分！
    *   **原理：** 如果没有它，无论网络有多少层，最终都只是一个线性模型，无法解决复杂问题（如异或问题 XOR）。它决定了神经元是否“被点燃”（fire）。

---

### 二、 常见的激活函数 (Common Activation Functions)

这部分通常要求列举 3-4 个，并简述特点。

#### 1. Sigmoid
*   **公式：** $f(x) = \frac{1}{1+e^{-x}}$
*   **特点：** 将输出压缩到 **(0, 1)** 之间。
*   **缺点：** 容易导致**梯度消失 (Gradient Vanishing)**；输出不是以 0 为中心的。
*   **适用：** 二分类问题的输出层（表示概率）。

#### 2. Tanh (双曲正切)
*   **公式：** (形状类似 Sigmoid，但是拉长了)
*   **特点：** 将输出压缩到 **(-1, 1)** 之间。
*   **优点：** 输出是以 0 为中心的 (Zero-centered)，通常比 Sigmoid 收敛快。
*   **缺点：** 依然存在梯度消失问题。

#### 3. ReLU (Rectified Linear Unit, 线性整流单元) —— **必写，最常用！**
*   **公式：** $f(x) = \max(0, x)$
*   **特点：** $x>0$ 时直接输出 $x$，$x\le0$ 时输出 0。
*   **优点：** 计算非常简单（速度快）；有效缓解了**梯度消失**问题。
*   **缺点：** Dead ReLU 问题（某些神经元可能永远不会被激活）。

#### 4. Softmax
*   **作用：** 通常用于**多分类问题的输出层**。
*   **特点：** 将一组输出数值转换为**概率分布**，所有概率之和为 1。

---

### 三、 满分论述模板 (可以直接背诵思路)

**Question:** *Explain the structure of a Perceptron and the role of Activation Functions.*

**Answer Structure (中文作答思路):**

1.  **定义 Perceptron：** 感知机是神经网络的基本构建单元，它模拟了生物神经元的工作方式。
2.  **描述结构：** 它由输入 ($x$)、权重 ($w$)、偏置 ($b$)、加权求和 ($\sum$) 和激活函数 ($f$) 组成。（此处可画图）。
3.  **解释线性部分：** 权重代表输入的强度，偏置代表阈值的调整。它们组合形成线性变换 $z = Wx + b$。
4.  **解释非线性部分（核心）：** 激活函数的作用是引入**非线性 (Non-linearity)**。
    *   *Why?* 如果没有激活函数，多层神经网络退化为单层线性网络，无法拟合复杂的非线性数据。
5.  **举例：**
    *   **Sigmoid:** 适用于二分类输出，但有梯度消失问题。
    *   **ReLU:** 现代深度学习最常用的函数，计算快且解决了梯度消失，常用于隐藏层。
    *   **Softmax:** 专门用于多分类的输出层，将输出转化为概率。

# 1 神经网络
神经网络的结构   画下来、讲清楚 其中部分的作用、原理、名字
常见的激活函数  


perceptron 神经元，是神经网络的基本单位
- inputs:接收来自外部的数据或上一层神经元的数据
- weights：
	- 作用：调节每个输入信号的重要性
	- 原理：输入的权重越大，该输入对结果的影响越大。它是网络通过训练主要学习的参数。
- bias：
	- 作用：调整激活的阈值
	- 原理：类似线性方程中的截距b，它允许激活函数左右平移，还能控制神经元被激活的阈值。
- Weighted Sum,线性加权求和, $z = \sum w_i x_i + b$ 
	- **作用：** 将所有输入信号进行汇总。
 - **Activation Function (激活函数, $\sigma(\cdot)$):**
    *   **作用：** **引入非线性因素**。
    *   **原理：** 如果没有它，无论网络有多少层，最终都只是一个线性模型，无法解决复杂问题（如异或问题 XOR）。


常见的激活函数：

  Sigmoid
*   **公式：** $f(x) = \frac{1}{1+e^{-x}}$
*   **特点：** 将输出压缩到 **(0, 1)** 之间。
*   **缺点：** 容易导致**梯度消失 (Gradient Vanishing)**；输出不是以 0 为中心的。
*   **适用：** 二分类问题的输出层（表示概率）。

Tanh (双曲正切)
*   **公式：**$tanh(x) = (e^x - e^-x) / (e^x + e^-x)$
*   **特点：** 将输出压缩到 **(-1, 1)** 之间。
*   **优点：** 输出是以 0 为中心的 (Zero-centered)，通常比 Sigmoid 收敛快。
*   **缺点：** 依然存在梯度消失问题。


  ReLU 
*   **公式：** $f(x) = \max(0, x)$
 $x>0$ 时直接输出 $x$，$x\le0$ 时输出 0。现代深度学习最常用的函数，计算快且解决了梯度消失，常用于隐藏层。




# 2 RNN到Transformer，LSTM细节
记忆相关的 记忆怎么一步步构建、实现的  基础的
序列 RNN LSTM  注意力  前沿的大模型  
LSTM！！！  清楚了解一切细节  PPT有 （结构、为什么）

这里的记忆，指上下文信息的保留能力。

- rnn，通过隐状态的传递，即当前状态等于当前输入＋上一刻的记忆   信息随着时间步衰减   存在梯度消失的问题，导致只有短期记忆 无法捕捉长距离依赖
    $$ h_t = \sigma(W x_t + U h_{t-1}) $$
- lstm  引入细胞态 ,理解为记忆。设计三个门（遗忘门，输入门，输出门） 。将记忆和输入分开，记忆横穿整个时间步，选择性地让关键信息保留在cell state. 模型可以自主学习记住什么，忘记什么，大大增强长距离记忆能力
- transformer 并行计算+自注意力机制构建记忆 不依赖时间步 没有距离衰减   生成动态词向量 提升了对复杂语义的理解

LSTM细节：
*   底部输入：$x_t$ (当前词), $h_{t-1}$ (上一时刻的短时状态)。
*   左上进入：$C_{t-1}$ (上一时刻的长时记忆)。
*   右上输出：$C_t$ (传给下一时刻的长时记忆)。
*   右下输出：$h_t$ (当前时刻的输出/短时状态)。
![](../../assets/image/认知计算-1765287969216.jpeg)
- 遗忘门，决定要丢弃什么旧信息。输入的是当前信息xt，上一时刻短时记忆ht-1，  使用sigmoid函数，0就是遗忘，1就是保留。最后输出的ft，直接乘到上一时刻的Ct-1，也就是，通过上一时刻短时记忆和当前信息，决定要不要丢弃过去的长时记忆。
- 输入门，决定这个时间步储存什么新信息。先是生成 $i_t$，叫输入们，决定每个信息的权重。然后生成候选记忆，是对xt和ht-1信息的处理，称为  $\tilde{C}_t$
- 有了遗忘门和输入门后，就可以直接更新Ct了。$$ C_t = f_t * C_{t-1} + i_t * \tilde{C}_t $$遗忘，就是旧记忆乘以遗忘系数（比如乘以 0.1 就忘得差不多了）**记忆**，就是根据输入门的权重，把新候选信息加进去。
- 输出门：
    $$ o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) $$
    (决定输出哪些部分)
    $$ h_t = o_t * \tanh(C_t) $$
    (把当前的细胞状态 $C_t$ 经过 Tanh 归一化到 -1~1 之间，再过滤一遍，得到最终输出 $h_t$)


一些细节：
- sigmoid函数，用在”门“上，0到1表示开关程度。Tanh，用在处理信息上。-1到1，表示增加或减少某种特征
- 细胞更新状态，是加法。只要ft经常接近1，梯度就能以近乎无损的方式一直传到很久以前的时间步。而在普通 RNN 中，梯度是连乘权重矩阵，容易梯度消失。
# 3注意力机制 类型、如何起作用、工作   侧重于自注意力机制

自注意力、交叉注意力、多头注意力。
以自注意力机制为例，
每个输入的词向量，都经过线性层生成Q、K、V三个矩阵。
假设最后的维度为，`[seq_len,d_model]` 
使用Q点积K转置矩阵，就能实现每一个词向量的Q与每一个词向量的K作”匹配“，生成注意力分数矩阵。`[seq_len,seq_len]`
然后我们使用softmax，讲分数变为权重。
整个注意力权重矩阵 与 V做矩阵乘法，实际上是一一加权求和。
最后得到`[seq_len,d_model]`  
我们再除以根号d_model ，保证数据的稳定和训练的收敛。
最后结果就是所有的新的词向量。






大模型智能体，设计类   给定一个背景（问题） 设计一套xxx系统 逐项作答
落实到案例？

多模态信息融合  什么叫多模态、融合时常用的方法，融合时会有什么问题？   如何做好    也是设计类题目（提供一个背景，比如教育相关、心理健康相关，比如什么十天以后的天气？考虑数据特点？）


为了实现记忆，现在有哪些前沿的方法，这些方法的特点、技术要点，相互作用是？
长上下文的理解，与rag的对比


关于贝叶斯理论的讨论，与具体背景结合
什么是？贝叶斯分类、网络； 公式各个部分的关系，模拟了怎样的人类认知。


前沿探索题目： 
智能决策  与具身智能小小结合  对强化学习的理解   马尔科夫的理解（为什么跟这个有关？）

