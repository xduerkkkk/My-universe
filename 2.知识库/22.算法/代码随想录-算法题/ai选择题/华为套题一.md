# **下面这段代码执行的功能是什么?**

`1. function forward(x, W,b): 2.   logits = Wx+b   3.   exp_values = exp(logits)   4. return exp_values / sum(exp_values)`

[[ai知识基础/大模型，nlp基础/归一化]]

## 雅可比迭代
![[华为套题一-1756545707849.jpeg]]

如果面试官追问原理，你不需要背诵数学定理，你可以说：

> “我理解雅可比迭代法的有效性，是因为当系数矩阵满足特定条件（比如对角占优）时，迭代过程会形成一个‘压缩映射’。这意味着每迭代一次，我们的解都会更接近真实解。对角占优保证了迭代的稳定性，使得解不会发散，而是稳定地收敛。”

## 奇异值分解
![[华为套题一-1756546970005.jpeg]]1. **线性代数核心概念**:
    
    - **矩阵转置 (Transpose)**: 理解 Aᵀ 的含义。
        
    - **正交矩阵 (Orthogonal Matrix)**: 知道其定义 QᵀQ = QQᵀ = I，以及它代表着旋转或反射，不改变向量长度。SVD中的 U 和 V 都是正交矩阵。
        
    - **对角矩阵 (Diagonal Matrix)**: 知道它的结构和性质。SVD中的 Σ 是对角矩阵。
        
    - **特征值与特征向量 (Eigenvalues and Eigenvectors)**: 必须深刻理解 Ax = λx 的定义，这是理解SVD与特征值分解关系的基础。
        
2. **奇异值分解 (SVD) 本身**:
    
    - **分解形式**: 必须牢记 A = UΣVᵀ。
        
    - **矩阵属性**: 清楚地知道 U, Σ, V 各自的名称（左奇异向量、奇异值、右奇异向量）和数学属性（正交、对角、正交）。
        
    - **与特征值分解的联系 (核心考点)**:
        
        - **左奇异向量 (U)** 是 A Aᵀ 的特征向量。
            
        - **右奇异向量 (V)** 是 Aᵀ A 的特征向量。
            
        - **奇异值 (σᵢ)** 是 Aᵀ A (或 A Aᵀ) 的特征值的平方根。
            
    - **唯一性问题**: 知道SVD不是严格唯一的，至少存在符号上的不确定性。
## 特征向量
![[华为套题一-1756558097307.jpeg]]

1. “不可逆”是一个非常重要的信号。一个矩阵不可逆，等价于说它的**行列式为0**，也等价于说它**至少有一个特征值为0**。