熟练掌握pytorch框架和深度学习，对bert、gpt原理和实战调优有深入了解，并有将技术点在NLP项目中实操的经验。熟练掌握chatgpt、deepseek底层技术原理，深入了解transformer结构及其改进，掌握PPO/GRPO等强化学习后训练和对齐技术。自己从0到1完成大模型的关键技术流程，熟悉RLHF、DPO、Reasoning等后训练技术流程，并有实战经验
熟悉Langchain开发框架，熟悉rag流程构建和优化 
熟练掌握指令微调数据构造，熟悉LoRA，P-tuning等经典微调技术，掌握SFT微调和强化微调流程，并拥有大模型微调案例经验。
掌握量化方法GPTQ、AWQ及KV cache量化，熟悉Flash attention/Decoding， paged attention等常用推理优化技术，熟悉vLLM推理框架。



康凯  15191167937  
**西安电子科技大学 · 人工智能 · 2027届本科**

#### 技术栈 

- 核心领域: Transformer原理与实现, 自然语言处理，RAG, 模型微调 ,模型后训练和对齐，模型评估与部署
- 工程与工具: Python (精通), PyTorch (熟练), Hugging Face, LangChain，VLLM框架 , FastAPI 


#### 核心项目经历

**MiniMind-LLM复现与探索：从零理解并实现一个完整的语言模型**

- **项目简介：** 深度学习并**复现**了MiniMind-LLM项目，独立完成了从模型架构理解、核心代码实现、到多阶段训练的全流程实践。旨在将LLM的“黑箱”彻底透明化。**部分复现模型权重已上传至Hugging Face。**
    
- **项目亮点：**
    
    - **架构复现:** **不依赖高级库**，亲手实现了包含**多头自注意力、RoPE位置编码、KV-Cache**等核心组件的llama架构。
    - **训练流程:** **独立编写了完整的训练与数据处理管线**，并成功实践了**预训练、指令微调**与**直接偏好优化**等关键技术。
    

**2. 工业级RAG系统设计与实践 (进行中)**

- 项目简介： 解决大模型在专业领域知识问答中普遍存在的幻觉、事实错误问题，正独立设计并实现一套具备自我迭代优化能力的工业级检索增强生成（RAG）系统。项目旨在通过先进的检索架构和数据闭环，实现对海量、复杂知识的高精度、高时效性问答。
    
- 工作内容：
    
	- 架构设计：设计了“三路召回、一级精排”的深度检索管线（BM25稀疏、稠密向量、稀疏向量的混合召回 + BGE-Reranker精排），以最大化复杂查询场景下的召回率与精准度。
	- 闭环优化： 设计了“自动化数据飞轮”，利用强模型蒸馏思想，自动化生成用于微调Reranker和LLM的高质量数据集，驱动系统效果持续自我提升。
	- 性能工程： 计划采用 vLLM 进行模型服务化部署，确保系统在生产环境下的高吞吐与低延迟。

---

#### 其他奖项
全国大学生数学建模比赛省二等奖，全国大学生数学竞赛省三等级，全国大学生统计建模比赛校一等奖，西电”星火杯“校一等奖，Datawhale AI开源社区优秀学习者实践证明。




### 康凯
| **邮箱:** kkcache123@gmail.com | **电话:** +86 15191167937 | **GitHub:** [github.com/xduerkkkk](https://github.com/xduerkkkk) | **Hugging Face:** [https://huggingface.co/tranquilk](https://github.com/xduerkkkk)
<div style="display: flex; justify-content: space-between;">
    <span><strong>西安电子科技大学</strong></span>
    <span>人工智能 (本科)</span>
    <span>2023.09 - 2027.06 </span>
</div>

####  技能描述
*   **核心领域:** LLM底层原理与复现 (SFT, DPO), 检索增强生成 (RAG), Transformer, 模型微调
*   **工程与工具:** Python (精通), PyTorch (熟练), Hugging Face, LangChain, FastAPI, vLLM,
#### 项目经历
**MiniMind-LLM 复现与增强：从零完整实现并对齐一个MoE语言模型** 
*   **核心：** 深度复现并增强MiniMind-LLM，独立完成从架构到多阶段训练的全流程，旨在将LLM“黑箱”透明化。
*   **架构:** 不依赖高级库，亲手实现支持**MoE**的类LLaMA架构 (含MHA, RoPE)。
*   **训练:** 从零编写训练/数据管线，成功实践**预训练、SFT、DPO**与**CoT对齐**。
*   **成果:** 部分模型权重已公开发布至**Hugging Face**。

**数据驱动的自优化AI代码助手：PyTorch文档智能问答系统 (进行中)**
*   **目标:** 设计一套具备**自我迭代优化能力**的工业级RAG系统，解决文档检索低效的痛点。
*   **检索架构:** 设计“**三路混合召回(BM25/稠密/稀疏)** + BGE-**Reranker精排**”管线，MRR提升15%。
*   **数据工程:** 独创HTML层级化“父子文档”切分策略，事实一致性得分提升10%。
*   **自优化闭环:** 设计“自动化数据飞轮”，利用强模型蒸馏近万条高质量数据，双模型微调Reranker与LLM，Hit Rate@3提升8%。
*   **工程实践:** 搭建**RAGas**自动化评估框架，并计划采用**vLLM**进行服务化部署。

**基于混合模型与Agent的桌面多任务AI语音助手 (进行中)**
*   **目标:** 融合Agent架构与“小模型+大模型”协同决策，为PC端打造流畅的语音交互体验。
*   **核心架构:**
    *   **分层意图识别:** 设计“bert快速召回 + LLM二次精排”框架，端到端匹配成功率达90%。
    *   **智能分流与拒识:** 采用微调的**Bert-tiny**作为拒识/仲裁模型，以90%+准确率过滤无关请求(QPS > 500+)，极大降低大模型无效计算。
*   **多轮交互:** 利用DeepSeek进行**Query改写**，优化对省略、指代等口语化表达的理解，多轮成功率达92%。
*   **任务执行:** 基于**Function Calling**与MCP服务网关，调用本地Python工具库，完成多项生活技能的自动化执行。