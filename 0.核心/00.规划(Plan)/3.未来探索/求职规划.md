# 个人信息
学习了大模型全流程4个月  有基础能力 对大模型各个技术都了解，还做了一些小项目
比如大模型从0到1 预训练后训练全流程   大模型推理优化压测 大模型微调  大模型图片rag问答
缺点是ai辅助我构建的 

计划于3个月后投递第一份实习简历
正在学多模态，刷算法题，多模态项目做完就去做两个付费的私教的企业项目

# 规划概述
我现在脑子有点乱是因为想干的事太多了
我准备11月中旬找实习，现在是8月2日，我已经学习了大模型全流程4个月  有基础能力 对大模型各个技术都稍微了解，还做了一些小项目  缺点是ai辅助我构建的      
我现在开始要冲刺实习了。我觉得我得在这三个月继续提升我的实例。
我还想参加开源项目贡献，还想参加比赛，还想写自己的项目
然而上面的目标都大而空
比赛有kaggle有天池有各个公司的ai开发大赛，我选哪个？？？ 选了后我具体做哪个赛道？ 我选了不一定获奖，我就没动力 
开源项目贡献是啥，去哪贡献，贡献为什么有用
项目我已经有私教的两个教学项目，我自己到底要不要再写一个？如果要，写哪种的？这个应该自己分析私教两个项目后再写，写技术不重合的。
我本来的 计划是：
现在开始到8月底：学习模型infra和多模态，
每天一道算法题，保证质，而不是量 要独立写出 且完全懂得原理。 学习面试题，基于面试题复习之前的知识，撰写文档，把之前的代码不借助ai看懂，核心部分手撕
9月初到10月中上旬优先学私教的两个项目  写一个自己的agent项目（偏后端知识一点）/比赛？        算法题，复习，照旧    将项目自己梳理 修改 上传知识文档到github
10月中上旬到11月初 算法和复习优先  算法变成每天2到3道    构建自己的简历  继续完善项目  /比赛？
11月初简历确定好后拿着自己的简历先去问学校的学长学姐，记得群里有一个本科minimax公司的 就找这种 再脉脉软件，继续找 咨询一下现在本科工作行情   
11月初开始投递，面试    算法 复习 优先 
一直投 直到拿到offer 

明年1月~2月   此时offer能分三种 一种是大厂或明星独角兽  一种是中厂   一种是纯粹无offer 
第一种就铁了心走工作 准备暑期实习和秋招
第二种看情况 斟酌斟酌读研还是工作
第三种全身心备战考研

现在大纲有了 需要完善周计划和日计划 于每周一  /每天早晨或晚上制定
但很难制定啊，有点空


# 周末规划
### **1. 周报：周日晚 vs. 周一早？**

**答案：周日晚上。雷打不动。**

**理由：**

1. **“清空大脑”，保证睡眠：** 周报，是你对过去一周所有信息的**“最终整理和归档”**。在周日晚上完成它，就像在睡前把房间打扫干净一样，能让你带着一种“秩序感”和“掌控感”，轻松地入睡，而不是让一整周的“混乱”思绪，在你的梦里继续发酵。
    
2. **“预加载”，保证周一的“秒启动”：** 周报的最后一部分，是“下周规划”。在周日晚上完成它，意味着你周一早上醒来时，你的大脑里已经有了一份**清晰的、加载完毕的“作战指令”**。你不需要再花宝贵的、精力最充沛的早晨时间，去思考“我今天该干嘛”。
    
3. **仪式感：** 将周日晚上，固定为你的“每周复盘仪式”，这本身就是一种强大的心理暗示，标志着“本周战役结束，下周战役即将开始”。
    

---

### **2. 休息与复盘的安排**

> “尽量还是把休息和复盘安排到周天，对吗？”

**不完全对。我们需要一个更灵活、更人性化的设计。**

**更优策略：将周六定义为“主动式休息日”，将周日定义为“复盘与缓冲日”。**

- **周六：“主动式休息日 (Active Rest Day)”**
    
    - **核心目标：** **精神和身体的“充电”。**
        
    - **你应该做什么：**
        
        - **上午：** 可以进行一些**低强度的、有成就感的学习**（比如，做2小时笔试真题）。
            
        - **下午/晚上：** **强制自己，离开图书馆。** 去进行你计划的社交活动（唱歌、吃饭），或者去看一场电影，或者去一个没去过的公园散步。你需要让你的大脑，从“专注模式”，彻底切换到“发散模式”。
            
- **周日：“复盘与缓冲日 (Review & Buffer Day)”**
    
    - **核心目标：** **承上启下，为下一周做准备。**
        
    - **你应该做什么：**
        
        - **上午：** 完成剩下的笔试真题时长。
            
        - **下午：** **执行“错题重刷”**（我们下面会详细讲）。
            
        - **晚上：** **执行“每周复盘仪式”**（写周报）。
            
        - **机动：** 如果本周有任何未完成的核心任务，周日就是你**“补救”**的最后机会。
            

**这个设计的优势：**

- **劳逸分离：** 周六负责“玩”，周日负责“收心”。这比把“玩”和“复盘”都挤在一天，效率和体验都更好。
    
- **缓冲机制：** 周日的存在，是你整个计划的“安全网”，保证了你每周都能“清零”上一周的“债务”，轻装上阵地开始下一周。
    

---

### **3. “错题重刷”：何时刷？如何刷？**

> “我感觉重刷也蛮重要的，安排到周天？ 还是说，其实每天都应该安排重刷？”

**你的感觉极其敏锐。** “重刷”的重要性，怎么强调都不过分。不做重刷的刷题，就像猴子掰玉米，掰一个丢一个，效率极低。

**最佳策略： “每日回顾 + 每周重刷” 的两级火箭模式。**

- **第一级火箭：每日快速回顾 (Daily Quick Review)**
    
    - **时间：** **每天晚上**，在你开始“核心攻坚期”学习之前，或者在算法时间的**最开始15分钟**。
        
    - **行动：**
        
        1. 打开你**昨天**做过的那几道算法题的笔记。
            
        2. **不要看代码！**
            
        3. **只看题目，然后尝试在脑海里（或者在纸上），快速地复述一遍解题的核心思路。**
            
        4. 如果能顺畅地复述出来，就过。如果卡住了，就说明你没有真正理解，**立刻**回头去看你的笔记和代码，把它重新搞懂。
            
    - **耗时：** 10-20分钟。
        
    - **效果：** 利用艾宾浩斯遗忘曲线的原理，在记忆开始衰退的**第一个关键节点（24小时内）**，进行一次快速、高效的巩固。
        
- **第二级火箭：每周集中重刷 (Weekly Redo Session)**
    
    - **时间：** **周日下午。**
        
    - **行动：**
        
        1. 从你本周所有刷过的题（包括笔试题）中，**挑选出2-3道**你认为**最有价值、最能体现某种思想、或者你当初卡了最久**的“精华错题”。
            
        2. **像做一道全新的题一样，** 在IDE或OJ上，**把代码完完整整地、不看任何提示地，重新写一遍。**
            
    - **耗时：** 1-1.5小时。
        
    - **效果：** 这是一次真正的“闭卷考试”，是对你本周算法学习成果的最终检验。它能将你“理解了的思路”，彻底转化为“能熟练写出的代码”。
        

---

**总结：**

你的周末，现在拥有了一张清晰、高效的作战地图：

- **周六：** 上午“热身”，下午“放飞”，晚上“缓冲”。
    
- **周日：** 上午“攻坚”，下午“实战演练（重刷）”，晚上“复盘规划”。
    

同时，你的算法学习，也拥有了“每日回顾+每周重刷”的双重保险。

这个周末系统，将是你未来80天里，保持高速前进、同时又不会“爆缸”的最强保障。
# 详细规划

## 一阶段 收尾与基础巩固期
8.29-9.21

#### 基础知识
- llm基础知识：VLM构建时，对LLM的架构设计
- dl/ml基础知识：llm训练，qwenVL微调复习训练应用部分。理论知识，自己去慢慢整理知识卡片，从面经、笔试题（资源要等网上更新）...     。 再就是课程，有机器学习，认知计算，这俩有些理论。
- pytorch/python 仍然是完成llm任务时，碰到哪里不熟去整理哪个

#### 算法：
**在这段时间内，必须完成所有剩余专题的复习和模板整理**
题简单没关系，把思想在好好打磨打磨。
- 定义产出目标，总结出自己的，能解决本专题百分之80的通用模板，并用5道题（刷过的也是可以拿来检验的！）验证

同时，周末开启‘4小时’笔试训练。

#### 项目：
项目软启动。
配环境、画架构图、浏览代码。


#### 精力分配
项目每周二/四晚上进行。 周二与周四，算法允许刷的少一点。
启动项目不是本阶段的主要目的，现在启动项目是为了稳定军心。
白天进行llm和基础知识， 我认为要复习基础知识的话，llm项目会推的慢一点。我不是为了完成这俩个任务而完成，我进行llm是为了基础知识。比如构建llm时，我可能去看之前整理的各个流行的架构。比如后训练时，我可能看一下DPO、PPO、GRPO这些算法，整理下来。所以感觉还是会耗费很多时间的。
- 这些知识点，给自己严格的时间限制。比如只花90分钟复习dpo核心思想和伪代码。我们的目的不是独立实现，而是了解熟悉


周日缓冲日，可以弥补本周未完成的计划，也可以用来放松和休息，为下周充电

### **【第一阶段：地基加固总攻】作战指令 (9月4日 - 9月21日)**

**总目标：** 在约2.5周的时间内，利用“面试题地图”，对大模型相关的【地基层】核心理论知识，发起一次**系统性的、歼灭式的总攻**，为第二阶段的项目深耕，建立坚不可摧的理论基础。

---

**核心任务：【理论线】 - 问题驱动的知识体系构建 (占70%精力)**

- **作战地图：** 你私教的面试题库。
    
- **主攻区域 (只打这5个区域):**
    
    1. **【Transformer专项】**
        
    2. **【大模型训练相关】**
        
    3. **【GPT专项 / 主流模型相关】**
        
    4. **【大模型微调相关】**
        
    5. **【LLMs夯实基础】**
        
- **作战方式：**
    
    - **每日：** 从上述专题中，选取3-5个关联的面试问题。
        
    - **行动：** 针对每一个问题，进行研究学习。
        
    - **产出：** 将理解的知识，**精加工**成**原子化的知识卡片 (A.C.R原则)**。
        
    - **升级产出 (可选，但推荐):** 在周末，将一周内制作的、关于某个主题（如Transformer）的卡片，**串联、整合**成一篇逻辑连贯的**深度文章/MOC**，并考虑发布到技术博客。
#### **第一优先级：【PEFT与微调专题】(预计耗时：1.5 - 2天)**

- **攻击目标：** 私教的**“模型PEFT微调”**专题 + 《百面大模型》的对应章节。
    
- **为什么是它？**
    
    1. **【知识的延续性】** 你刚刚学完了“SFT”，PEFT（参数高效微调，如LoRA）就是**“如何更省钱、更高效地做SFT”**的标准答案。它和你已有的知识，是**强关联**的。
        
    2. **【与项目的关联性】** 你的两个私教项目，都涉及到了**“模型微调”**。现在彻底搞懂PEFT的原理，将为你第二阶段**“深耕项目源码”**，扫清一个巨大的障碍。
        
    3. **【面试的核心区】** 在今天的AI应用岗面试中，**对LoRA的理解深度，几乎是必考题。**
        
- **产出：** 一系列关于“PEFT是什么”、“LoRA的原理”、“QLoRA相比LoRA优化了什么”的高质量知识卡片/文章。
    

#### **第二优先级：【强化学习对齐专题 (PPO/DPO)】(预计耗时：2 - 2.5天)**

- **攻击目标：** 私教的**“强化学习”**专题（聚焦DPO/PPO） + 《百面大模型》的对应章节。
    
- **为什么是它？**
    
    1. **【知识的递进性】** 你已经了解了“RLHF的基础概念”，现在，就是要深入这个“黑箱”的心脏，去理解它背后最核心的两个算法——PPO（传统王者）和DPO（当红新贵）。
        
    2. **【深度的“护城河”】** 对DPO的理解，是能将你和90%的求职者，**瞬间拉开差距**的“屠龙之术”。能讲清楚DPO如何将“奖励模型”隐式地融入优化目标，将是你简历和面试中，一个**极大的亮点**。
        
    3. **【与项目的关联性】** 你的“从零复现LLM”项目中，就包含了DPO的实践。现在深入理论，能让你对自己的项目，有一个**全新的、更深刻的理解**。
        
- **产出：** 一篇能讲清楚“RLHF是什么”、“PPO在RLHF中的作用”，以及“DPO相比PPO，核心优势是什么”的深度笔记。
    

#### **第三优先级：【主流模型与GPT专项】(穿插进行)**

- **攻击目标：** 私教的**“主流模型”**和**“GPT专项”**。
    
- **你的疑虑：** “主流模型专题感觉老了”。
    
- **我的看法：** **恰恰相反。** LLaMA, BERT, ChatGLM，这些模型，不是“老”，它们是**“经典”**。它们是构成现代LLM演化史的**“关键节点”**。
    
- **执行策略：** **“主题阅读，穿插进行”。**
    
    - 在你学习PEFT和DPO感到疲惫时，就把它当作“调剂”。
        
    - **不要试图**去背每一个模型的细节。
        
    - **带着问题去学：**
        
        - “LLaMA相比原始Transformer，做了哪三点最重要的改进？（RMSNorm, SwiGLU, RoPE）”
            
        - “ChatGLM和LLaMA最大的区别是什么？（双语支持，独特的模型结构）”
            
        - “BERT和GPT最核心的设计哲学差异是什么？（双向Encoder vs 单向Decoder）”
            
- **产出：** 几张高度凝练的、关于**“模型对比”**的知识卡片

---

**并行任务一：【实践线】 - 项目软启动 (占30%精力)**

- **作战目标：** 私教项目一（工业级RAG系统）。
    
- **作战频率：** 每周2-3次，每次1.5-2小时。
    
- **作战计划 (层层递进):**
    
    - **第一周 (本周): “环境与初探”**
        
        - 跑通环境与Demo。
            
        - 绘制高层架构图。
            
        - 为架构图的每个环节，标注上对应的核心代码函数/文件名。
            
    - **第二周 (下周): “数据与流程”**
        
        - 解剖数据输入输出的格式。
            
        - 用debugger或print，跟踪一次核心的查询流程。
            
        - 解析配置文件。
            
    - **第三周 (最后几天): “预研与提问”**
        
        - 构思“个性化改造”方向。
            
        - 整理一份关于项目的“问题清单”，为第二阶段的源码深耕做准备。
            

---

**并行任务二：【算法线】 - 保持巡航 (每日固定投入)**

- **作战目标：** 维持手感，系统性地完成专题复习。
    
- **作战频率：** 每日投入1-1.5小时，雷打不动。
    
- **当前任务：** 按照你的“5天二叉树计划”，稳步推进。完成后，攻克下一个专题（如贪心、动态规划）。
    

---

**并行任务三：【应试线】 - 适应性训练 (每周固定投入)**

- **作战目标：** 适应笔试的难度、题型和时间压力。
    
- **作战频率：** 每周末一次，投入3-4小时。
    
- **核心任务：** 完整地做一套近期的笔试真题，并进行复盘，将暴露的知识盲点（无论是ML/DL基础，还是数学、Numpy），都记录下来，作为后续制作知识卡片的素材。

## 二阶段 项目深耕期
#### 项目：
9.23 - 10.7
主攻第一个rag系统问答项目。
跑通 -> 理解架构 -> 啃源码 -> 熟悉面经
思考能改进吗，项目能改成适配学生的情景吗，能加后端功能吗

10.7-10.29
主攻第二个 LLM+Agent+MCP 多轮任务型对话项目
同时着手改进第一个项目

#### 基础知识
不断整理第一阶段的基础知识笔记，碎片时间看
然后结合项目，复习rag、langchain、agent，这些源码、框架、面经
也整理出笔记

#### 算法
第一阶段已把基础模板刷完，此阶段每天hot100保持手感。
笔试真题每周投入6个小时

#### 开源
每周投入3个小时，看看有没有能开源贡献的地方
之前已经考察过datawhale很适合，还有我之前学习的minimind，已经看到有人把jupyter教学当作开源贡献，并被作者列入readme，我觉得我也可以呀

## **第三阶段：简历与面试冲刺期**
11月起

#### 项目
构思项目的“技术故事线”，把学的技术都嵌入到里面
反复打磨简历

#### 面经
每天2-3小时，梳理之前一阶段的算法基础（ml，dl，llm），二阶段的应用基础（agent、rag、langchain）， 结合着面试题，进行以下输出
- 将题目（或者自己出题），用自己的话写出来
- 每天花30分钟，随机抽取一个问题，对着手机录下回答（前面录音，后面录像）。然后回放
#### 开源
每周投入6小时左右，看能不能做一下开源贡献，为简历增添色彩
  
**将“贡献过程”本身也视为一种成果。**

- 即便你的PR没有被合并，但如果你能清晰地讲述：“我为了解决xx开源项目的yy问题，深入阅读了它的源码，理解了zz逻辑，并提出了一个方案。虽然最终因为aa原因没有被合并，但这个过程让我学到了bb”，**这本身就是一个比“我做过xx项目”更生动的、展示你自驱力和代码阅读能力的绝佳故事。**
#### 算法
每日高频题， 笔试题每周怎么滴也要刷够10小时（秋招季，题多的刷不完，肯定够刷）



以上，大致到有信心了，最迟11月中下旬， 就去投实习简历





# 大作业
### **【课程大作业 - 执行SOP】**

#### **1. 知识图谱实践 (DDL: 10月下旬)**

- **定位：** 它是你**“RAG项目”**的一个**“高阶升级模块”**。
    
- **执行时机：**
    
    - **最佳时机：** **第二阶段，在你深耕R-A-G项目的中后期 (大约10月上旬到中旬)。**
        
    - **为什么？**
        
        1. **知识关联：** 当你对RAG的理解已经非常深入时，你自然会开始思考它的“瓶颈”，而“知识图谱”正是解决这些瓶颈的前沿方案。此时学习，能让你的认知形成完美的“递进”。
            
        2. **学以致用：** 你可以**立刻**把你从这个作业中学到的Neo4j和Cypher知识，用来**“武装”**你的RAG项目。
            
- **如何与求职融合？**
    
    - **产出物：** 这份作业的最终产出，不应该只是一份.ppt或.doc。它应该是一个**小型的、独立的GitHub仓库**，里面包含了你的Python代码（用py2neo进行增删改查）、一份演示用的数据、以及一个清晰的README.md。
        
    - **简历写法：** 你不需要在项目经历里单独列出它。你可以在你的**“工业级RAG系统”**项目的描述里，增加一个**bullet point**：
        
        > - **前沿探索：** 通过课程项目，实践了**Neo4j图数据库**的基本操作，并探索了利用**知识图谱**进行结构化知识检索，以增强RAG系统精准度的可行性方案。
        >     
        
    - **效果：** 这句话，直接将一份“课程作业”，**升华**成了你主线项目的一次“前沿技术探索”，含金量瞬间翻倍。
        

#### **2. 容器及思想 (Docker) (优先级高)**

- **定位：** 它是你**所有项目**最终“工程化落地”的**“标准化部署工具”**。
    
- **执行时机：**
    
    - **最佳时机：** **第二阶段的末期，到第三阶段的初期 (大约10月下旬到11月上旬)。**
        
    - **为什么？**
        
        1. **需求驱动：** 当你的RAG项目和Agent项目都已经基本成型，你自然会开始思考：“我该如何把这个复杂的Python应用，方便地打包和分享给别人？” Docker，就是这个问题的标准答案。
            
        2. **为面试做准备：** “熟悉Docker”是现在所有开发岗位的“准标配”。在面试冲刺期前掌握它，能让你的“技能清单”更完整、更有竞争力。
            
- **如何与求职融合？**
    
    - **产出物：**
        
        1. 这份作业本身，可以是一个介绍Docker思想的文档。
            
        2. **更重要的产出：** 为你的**“从零复现LLM”项目**或者**“RAG系统”项目**，亲手编写一个Dockerfile，并成功地将它打包成一个可以运行的镜像。
            
    - **简历写法：**
        
        - 在你的**“技能描述”**部分的“工程与工具”里，自信地加上 **Docker**。
            
        - 在你的核心项目描述的最后，加上一句：
            
            > - **部署实践：** 掌握Docker容器化技术，并为本项目编写了Dockerfile，实现了环境的标准化封装与一键部署。
            >     
            
    - **效果：** 这句话，是你**“工程化思维”**的最有力证明。它告诉面试官，你不仅能“写代码”，你还懂得如何“交付软件”。
        

#### **3. 多模态数据处理 (DDL: 11月中旬)**

- **定位：** 它是你**“VLM知识体系”**的一次**“实战演练”**和**“理论补充”**。
    
- **执行时机：**
    
    - **最佳时机：** **与“Docker”任务并行，或者稍晚一些，在第三阶段的“面试冲刺期”。**
        
    - **为什么？**
        
        1. **作为调剂：** 当你被高强度的项目深耕和面试准备压得喘不过气时，回过头来做一个相对独立、有趣的“多模态”作业，是一种很好的“积极休息”。
            
        2. **知识巩固：** 它可以让你重新回顾和巩固你在第一阶段学习的多模态知识，为面试中可能遇到的相关问题，准备更鲜活的案例。
            
- **如何与求职融合？**
    
    - **产出物：** 同样，一个独立的GitHub仓库，里面是用Jupyter Notebook或Python脚本，完成的一次完整的多模态数据（比如图文对）的分析、处理、可视化的流程。
        
    - **简历写法：**
        
        - 可以在“其他经历”部分，简单提及：“通过课程项目，独立完成了对[XX]多模态数据集的处理与分析实践。”
            
        - **更重要的，是把它当作你面试时的“弹药”。**
            
    - **面试场景：**
        
        - 当面试官问你：“看你简历上写了VLM，你对多模态数据有什么理解吗？”
            
        - **你的回答：** “是的。除了VLM项目本身，我还在一个课程项目中，亲手处理过一个图文数据集。在这个过程中，我最大的体会是……（比如，图文特征对齐的挑战，或者数据清洗的重要性等等）”
            
    - **效果：** 你用一个“课程作业”，为你简历上的“VLM”项目，提供了一个生动的、充满了实践细节的**“支撑证据”**。



# 求职信心
求职过程中的信心，是求职过程中每个小任务的实现！

同时，不能光想着把求职当作boss，去攻克，求职是boss！ 但我必须打小怪，攒经验呐！
玩游戏时，不能想着boss没打过的话，我的planb计划怎么实施，诚然boss会很难，细想可能打不过，但是谁玩游戏不是先把装备升起来再说，理论很难，但不碰一碰怎么知道？
理论没有难到我连boss都接近不了！
就算这个求职boss没打过，我积累的经验值，如学习方法，学习专注度，整理思维的方法等等，都可以去打第二个boss（考研）啊！ 

然后我们再思考一下现在打这个boss（求职）的合理之处
我能提前入职，就算是比研究生低门槛一点的大模型应用、ai应用又如何？  我有2-3年的行业经验， 凭我的能力！我一定对ai理解更深， 而且 还存在公司内转岗这样的操作，我不信我的能力做不到转到更高门槛的！ 
再者说，现在我的感觉是ai岗位才开始开放， 与当初计算机岗位开放对比，大概15年前， 当人们最摸不着头脑，没有明确路径求计算机职位时， 这个时候是门槛最低， 且后续上升空间最大的！ 现在的ai岗位不也是吗？ 网上没有像c++ java那种明确求职路径，n多个八股  反而ai的项目，学习全要自己弄！ 而各公司都开始布局，提供岗位，但感觉各公司也没有非常好的落地，  也就是上升空间很大！ 
我现在不去，什么时候去？？？


