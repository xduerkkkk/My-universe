一个特斯拉知识引擎车书问答系统实战
# rag核心
1. 查询拓展query
2. 多路召回
	 - 字面上的召回，bm25召回
	 - dense召回， qwen3-embedding dense召回
	 - sparse召回， BGE-M3 sparse召回
	针对后俩个召回，做一个RRF粗排截断， 然后根据id文档去重
3. 去重后的文档，bge/qwen3 进行reranker模型精排，topk
4. topk后的进行问答，使用qwen3-8B
5. answer进行后处理
	- 引用了哪些文档？ 给出文档索引
	- 关联文档的插图 也就是图文混合输出
	- 讲所有东西结构化输出，送给前端展示（图文混排、超链接、高亮等）
其中，reranker模型，问答模型，经过微调。

# 数据收集，微调数据集形成
我们要从文档收集QA对，才能进行模型训练
1. 原始pdf文档，pyMuPDF解析出文本
2. 基于这些文本，进行清洗和规整。使用大模型初排，使用topic分栏
3. 基于清洗后的文档，使用deepseek抽取初始QA对，提示词工程约束
4. 继续用大模型，对上一步的QA对打分，交叉验证，过滤低质量QA对
5. query泛化，一条变多条，这个好理解，就是同一个句子不同语气语序等等
6. 数据量目前不错，扩充了，接着我们就进行训练集测试集切分。测试集500条，训练集几千到万条

# 拿到数据 如何训练
1. 用拿到的数据，过一遍rag系统。此时过的时候，是要把原本设计的模型替换成很大的商业模型。使用蒸馏思想
2. 替换的模型是这样，qwen3reranker-8B，llm用deepseekv3最新版
3. 过数据，我们是要保存中间结果的。召回的上下文、排序后的文档列表、问题答案、文本块引用
4. 我们要拿这些数据进行微调。
	- 我们对排序数据进行头部、中部、尾部采样，构造pointwise数据， 这些数据是reranker微调数据集
	- 我们对答案数据进行解析，构造精确的答案引用数据格式，使用精准的数据格式作为llm微调数据集。
# 测试数据的生产流程
1. 测试分片数据
2. 去除低质数据
3. deepseek抽取关键词，可以用关键词和语义相似度做综合的打分，为了不偏移真实情况，减小幻觉
4. 增加负样本，包括闲聊、百科、无意义、反正与主题无关的
5. 人工check+修正，把简单的样本去除，然后校准关键词
6. 拿到最终测试集
# reranker微调流程
我们有排序数据了
使用flagembedding框架进行微调
插一嘴
业界选择微调reranker模型，而不是embedding模型
通过微调后，进行模型训练和评估，进行hit@k 

# LLM微调
我们现在有QA数据了， 微调Qwen3-8B
我们使用llama-Factory微调，当然也可以peft
继续进行训练和评估
使用RAGas评估，
然后LoRA合并，导出
使用vLLM Serve进行微服务，模型本地化，接口化

# 文档切分和数据处理流程
1. 输入是pdf文档，经过PyMuPDF进行解析，我们得到的是
	- 图片、页码
	- 文本块
2. 我们要把图片和文本关联，方便后续多模态展示。
3. 文本清洗和规整
4. 传入Mongo数据库
5. 接着主题+语义切分。主题切分，这叫初切，切完如果小，（相比约定好的文本块）就保存，如果大，就继续采用语义。语义切分采用的是m3e，加上聚类
6. 上一步，语义块都是独立的，还是大，就采用二级的滑动窗口切分。我们还采用父子文本块关联，正在召回的时候，子文本被召回，父文本也要召回。 这样我们避免被切碎的文本，被召回时，丢失信息的状况
7. 分片做完后，向量化， 存到MilVus数据库