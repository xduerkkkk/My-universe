# **项目名称：** **面向智慧校园场景的多模态AI助教系统 **

**项目描述：**  
本项目旨在为西安电子科技大学学生打造一款智能AI助教，以解决校园生活中信息查询繁琐、服务入口分散的痛点。系统深度融合**Agent**思想与**小模型+大模型**协同决策架构，通过**分层式Function Calling**与**MCP(Media & Communication Processing)服务**，精准理解学生意图，高效完成**课程查询、服务办理、信息导航**等8大类、超过300项校园服务闭环，实现校园场景下的精准、高效、多轮流畅对话。

**工作内容：**

1. **分层意图识别 (Hierarchical Intent Recognition):** 针对校园服务**高频刚需**的特点，设计了“**Bert-large召回 + LLM精确匹配**”的分层意图识别框架。先用微调的Bert-large从300+校园服务API中快速召回Top5候选意图，再利用LLM的强大语义理解能力进行二次精排与槽位抽取，**解决了LLM直接处理海量工具库的幻觉和效率问题**，一级意图Top5准确率达97%，最终端到端匹配成功率达90%。
    
2. **上下文感知Query改写:** 为解决多轮对话中的指代消解与信息省略问题，利用**DeepSeek**对多轮历史进行摘要与改写。针对校园场景特有的**课程代码、地点简称**等进行Prompt优化，使跨领域（如图书馆借阅 -> 食堂菜品咨询）的综合改写准确率达到92%。
    
3. **服务执行与生成 (MCP & NLG):** 通过Function Calling的解析结果，精确调用校内**教务系统、图书馆系统、后勤报修**等多种内部服务API。利用MCP协议封装调用逻辑，并根据返回的结构化数据，生成符合学生口吻的自然、流畅回复。


**核心亮点/优化点:**

- **智能分流与资源优化:** 独立设计并微调了一个**三层Bert-tiny拒识模型**，用于过滤与校园服务无关的闲聊、噪音。该模型**QPS高达500+**，以不足1%的推理成本，**将90%+的无效流量拦截在LLM之前**，显著降低了整体运营成本与系统延迟。
    
- **歧义仲裁与路径规划:** 针对“查高数”这类高歧义指令（查成绩？查课表？查教材？），设计了一个基于规则与轻量级模型的**任务仲裁模块**。通过上下文和主动提问（Disambiguation）引导用户明确意图，将复杂任务**解耦**为清晰的子任务序列，有效解决了单一意图模型的局限性。


“这个项目源于我导师与学校**网络与信息技术中心**的一个**横向课题**，旨在提升校园信息化服务的智能化水平。我作为项目核心成员，发现学生的日常需求——比如查课表、查成绩、约自习室——入口分散，体验繁琐。

于是，我主动提出可以利用**大模型Agent**的能力，构建一个统一的AI助教入口。在获得导师和网信中心的支持后，我们拿到了部分核心服务的**沙箱环境API接口**（注意：强调沙箱，意味着安全、不涉及真实数据隐私）。

我的工作分为两个阶段：

**第一阶段 (V0):** 我利用大模型的零样本Function Calling能力，快速搭建了一个原型系统。然后，我们**组织了我们学院约50名同学，进行为期两周的邀请制内测**。这次内测为我们收集了**近5000条真实、高质量的用户交互日志**。

**第二阶段 (V1):** 基于V0收集到的宝贵数据，我进行了深度分析，并针对系统的性能瓶颈——即高频的拒识和意图识别任务——**训练了两个专用的轻量级模型**，最终构建了现在这个**‘小模型前端过滤+大模型后端兜底’的混合架构**，显著降低了系统的响应延迟和运行成本。”


# **项目名称：基于混合模型与Agent的桌面多任务AI语音助手**

**项目描述：**  
本项目旨在解决PC端日常操作繁琐、上下文切换成本高的痛点，为个人用户打造一款桌面AI语音助手。项目深度融合Agent架构与小模型+大模型协同决策，通过语音唤醒、分层意图识别及MCP技术，实现了对音乐、天气、导航等5大核心领域、超过30项生活技能的精准理解与自动化执行，打造了PC环境下高效、流畅的下一代语音交互体验。

**工作内容：**

1. 分层意图识别 :针对生活场景指令高频、多样的特点，设计了“Bert-large召回 + LLM精确匹配”的分层意图识别框架。先通过微调的Bert-large从30+项个人技能库中快速召回Top5候选意图，再利用LLM进行二次精排与槽位抽取，解决了LLM直接处理多工具时的效率与幻觉问题。一级意图Top5召回准确率达97%，最终端到端匹配成功率达90%**。
2. Query改写 : 为实现流畅的多轮语音交互，利用DeepSeek对对话历史进行摘要与指代消解。通过构建特定场景的Few-shot Prompt，优化了对省略、指代等口语化表达的改写效果，使多轮指令的综合成功率达到92%**。
3. 服务执行与响应生成 :基于Function Calling完成意图的精准匹配和关键槽位信息的抽取，通过MCP服务网关调用本地封装的Python工具（如音乐播放、天气查询API等）执行任务和NLG生成。

**核心亮点/优化点:**

- 智能分流与拒识: 针对桌面环境的噪音干扰，设计并微调了三层 Bert-tiny 作为拒识模型，以 90%+的准确率过滤无关咨询，具备毫秒级响应与高并发能力(QPS 达 500+)，显著降低了后端大模型的无效计算

- 任务仲裁与服务解耦:设计并实现了任务/闲聊双轨分流策略，通过提示词工程优化仲裁模型，精准切分用户意图，实现业务模块解耦;同时为闲聊链路引入联网搜索，解决了开放域对话中的时效性问题。



这个项目的灵感就来自于我日常使用小爱同学的体验，还有车载语音助手的体验。我发现它们在PC端是缺失的。于是，我决定为自己做一个**桌面端的语音生活助手**。


为了让我的系统架构更规范，我把拒识模型单独部署成了一个本地的API服务（比如用Flask或FastAPI）。这样做的好处是主程序和模型服务解耦，未来扩展性很强
所以我使用了像**Locust**或**JMeter**这样的压测工具，模拟了高并发的请求场景，对我的模型服务进行了压力测试。最终测得它的性能瓶颈大约在500 QPS，响应时间在毫秒级，这完全满足了实时语音助手的苛刻要求。这个测试也让我对模型部署和性能优化有了更深入的理解。

1. **完整的系统架构设计：** 包括拒识、仲裁、NLU、MCP等模块如何协作，以及数据如何在它们之间流动，我都画出了详细的架构图和接口定义。
    
2. **核心技术链路的验证：** 我搭建了一个最小化的原型，验证了最关键、风险最高的技术链路是可行的。具体来说，就是成功地用Python的语音识别库捕捉音频，转换成文本后，通过一个零样本的LLM Function Calling，成功调用到了我本地封装好的音乐播放和天气查询的Python工具。这证明了整个Agent的‘大脑’和‘手脚’是可以连接起来的。 



初步介绍

首先，我的灵感来自于车载和手机上的语音助手。我就在想，我也可以弄一个桌面上的，对于长期用电脑办公的，也可以随时呼叫桌面助手， 去放歌， 查天气，看论文等。  
我首先是用langchain框架验证，整个项目架构是agent嘛， 感知、规划、行动，这一流程。我用langchain进行最小化的原型验证， 就是语音识别库识别音频转文本，然后LLM Function Calling（还是说mcp？如果调用网易云api这种就算mcp，如果调用意图识别 识别到的function就叫functioncall？） 。  
验证可行性后， 我设计模块，拒识，仲裁，NLU，MCP，四大模块如何协作，数据如何流动，我画好了项目架构图， 接口定义。 并且打算接下来，不使用langchain框架，因为我要做混合模型， 框架有点死板。


第一是**成本与延迟**，所有请求都依赖大模型，响应慢且昂贵；第二是**鲁棒性**，它无法处理环境噪音，经常被错误唤醒
因此，项目进入了现在的**详细架构设计阶段**。为了解决上述瓶颈，我决定**‘回归第一性原理’**，设计一套**‘小模型+大模型’的混合架构**，并手写核心的调度逻辑来替代通用框架。具体来说，我设计了拒识、仲裁、NLU、MCP四大核心模块。比如，我会用一个微调的轻量级拒识模型来过滤90%以上的环境噪音。目前，我已经完成了**整个系统的架构图、核心模块的接口定义，以及下一步的开发和数据收集计划**

我的下一步，就是将这套设计付诸实践，开发出V0版本，并在我自己的日常使用中，开始收集第一批真实数据，以数据驱动的方式，去迭代和微调我的专属小模型。