车载上的语音助手
自己简述一遍：
首先是用户的输入，我们初步是文本，后续想做一下语音转文本
然后文本进入仲裁模型，判断是知识型还是技能型
知识型属于百科闲聊，
我们先经过拒识模型，过滤掉明显无意义的输入（如环境噪音、纯聊天、不构成问题的词语），避免浪费昂贵的LLM资源。
然后通过后，将Query交给**具备联网能力的大模型**（如豆包-pro、Kimi等）来处理，生成回答
技能型就是一种功能调用
如果存在多轮对话，首先进行Query改写，将历史信息融入当前Query，形成一个完整的、无歧义的指令。，接着我们调用nlu服务，NLU服务的**唯一目标**是：将用户的自然语言，**翻译**成一个机器可以执行的结构化指令，即 **{意图(Intent) + 槽位(Slots)}**。 调用mcpsever（是如MCP服务接收NLU生成的 **{意图, 槽位}**，根据意图调用**对应的后端服务或工具**（如播放音乐的API），并将槽位作为**参数**传给这个API。   然后完全让nlg生成回复。
具体的nlu模块呢， 首先因为是技能型的，我们可以进行一个意图召回，使用的小模型 Roberta
看看数据库有没有类似的技能。然后做个top5意图，如果低置信度就function calling，（置信度到底指什么？）置信度是softmax后的概率， 调用工具库，获取意图和槽位，NLU是填槽位， 然后规范化＋后处理
如果高置信度就直接输出意图＋槽位，规范化后处理


**我们从一个完全由LLM驱动的V0版本冷启动，其主要目的是为了收集真实的线上数据。在积累了第一批数据后，我们针对**‘拒识’**和**‘高频意图识别’**这两个最高频、最影响成本和体验的环节，分别微调了两个专用的轻量级模型。这样，我们的系统就从一个‘大一统’的LLM架构，演进成了一个**由小模型处理70-80%的简单高频流量，由大模型负责兜底复杂任务和开放式问答的、更具性价比和鲁棒性的混合架构。”

# 流程
用户输入音频，做个信号处理（语音增强、回声消除）
接着ASR，语音转文本
车机端调用云端服务（系统部署在云端）
首先通过仲裁模型 判断当前是技能型还是知识型
- 技能型：播放音乐、打开天窗、导航去xx
- 知识型：问问实时性的内容，聊天，问轮胎怎么换等等百科

技能型：
1. query改写，多轮对话改写！ 为了防止多轮对话丢失信息 要做prompt拼接
2. NLU模块  自然语音理解，完成意图识别（匹配技能库）、槽位匹配（天气、导航、媒体类都需要接收参数） 
3. MCP server，比如音乐类调用网易云第三方服务  
4. 内容+query+槽位 拿这些生成NLG回复，（类似于：好的，请稍等...已经为您找到... 请欣赏...） 

百科闲聊：
1. 拒识模型：过滤掉无效的指令，比如车上人与人的交流  无意义的话等
2. 实效性问答： 联网插件，流式播报

上述回复 进行前端展示并TTS

# 拒识模型
- 线上日志，大模型数据清理
若发现某类问题不多 或者某类拒绝的不太好 我们要进行第二支线
- 人工收集整理种子数据，deepseek泛化

再次人工check
接下来roberta_tiny_clue （高频度、快响应，小模型，大训练数据）
进行模型评估+准确率
接着导出模型，部署sever，压力测试


# 意图召回
仍然是 
- 线上日志，大模型数据清理
若发现某类问题不多 我们要进行第二支线
- 人工收集整理种子数据，deepseek泛化、增强

大模型初标，
再次人工check
接下来chinese_Roberta_WWM_EXT  稍微大一点330mb，是效率与准确率的折中
模型评估+Acc@k
导出模型，部署sever，压力测试

# NLU服务

1. 输入query，finetuned意图召回模型
2. Top5意图， 看置信度，超过某个阈值直接输出
3. 如果没有，使用豆包-pro（或其他模型）动态fuction calling
4. 调用工具库tools  
5. 意图+slots（槽位暴露出来）
6. 规范化+后处理（比如把空调调成3， 我们应该处理成”空调调成三“，包括其他一些数据转换等）
7. 调用mcp 拿到mcp就能做后面的rag生成

可以进行自己专业领域的适配，修改

我们这个项目没有使用LangChain这类高度封装的Agent框架，而是选择**手写了整个核心的调度和执行逻辑**。这样做的好处是，我们对系统的**每一处细节都有完全的控制力**，可以进行深度优化，比如我们为高频任务定制了小模型，这是框架很难实现的。这也让我对Agent的底层工作原理，即‘感知-规划-行动’的循环，有了更本质的理解