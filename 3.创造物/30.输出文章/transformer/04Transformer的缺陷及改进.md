我们回顾一下transformer的强大，

2017年，那篇论文横空出世，确立了**Encoder-Decoder**的标准架构。

- 送入一个句子，**Encoder** 负责输出“深度理解”（蕴含上下文语境的词向量）；
- **Decoder** 负责依照理解，输出“翻译”结果

随后，我们还衍生出俩种模型
- **Encoder-only** ，专门做极致理解的模型，如Bert
- **Decoder-only** ，专门用于生成文字的模型，如GPT、Gemini、DeepSeek。
其中，当我们提到“大语言模型（LLM）”时，指的几乎都是 **Decoder-Only** 架构。它也是最火、最融入我们生活的AI形态。

但是，不知道大家是否留意到，对于AI的未来，业界大佬们似乎都有着某种“焦虑”。

- 李飞飞说要往”世界模型、具身智能“转变，
- Yann LeCun认为当下LLM很不聪明，我们未来一定会找到”新范式“，
-  Transformer之父 Jones 也是语出惊人：”我受够了transformer，现在就像Transformer 出现前的那段时期，研究人员们无休止地调整循环神经网络（RNN）以获取微小的增量收益,是时候突破了"

我们不禁疑惑，

是什么，让Transformer没有在当今”继续发力“，没有继续像23年发布chatgpt一样继续以指数级速度引爆AI界？它的缺陷到底是什么？它的潜力被榨干了吗？

实际上，我们真的已经很大程度去改进他了，这几年来，各大模型，llama，gpt，deepseek，他们用的，虽然也是transformer架构，虽然也是decoder-only，但在细节之下，早已不是2017年那个原始的模样了。


接下来的文章，我们一起去探究Transformer架构中那些**最本质的缺陷**，以及目前业界最硬核的**架构级改进**。

我依旧会保持文章风格，通俗易懂，不陷入复杂的数学推导（严谨的数学证明留给专门的篇章）。

我会给大家，建立起”RoPE，Sparse attention, MoE，MQA/GQA/MLA,..... “，这些大模型圈子里高频名词的直觉理解。


# 位置编码的改进

还记得原始Transformer里的正弦位置编码吗？它是把位置信息直接**加**到词向量里的。
另外，我们是有专门的式子，讲了如何”计算“各个词、各个维度里具体填的数字，

- **偶数维度 (2i):** $PE(pos, 2i) = sin(\frac{pos} {10000^{2i/d_{model} } })$
- **奇数维度 (2i+1):** $PE(pos, 2i) = cos(\frac{pos} {10000^{2i/d_{model} } })$

不用细究公式，你只需要明白，每一行，都是一个**独一无二**的“位置向量”。
![](../../../assets1/image/Transformer的核心引擎：深入理解attention机制-1761297593964.jpeg)

这种硬性的，计算出的位置向量，我们称为”绝对位置向量“。

咦，那他有什么问题？
**外推性差：**
假设我们在训练时，喂给模型的最长句子是**2048**个词。这意味着，模型能看懂了 pos=0 到 pos=2047 的位置向量长什么样，模型可不懂公式，但他经过大量训练发现”哦，似乎0-2047的向量，每一个位置都加了一个固定的、不同的数字，似乎蕴含着“位置”这一信息“。

但当我们使用时，如果直接丢了一个长篇文字，4000个词（很常见的情况吧！） 模型会懵。
诶？不是有公式吗？根据正弦公式，我们完全算得出 pos=2048 的向量值，模型为什么会懵？  因为模型不会计算！ 模型内部的权重矩阵（Wq, Wk等）是根据它见过的0-2047这些位置的特征调整好的。突然来了一个从未见过的、高频和低频组合完全陌生的位置向量，模型就会“懵圈”，注意力机制会乱套，导致生成的内容完全不可读。

有一个h

