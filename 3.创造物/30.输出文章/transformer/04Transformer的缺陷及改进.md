我们回顾一下transformer的强大，

2017年，那篇论文横空出世，确立了**Encoder-Decoder**的标准架构。

- 送入一个句子，**Encoder** 负责输出“深度理解”（蕴含上下文语境的词向量）；
- **Decoder** 负责依照理解，输出“翻译”结果

随后，我们还衍生出俩种模型
- **Encoder-only** ，专门做极致理解的模型，如Bert
- **Decoder-only** ，专门用于生成文字的模型，如GPT、Gemini、DeepSeek。
其中，当我们提到“大语言模型（LLM）”时，指的几乎都是 **Decoder-Only** 架构。它也是最火、最融入我们生活的AI形态。

但是，不知道大家是否留意到，对于AI的未来，业界大佬们似乎都有着某种“焦虑”。

- 李飞飞说要往”世界模型、具身智能“转变，
- Yann LeCun认为当下LLM很不聪明，我们未来一定会找到”新范式“，
-  Transformer之父 Jones 也是语出惊人：”我受够了transformer，现在就像Transformer 出现前的那段时期，研究人员们无休止地调整循环神经网络（RNN）以获取微小的增量收益,是时候突破了"

我们不禁疑惑，

是什么，让Transformer没有在当今”继续发力“，没有继续像23年发布chatgpt一样继续以指数级速度引爆AI界？它的缺陷到底是什么？它的潜力被榨干了吗？

实际上，我们真的已经很大程度去改进他了，这几年来，各大模型，llama，gpt，deepseek，他们用的，虽然也是transformer架构，虽然也是decoder-only，但在细节之下，早已不是2017年那个原始的模样了。


接下来的文章，我们一起去探究Transformer架构中那些**最本质的缺陷**，以及目前业界最硬核的**架构级改进**。

我依旧会保持文章风格，通俗易懂，不陷入复杂的数学推导（严谨的数学证明留给专门的篇章）。

我会给大家，建立起”RoPE，Sparse attention, MoE，MQA/GQA/MLA,..... “，这些大模型圈子里高频名词的直觉理解。


# 位置编码的改进


## 最初始的位置编码
还记得原始Transformer里的正弦位置编码吗？它是把位置信息直接**加**到词向量里的。
另外，我们是有专门的式子，讲了如何”计算“各个词、各个维度里具体填的数字，

我们不需要再回顾复杂的公式，你只需要记住它最核心的操作逻辑：**它计算出一个代表位置的向量，然后直接“加”到了词向量里。**
![](../../../assets1/image/Transformer的核心引擎：深入理解attention机制-1761297593964.jpeg)

这种硬性的，计算出的位置向量，我们称为”**绝对位置向量“**。

咦，那他有什么问题？
### **外推性差：**
假设我们在训练时，喂给模型的最长句子是**2048**个词。这意味着，模型能看懂pos=0 到 pos=2047 这 2048 个位置向量长什么样。他经过大量训练发现”哦，似乎0-2047的向量，每一个位置都加了一个固定的、不同的数字，似乎蕴含着“位置”这一信息“。

当我们使用时，如果直接丢了一个长篇文字，4000个词（很常见的情况吧！） 模型会懵。
诶？不是有公式吗？根据正弦公式，我们完全算得出 pos=2048 的向量值，模型为什么会懵？

**但模型并没有学会那个正弦公式！**

 模型内部的权重矩阵（Wq, Wk等）是根据它见过的0-2047这些位置的特征调整好的。突然来了一个从未见过的、高频和低频组合完全陌生的位置向量，模型就会“懵”，注意力机制会乱套，导致生成的内容完全不可读。

有一个很有趣的例子，模型到底懂不懂1+1等于多少？  其实可以说，模型是不懂的。模型只是通过大量的训练文本，发现了这个规律。

同理，模型的训练文本只有0-2047这些加了位置编码的向量的话， 模型只能学到这些规律，他学不到那个硬性公式，然后自己推理。

### **语义干扰与距离感缺失**：

**首先，是“加法”带来的污染。**  
公式是 最终向量 = 语义向量 + 位置向量。
这个位置向量**直接加**到语义向量的操作，把语义和位置搅和到一块了，
需要模型需要训练很久，才能学会把这两层信息“解耦”开来。


**其次，是“绝对坐标”带来的缺陷**

比如理解“**非常**”修饰“**好**”：

- **绝对位置**：“非常”在第1005个位置，“好”在第1006个位置
- **相对距离**：不管它们在哪，只要“非常”在“好”的**前一位**，它们就有强烈的修饰关系。

显然，**相对距离才是语义的关键。**
Transformer原论文声称做到了。因为数学上，他们设计的公式可以让 $PE(pos+k)$ 被 $PE(pos)$ 线性表示。
**但在实践中，效果并不好。**

模型必须花费额外的“脑力”去从这个绝对坐标中，反推算出““非常”在第1005个词，“好”在第1006个词,所以他们差1个词，离的近”。它不是天然对“距离”敏感的。

什么是”**天然**“，就是拿到一个”非常“，一个”好“，即使“非常”在203个词，“好”再第204个词，模型也可以不管他们的具体位置，直接感知到“哦，他们是差一个词的，离的近”。而不需要去关心那个庞大的绝对坐标数字。



故而，
大家想要这样一个完美的位置编码：

1. **相对性**：模型只关心词与词之间的距离
2. **外推性**：训练时只看了2k长度，推理时给我8k、16k，我也能照样读懂！
3. **不污染语义**：最好不要直接做加法

**于是，RoPE（旋转位置编码）应运而生。** 它是目前最完美的答案。
它是目前Llama、DeepSeek等主流大模型的标配，

## RoPE
