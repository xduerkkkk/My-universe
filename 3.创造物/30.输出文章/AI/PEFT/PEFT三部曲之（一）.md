---
share_link: https://share.note.sx/gnfmrat7#fXKvwfk5IUZfeUtOF7SyfA/gqiUDAtIXD1P/EFHxW6U
share_updated: 2025-09-27T12:23:28+08:00
---

# **“训练”一个专家，为何如此之难？**
如果把模型比作一个人
我们的工程，需要什么样的人呢？ 法律领域需要法律专业出身的，金融领域需要金融专业出身的。
模型的预训练呢，就是把模型从什么都不会说的婴儿，变成经历过九年义务教育的大学生。
大学生是有基础知识的，但是没有专业知识，因为初入大学，专业理论并不扎实。

本篇文章，专注于如何让模型从一个懵懂大学生，到一个专业领域的人才，PEFT。
也叫低成本、高效率地适应我们的下游任务
可能需要大学生会交流了再教专业知识，也可以不会交流也直觉教，取决于专业特质。

最简单的方法，直觉能想到的，就是给他看书啊，上课啊
我们把领域的大量语料，直接喂给大模型，进行监督训练。
这种看似直观的方法，就是我们常说的**全量微调（Full Fine-Tuning）**。然而，这种‘简单粗暴’的策略背后，隐藏着三座难以逾越的大山...：
- 算力成本高：我们面对的是，海量的参数，全部训练一遍！
- 存储成本高：那微调的新的模型，就好比一个完全训练的新模型，于原来模型一样重的权重
- 灾难性遗忘：比如模型要学法律性，假如样本还挺少的，模型为了学的好，会出动非常多参数去保证他能说出法律专业术语，在这个过程中，可能把模型的通识知识抹掉了。
全量微调这么多弊端，我们有什么好办法呢？
那我们不全量就好了啊！所以PEFT参数高效微调，的高效，就是少量的参数，达到接近全量微调的效果。 
具体都有哪些方法呢？

# **PEFT的两大流派：“魔改输入”与“模型插件”**

对于PEFT方法，我分为两种
- 一种叫提示型，核心思想是冻结模型本体 ，魔改输入。
我们只需要将输入引导模型，好像就是教了模型“精灵语”，比如“民法典中xxx” 经过提示式微调的模型，会把这句话重新翻译成，“%&@）...“ （某种神秘的语言）。   这串‘精灵语’在技术上，其实是一组**可学习的、连续的向量**。它不像我们输入的‘民法典’三个字有明确含义，但模型理解这个被精灵语加工后的神秘指令，能比直接理解“民法典xxx”**更能领会其深层意图**。经过这个翻译，模型能给出非常契合我们本意的答案。
- 一种叫旁路型，原有模型上“加装插件”，冻结模型本体，但在其中插入了一些小型的、可训练的模块。我们只训练这些“插件”。
这个比较符合直觉。我们会把‘插件’训练出的效能，直接融入（或合并）到原始模型中，以极小的代价实现对模型行为的调整。

读到这里，相信你一定很好奇”精灵语到底是怎么回事？“，”旁路式为什么小参数大奇迹？“
下面我们来揭开他们的面纱

##  **第一式：提示微调——给模型念“咒语”**
### **Prompt Tuning (软提示-前缀)**:
本方法一句话就是，

> 在prompt前拼接离散的k个token，只训练这些虚拟向量


那么在这里，我们详细讲一下为什么“虚拟向量”这个咒语能起作用，
关键在于，大模型的思考空间，是在高维度空间，它有很多知识都已储存。用比喻来说，模型空间已经有一个广袤的地形图，里面有各个蕴含知识的区域。 但真正激活某个特定领域的知识，只有很少一部分向量，或者说区域。
因此，俩种路径的区别就很显然：
全量微调，是输入不变，但调整高维空间，让区域去靠近我们的输入，即重塑整个地形，从而让知识与输入对应。这就导致了成本高，还可能破坏地形图的其他部分（灾难性遗忘）
而我们的提示微调，则是引导输入，去靠近拥有知识的区域。地图不改变

训练好就不变了，比如我们是微调法律领域
接下来用户使用大模型，输入法律领域的prompt时，这个拼接在prompt的向量就好像激活了一般，进入模型，模型就识别到了“哦这是法律领域的问题”

具体做法：
我们不再触碰模型庞大的权重，而是创建一个小型的、可训练的“咒语手册”（一个包含k个向量的Embedding层）。在每次前向传播时：
- 将用户的输入文本正常通过模型的input_embedding层，得到一个形状为` [batch_size, seq_len, hidden_dim]` 的输入向量。
- 我们从“咒语手册”中取出全部的k个“咒语”向量` [k, hidden_dim] `
- 将这串“咒语”向量拼接到输入向量的前面，形成一个新的、更长的序列 `[batch_size, k + seq_len, hidden_dim]`。
- 将这个拼接后的序列送入模型进行计算


这k个虚拟Token在**优化空间**中是**相互独立**的点。它们之间没有平滑的内在联系。它们在优化空间中被独立地调整，彼此之前没有显式的关联，最终形成一组最优的“激活码”。**这组由模型自己“学会”的、人类无法直接解读的向量序列，便是我们所说的“精灵语”**


### **P-Tuning (v1)**:
 和Prompt Tuning非常相似，也是学习虚拟的Token，也是拼接在prompt
 但是，Prompt Tuning的“咒语”是k个相互独立的向量，好比k个独立的音符。P-Tuning v1的作者认为，一个好的“咒语”应该更像一段**连贯的旋律**，音符之间应当有关联
为此，他们引入了一个小型的“咒语生成器”，通常由一个BiLSTM网络构成。这个生成器的作用，就是根据一些初始输入，生成k个**内部状态连续、相互关联**的虚拟Token。这样做的好处是，通过训练这个小小的“咒语生成器”，而不是直接训练k个独立的向量，理论上可以得到更平滑、更稳定的“咒语”，提升了训练的稳定性和最终效果
- 输入一些“占位符”token到这个Prompt Encoder里，然后由它输出最终的 k 个虚拟token向量。
- 和Prompt Tuning一样，将生成的“咒语”向量拼接到真实输入的embedding前面，送入大模型。在训练时，只更新这个小小的Prompt Encoder的参数。
这样，**我们的优化目标从一组离散的向量参数，变成了一个结构化的、小而完整的神经网络，使得学习过程可能更加稳定和高效。**

### **P-Tuning v2 (更重要)**:

如果只是拼接prompt，有什么局限？
模型很长！距prompt的位置，到最终输出的位置，经典的transformer，有数十层之深模型层层传递都把拼接的信息传递没了，
于是说，我们把这个虚拟token每一层都注入，不就保证每一层都能有精灵语注入吗？
但，这是残差一样的注入吗？就是每一层注入的token应该都一样？还是？
- **每一层注入的token是不同的！** 我们会为模型的每一层都创建一组**独立可学习**的虚拟token。
我们的做法是：
1. 创建一个可训练的参数张量，形状为` [num_layers, k, hidden_dim]`。

2. 在模型进行前向传播时，对于第 L 层，我们取出` [L, :, :]` 这个切片，得到属于第 L 层的 k 个虚拟token。

3. 将这 k 个虚拟token，拼接到从第 L-1 层传过来的真实token的隐藏状态序列的前面。

4. 这个拼接后的更长的序列，再送入第 L 层的自注意力模块和FFN中进行计算。

区别也很简单，就是prompt-tuing和p-tuing v1的精灵语，形状是`[k, hidden_dim]` 而P-Tuing v2是 ` [num_layers, k, hidden_dim]`。
我们每一层都会进行一次拼接，也就是每一层都拓展了序列长度，从而确保“精灵语”的指令信号，在模型的整个计算流程中都**保持着清晰且强大的影响力**


### Prefix-tuning

以上三种提示，都是给prompt加虚拟向量，魔改prompt
通过“魔改”完整的输入序列（Hidden States）来施加影响

Prefix-tuning是，我们直接动刀模型深层的，注意力机制，会比光改输入更高效。
但仍记得，我们是“提示型”微调，也就是同样注入精灵语
那这种方法是在哪注入呢？
我们在计算attention的QKV矩阵时，只为K和V矩阵各自拼接上一段可学习的前缀向量，也就是，“精灵语”作用的地方不在输入，而在注意力机制，提醒模型“你更要注意哪里” ，这种作用更高效直接
具体训练呢，就单独训练这个向量，其他冻结

- **和P-Tuning v2的细微但重要的区别**：
    
    - P-Tuning v2是把虚拟token和**真实的token隐藏状态**拼接，然后这个拼接后的整体再去计算Q, K, V。
    - Prefix-Tuning是先让**真实的token隐藏状态**算出自己的Q, K, V，然后拿出**另外**学习到的“前缀向量”，直接拼接到K和V序列的前面。
        
这带来了什么效果？这意味着，真实的token（通过它们的Q）可以attend to（关注）这些虚拟的前缀token（通过它们的K和V），但虚拟的前缀token之间以及它们自身不会进行自注意力计算。这是一种更直接地干预注意力计算过程的方式。



### 提示型总结


| 方法                | “动刀”位置                  | 核心机制                      | 特点                             |
| ----------------- | ----------------------- | ------------------------- | ------------------------------ |
| **Prompt Tuning** | 输入层 (Embedding)         | 拼接**独立**的虚拟Token          | 最简单，参数量最少，但有时不稳定               |
| **P-Tuning v1**   | 输入层 (Embedding)         | 用LSTM生成**连续**的虚拟Token再拼接  | 试图让提示更平滑，但更复杂，已被v2取代           |
| **P-Tuning v2**   | **每一层** (Hidden States) | 在**每一层**都拼接**不同**的虚拟Token | 控制力强，效果稳定，是目前的主流方法之一           |
| **Prefix-Tuning** | **每一层** (Attention K,V) | 在**每一层**为K和V矩阵拼接**前缀向量**  | 直接干预注意力计算，效果好，与P-Tuning v2并驾齐驱 |


##  **第二式：旁路微调——为模型装“插件”**

### Adapter：
与在模型入口处做文章的“提示微调”不同，旁路微tools选择了一条更深入的路径：**在Transformer的每一个层级中，加装“插件”**。它的核心思想是：冻结模型庞大的原始权重，以保留其强大的通用知识。只训练这些轻量级的、即插即用的Adapter模块，让它们专门学习和注入下游任务所需的领域知识。
Adapter的设计充满了工程的智慧。
**其一在于它的安装位置**
- 在每一层transformer的layer中，插入俩个
- 在两次layernorm之前 分别插一个插件
- 注意在与原始输入的**主残差连接相加之前**
为什么插在这样的位置？ 可以理解为， 之前那个通用的结果FFN(x) ，我们再经过Adapter这个专家的修正，优化。Adapter所学习的，**永远是基于模型已有通用能力之上的一个“增量”或“修正量”** 符合“插件”思想。
**其二在于它的内部结构**
这是一个沙漏形状的维度变换，关键就在与沙漏那个窄的，流通俩个宽空间的地
- down-proj ---激活函数---up-proj
- 经典的降维，激活函数，升维
结构的含义为：
- 首先提炼信息，将高维压缩到低维，强迫模型必须学习将任务相关的知识高度浓缩到一个低维子空间
- 激活函数，注入复杂的处理能力
- 升维，将精炼的任务信息重新拓展回原始维度，为下一步的融合做准备

请注意，Adapter模块是‘插入’到原有结构中的，原始的Attention和FFN模块的权重**完全冻结**。
作者实验得出，我们就训练这么一个小插件， 微调效果也会非常好
 
![[PEFT三部曲之（一）-1758334789440.jpeg]]
### LoRA
Adapter已经证明了，训练小参数，将其影响加在原架构上，也能得到很好的效果，
但是，虽然adapter模块很小，但它终究是一次额外的、**串行**的计算。在一个拥有几十层深度的模型里，这几十次（甚至上百次，因为每层有两个Adapter）的额外计算累加起来，就会使得**模型生成每一个token的速度变慢**。
我们能否在**不改变模型原有计算路径、不增加模型深度**的前提下，实现类似的效果？

答案就是，设计一个并行的，能附加的小插件。
那就是，训练出的参数，能不能直接加到原有的参数矩阵上？ 这样就是一个不改变模型结构的想法。
当然！无非是让W 变成 W + ΔW。我们训练的，正是这个变化量 ΔW。

LoRA基于“预训练模型的低内在维度”这一核心假设，做出了一个大胆的猜想：**这个巨大的变化矩阵 ΔW，其本身是“低秩”的。** 也就是说，它的大部分信息可以用两个更小的矩阵来表示。
**因此，微调的关键，就在于精准地找出并学习这个蕴含核心知识的低维子空间。**

综上两点，lora设计的公式为
W_new = W + B @ A   其中，A 是一个降维矩阵（如 1000 x 2），B 是一个升维矩阵（如 2 x 1000）
- 加法，意味着学习到的旁路矩阵 A 和 B 可以被**直接合并**回原始的权重矩阵中
- 将 ΔW 矩阵分解为B@A，举个例子，就好比1000 乘 1000  转换为 1000乘2   @ 2乘1000  原本1000000的参数，变成了4000的参数，  在“2”这个低维里，让模型去理解知识




### lora的优势之处

在以上方法中，lora是使用最多，最普遍，最有效的方法
对比Adaptor微调，Adaptor其实是改了模型架构，增加了模型深度，势必增加了时间
Prefix微调呢，在模型流动层的kvcache加了虚拟token，相当于增加了序列长度，即增加了时间复杂度和空间复杂度。
而LoRA所谓的“旁路”，就是说计算和原权重是并行的，只是反向传播调整参数而已。
所以没有增加任何复杂度。
在推理时，更是直接和原架构没有任何区别。
**模型结构没有任何变化，计算路径没有任何增加，因此，LoRA真正实现了“零额外推理延迟”**

所以lora很棒，很有优势。
有没有对lora的具体实现细节感兴趣呢？ 以及，还有比lora更强大的QLoRA，AdaLoRA！ 他们会把高效推到极致。
？在下一篇文章中，我们将深入探讨LoRA细节，以及进阶变体——AdaLoRA与QLoRA。
