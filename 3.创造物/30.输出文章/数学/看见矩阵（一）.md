矩阵，线性代数里非常常见的元素。

在大多数人的印象里，它似乎只是一张枯燥的、由数字排列而成的方方正正的表格。如果不幸通过应试教育去认识它，它更像是一个用来进行繁琐加减乘除的“计算容器”。“哦，他作用于一堆数字，然后通过繁杂的公式，得出另一堆数字”......

但在我看来，**矩阵不应当仅仅是枯燥的数字表格。**


我们需要理解矩阵的“几何直觉”。明白他是“空间变换”和“信息处理”的语言，我们才能更好应用他。无论是在人工智能领域如鱼得水地应用矩阵，还是面对”考研“”期末“，不想那么痛苦的应试。去建立直觉，永远，是一个好方向。


# 矩阵乘法：为了“作用”而生
矩阵不能凭空出现，孤零零地呆在那。

如果我们还是用“应试”的角度，去看矩阵，矩阵确实可以孤零零的。 因为它就是一堆毫无意义的数字。

然而，矩阵从诞生的那一刻起，就有个使命，“作用”于他人的。

这个“作用”，在数学上，正是矩阵乘法。

我们可以这样理解：**矩阵一旦出现，就是为了去“乘”某个对象的。** 它是一个动词，是一个机器，而不是一个名词。

那么矩阵乘法到底蕴含着什么直觉呢？

忘掉那背的滚瓜烂熟的“行乘列求和”公式。

从现在开始，我会展示矩阵乘法，但请不要套用公式，去验证它。

我们要重新建立对它的直觉。

## 场景一：矩阵与向量的初次相遇

让我们从最基础的场景开始：一个矩阵 $A$ 想要“作用”于一个向量 $x$。我们默认，“作用”，是将矩阵，放到向量的“左边”！！！去作用的。位置不能错。

$$
\text{Output} = \text{Matrix} \cdot \text{Input}
$$

我们设定：
*   **变换机器 $A$**：一个 $2 \times 2$ 的矩阵。
*   **输入对象 $x$**：一个 $2 \times 1$ 的列向量。
举个例子：
$$
\begin{bmatrix}
1 & 3 \\
2 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
y
\end{bmatrix}
= \ ?
$$

A，是个机器，我们不用想它的物理含义！ 目前只知道，它用于变换别人。

在按下“计算”按钮之前，我们先好好看看这个**输入对象**。
我们可以理解为熟知的二维坐标（x，y），高中知识告诉我们，
可以表示为，以原点为起点，终点为（x，y） 箭头方向从原点到终点的向量。

![](../../../assets/image/看见矩阵（一）-1764838028818.jpeg)


`图：《矩阵力量》 https://github.com/Visualize-ML/Book4_Power-of-Matrix `


高中知识还告诉我们，二维向量 $\begin{bmatrix} x \\ y \end{bmatrix}$ 其实是两个**基向量**的组合：
*   $\hat{i}$ ：横轴单位向量 $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$
*   $\hat{j}$ ：纵轴单位向量 $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$

显然，他们的长度`|i|、|j|`都是1。
向量 $\begin{bmatrix} x \\ y \end{bmatrix}$ 的本质含义是：**“沿着 $\hat{i}$ 方向走 $x倍的|i|$ 步，再沿着 $\hat{j}$ 方向走 $y倍的|j|$ 步”。** 或者直接说，x倍的i于y倍的j做向量加法。

写成数学式子就是：
$$
\text{Input} = x \cdot \hat{i} + y \cdot \hat{j}
$$
如图，向量v，可以拆分成$\hat{i} , \hat{j}$的组合，本例中为：$1\cdot \hat{i} + 2 \cdot \hat{j}$

![](../../../assets/image/看见矩阵（一）-1764852828496.jpeg)


### 列视角

现在，矩阵 $A$ 开始发挥作用了。这个机器到底做了什么？
**列视角告诉我们要盯着矩阵的“列”看：**

*   **矩阵的第一列** $\begin{bmatrix} 1 \\ 2 \end{bmatrix}$：这是 $\hat{i}$ 变换后的**新位置**（New $\hat{i}$）。
*   **矩阵的第二列** $\begin{bmatrix} 3 \\ 0 \end{bmatrix}$：这是 $\hat{j}$ 变换后的**新位置**（New $\hat{j}$）。


随着基向量i和j的改变，整个坐标轴，我们都可以理解为变了。


既然原来的向量是 “$x$ 份的 $\hat{i}$ 加上 $y$ 份的 $\hat{j}$”，
那么变换后的向量，必然就是 **“$x$ 份的** **新 $\hat{i}$** **加上 $y$ 份的** **新 $\hat{j}$”**

带来的结果是，空间里千千万万个向量，都变成了新向量。同时，网格线始终保持平行且等距，原点保持不动。

这就叫”线性变换“  。

我们不需要关心空间里千千万万个向量各自去了哪里，我们只需要追踪这两个基向量，整个空间的就被确定了。


原来的坐标轴或许已经被拉伸、旋转，不再是标准的十字架，但我们的**路径法则**依然有效：

$$
\text{Output} = x \cdot (\text{New } \hat{i}) + y \cdot (\text{New } \hat{j})
$$

把它代入我们具体的矩阵 $A$：

$$
\begin{bmatrix}
1 & 3 \\
2 & 0
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
x \cdot \underbrace{\begin{bmatrix} 1 \\ 2 \end{bmatrix}}_{\text{Col 1}}
+ 
y \cdot \underbrace{\begin{bmatrix} 3 \\ 0 \end{bmatrix}}_{\text{Col 2}}
=
\begin{bmatrix}
1x + 3y \\
2x + 0y
\end{bmatrix}
$$


![](../../../assets/image/看见矩阵（一）-1764852937643.jpeg)
如图，我们的原始向量v是$1\cdot \hat{i} + 2 \cdot \hat{j}$
现在只不过变成了$1\cdot (\text{New } \hat{i}) + 2 \cdot (\text{New } \hat{j})$。

当然，实际上，可以理解为整个空间坐标轴都因为新的基向量，而改变
![](../../../assets/image/看见矩阵（一）-1764853133323.jpeg)




**这就是列视角：**

**矩阵列向量**，看做$\text{New } \hat{i},\text{New } \hat{j}$
矩阵与向量的乘法，本质上是在说，基向量变换后的空间，向量 $\begin{bmatrix}x \\y\end{bmatrix}$去哪了，去到了output
$\text{Output} = x \cdot (\text{New } \hat{i}) + y \cdot (\text{New } \hat{j})$ 

也可以看作，结果是**矩阵列向量的线性组合**。输入向量的坐标 $(x, y)$，其实就是分配给这些列向量的**权重**。


ok  这样一来，矩阵的形状（几行几列）就不再是死记硬背的规则，而是逻辑的必然。

我们能用一个 **$2 \times 3$**（2行3列）的矩阵，去变换一个 **$2 \times 1$** 的向量 $\begin{bmatrix}x \\y\end{bmatrix}$ 吗？
 
不能，这个向量，是二维的，表明，只有俩个基向量组成他们。$x$ 和 $y$。

那么$\text{New } \hat{i},\text{New } \hat{j}$ 也一定是俩个，

则矩阵，一定是，俩列！！！

从刚才线性加权角度思考，
$2 \times 3$ 的矩阵有 **3列**（3个基向量）。
我们有 3 个基向量等待被缩放，却只来了 2 个权重指令。第 3 列基向量就问了“谁来乘我？”


故而，**矩阵的列数，必须等于输入向量的维度数（行数）。**





### 批量处理
假设我们不是处理一个向量，而是同时处理两个向量 $\vec{v_1} = \begin{bmatrix}x_1 \\y_1\end{bmatrix}$ 和 $\vec{v_2} = \begin{bmatrix}x_2 \\y_2\end{bmatrix}$。
我们可以把它们拼起来，变成一个 $2 \times 2$ 的矩阵 $B = [\vec{v_1}, \vec{v_2}]$。

$$
A \cdot \begin{bmatrix} \vec{v_1} & \vec{v_2} \end{bmatrix} = \begin{bmatrix} A\vec{v_1} & A\vec{v_2} \end{bmatrix}
$$


这只是**批量处理！**
矩阵 $A$ 并没有发生什么神奇的变化，它只是勤勤恳恳地、独立地对 $B$ 中的**每一列**分别进行了变换，然后把结果并排摆放。


*   **输入：** 2个向量。
*   **输出：** 2个变换后的向量。


### 维度的含义
可能又有人问，用刚才这个矩阵A，作用于横着写的向量 $[x, y]$行不行？

有的人很容易把这个向量，也立即为二维的。

可是按照上面那个“批量处理”的法则，实际上，

这应该是，**一维的，数轴上的，俩个独立的点，拼接的向量！**

所以不可以。

- 行数（一共多少行）代表维数！  
- 列数（一行有多少个）说明了向量的个数。



### 维度的跃迁

现在来到最精彩的部分。

ok  那我们这样，

如果矩阵是 **$3 \times 2$**（3行2列）的，它可以作用于二维向量 $\begin{bmatrix}x \\y\end{bmatrix}$ 吗？

**检查规则：**
*   输入向量有 2 个权重（$x, y$）。
*   矩阵有 2 列。
*   **没问题！**

但是，这一次发生了质的变化。让我们看看这个矩阵长什么样：

$$
A = \begin{bmatrix}
a_1 & b_1 \\
a_2 & b_2 \\
a_3 & b_3
\end{bmatrix}
$$

*   **第一列（New $\hat{i}$）：** $\begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix}$。这是一个 **三维空间** 中的向量！
*   **第二列（New $\hat{j}$）：** $\begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}$。这也是一个 **三维空间** 中的向量！

**运算过程：**
$$
\text{Output} = x \cdot \underbrace{\begin{bmatrix} \text{3D Vector} \end{bmatrix}}_{\text{New } \hat{i}} + y \cdot \underbrace{\begin{bmatrix} \text{3D Vector} \end{bmatrix}}_{\text{New } \hat{j}} = \begin{bmatrix} \text{New 3D Vector} \end{bmatrix}
$$

![](../../../assets/image/看见矩阵（一）-1764853657714.jpeg)

如图，

从矩阵形状来说，确实，规定了仅有的俩个基向量（图中蓝色、红色向量）的新去处，即$\text{New } \hat{i},\text{New } \hat{j}$ 

但！

这个新地方，不在原来$\begin{bmatrix}x \\y\end{bmatrix}$ （如图中绿色向量）所生活的二维平面的世界了， 而是去了三维世界！

*   **矩阵的列数（Columns）** = **输入空间**的维度（因为列数就是原来空间的基向量个数）。
*   **矩阵的行数（Rows）** = **输出空间**的维度（把你送去几维世界？）。


读者！你现在可以自己想象，一个$1 \times 2$的向量，当作矩阵，变换的机器，作用于$2 \times 2$ 的拼接向量，当然我们也称为矩阵。
## 维度的“降维”

现在，让我们把思维逆转过来。
如果我们的矩阵A，这个变换的**机器**变得非常薄，只有一行，会发生什么？

设想我们的机器 $A$ 是一个 **$1 \times 2$** 的矩阵：
$$
A = \begin{bmatrix} 1 & 2 \end{bmatrix}
$$

与我们之前建立的“**列视角**”依然坚不可摧，逻辑完全一致：

*   **看第一列 $[1]$：** 这意味着原来的横向基向量 $\hat{i}$，在这个一维新世界里，变成了数字 **1**。
*   **看第二列 $[2]$：** 这意味着原来的纵向基向量 $\hat{j}$，在这个一维新世界里，变成了数字 **2**。

**变换过程：**
输入向量 $\begin{bmatrix} x \\ y \end{bmatrix}$ 本质上是 $x$ 个 $\hat{i}$ 加上 $y$ 个 $\hat{j}$。
既然 $\hat{i}$ 变成了 $1$，$\hat{j}$ 变成了 $2$，那么结果自然就是：

$$
\text{Output} = x \cdot (1) + y \cdot (2) = x + 2y
$$




我们的输入对象 $B$ 是一个 **$2 \times 2$** 的矩阵（代表两个二维向量）：
$$
B = \begin{bmatrix} 3 & 0 \\ 1 & 4 \end{bmatrix}
$$


让我们看看 $A$ 是如何“作用”于 $B$ 的：

$$
\underbrace{\begin{bmatrix} 1 & 2 \end{bmatrix}}_{\text{1D Machine}} \cdot \underbrace{\begin{bmatrix} 3 & 0 \\ 1 & 4 \end{bmatrix}}_{\text{2D Data}} = \begin{bmatrix} (1\cdot3 + 2\cdot1) & (1\cdot0 + 2\cdot4) \end{bmatrix} = \begin{bmatrix} 5 & 8 \end{bmatrix}
$$

**发生了什么？**
1.  **输入：** 两个生活在二维平面上的向量 $\begin{bmatrix} 3 \\ 1 \end{bmatrix}$ 和 $\begin{bmatrix} 0 \\ 4 \end{bmatrix}$。
2.  **输出：** 两个躺在数轴上的数字 $5$ 和 $8$。


这是三体里的“**二向箔**”攻击啊！
*   $1 \times 2$ 的矩阵代表了一个**投影动作**。
*   它把二维空间中的所有物体，强行压缩（投影）到了一根数轴上。
*   虽然输入有 $x$ 和 $y$ 两个维度的信息，但经过这个机器的降维，最后只剩下一个维度的标量。
# 行视角

在纯几何变换（移动向量）的世界里，列视角确实够了。
但一旦涉及到**数据处理、方程求解、AI特征提取**，行视角就是不可或缺的上帝视角。
     
假设我们要计算 $C = A \cdot B$。
**列视角，会如何看待这个过程**？
如”批量处理“那一小节， $B$ 实际上是看作一列一列的向量 ($\vec{b_1}, \vec{b_2}, ...$)拼起来，等待A的作用。
$A$ 是什么？一个**空间的变换机器**。它勤勤恳恳地分别把 $\vec{b_1}$ 扭曲成了 $\vec{c_1}$，把 $\vec{b_2}$ 扭曲成了 $\vec{c_2}$...
最后得到 $C = [A\vec{b_1} \ | \ A\vec{b_2} \ | \ ... ]$


**行视角 ，会如何看待这个过程**

**$B$ 是仍然是等待作用的原材料**（每一行是同一种原料）， **$A$ 仍然是作用在B上**。
但！我们要把 $A$ 切成一行一行的向量 ($\vec{r_1}, \vec{r_2}, ...$)。
$C$ 的每一行，都是 $A$ 的那一行（指令）对 $B$ 的所有行（原料）进行的**加权混合**
*   **公式：**
    $$
    C = \begin{bmatrix}
    \text{Row}_1(A) \cdot B \\
    \text{Row}_2(A) \cdot B \\
    ...
    \end{bmatrix}
    $$



让我们用行视角重新审视那个 **维度跃迁 ($3 \times 2$)** 的例子。

$$
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
1 & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
\begin{bmatrix}
x \\
y \\
x+y
\end{bmatrix}
$$

**我们之前讲的列视角：**
我们在 3D 空间里组合两个基向量


**现在来看看行视角：**
我们将矩阵看作 3 行独立的“指令”：

1.  **第一行 $[1, 0]$：**
	 “只保留第一个分量，忽略第二个。”
    *   **结果：** $x$。 提取了 x 轴的信息。

2.  **第二行 $[0, 1]$：**
     “忽略第一个分量，只保留第二个。”
    *   **结果：** $y$。 提取了 y 轴的信息。

3.  **第三行 $[1, 1]$：**
	 “把两个分量等比例混合。”
    *   **结果：** $x + y$。**创造了一个新的特征**

这三行指令，提取的数据结果，纵向拼接，形成最终的矩阵计算结果。

**再回到那个 $1 \times 2$ 的例子 $[1, 2]$$\begin{bmatrix} x \\ y \end{bmatrix}$ 这俩个矩阵作用，看看：**
*   **列视角：** 把基向量变成了1和2。（想象空间变化的话，确实有点抽象）
*   **行视角：** 只有一行指令， $[1, 2]$。就是把二维数据压缩成了一个读数。要一一倍的$x$ ，两倍的$y$ ，结束。



## AI场景举例
行视角的威力，在人工智能领域（特别是计算机视觉）展现得淋漓尽致。

在神经网络中，矩阵 $W$（权重矩阵），

通常左乘输入向量 $x$：
$$y = Wx$$

**场景：手写数字识别**

*   **输入向量 $x$**：我们将一张 $28 \times 28$ 像素的图片拉直，变成一个 $784 \times 1$ 的长向量。这就是**原材料**。假如，它是数字”7“
*   **权重矩阵 $W$**：假设这是一个 $10 \times 784$ 的矩阵。它有 10 行，分别代表数字 0 到 9 的“过滤器”。

如果用列视角，嗯.... 一个784维的向量，其784个基向量，每个都被投影到了10维空间里，比如W的第645列，就是645维方向上的基向量，投影到10维空间时的坐标...然后x经过这些新基向量再次组合成新向量....

*我们只可以粗略理解为，输入向量x经过”降维“的空间变换。除此之外，，再想不到什么特别有效的信息*


**行视角视角，则更好理解整个过程：**

**我们看 $W$ 的第 "7" 行（对应检测数字7的那一行）：**

在这行向量里，对应图片“顶部横线”和“右侧斜线”位置的数值会非常大（正数），比如说，第600维度到700维度，代表了这个含义。

其他位置可能是 0 或负数。

代表我们想要提取的，就是600维度到700维度，这些特征的数。

 当我们用 $W$  去乘 **输入图片 $x$** 时，$W$ 的第七行明确说了，它要提取的信息就是600维度到700维度  比如这些维度的”配方“是`[12,34,12,...]

以单独的行视角来看，本质上是在做**点积（Dot Product）**，也就是**相似度匹配**：



$$
\underbrace{\begin{bmatrix} 0 & 0& \dots &  12 & 34 & 12 & \dots & 0& 0 \end{bmatrix}}_{\text{W 的第7行 (模版): 关注 600-700 维的特征}}
\cdot
\underbrace{\begin{bmatrix} 0 \\ 0\\ \vdots \\ 200 \\ 255 \\ 200 \\ \vdots \\ 0 \\0 \end{bmatrix}}_{\text{输入图片 x (数据): 在 600-700 维有笔画}}
= \text{巨大的正数}
$$
**结果** 算出来一个**巨大的正数**。 说明，诶，我们的**输入图片 $x$**  这些维度真的有数字且不小。那确实证明，输入图片是7


 **看 $W$ 的第 "0" 行，**它去乘输入图片 "7"，600维到700维没多几个数，即像素位置对不上。 算出来很小，甚至接近 0。  确实证明，它不是”0“

所以，在 AI 里，**行视角**的”信息提取“非常有用
矩阵的每一行都是一个**特征检测器（Feature Detector）**。
*   行1 检测“横线”。
*   行2 检测“圆圈”。
*   行3 检测“眼睛”。

这就是行视角在处理信息时最直观的“好处”——**它在扫描和匹配**。












1.  **列视角** 让我们看见了 **“结果去哪了”** （Geometry / Destination）。
    *   *适用于：理解秩、列空间、线性变换的可视化。*
2.  **行视角** 让我们明白了 **“结果是怎么算出来的”** （Mechanism / Construction）。
    *   *适用于：解方程（高斯消元）、特征提取（PCA/神经网络）、理解数据是如何被剖析的。*





