

---
# 今日随笔



## 今日困惑
### enumerate(train_dataloader) 会在每个循环中返回 (批次索引, 批次数据)

    for step, batch_data in enumerate(train_dataloader):
   
### CrossEntropyLoss的内容

## 计算损失的究极思考
我想想啊， 模型输入时要经过一个矩阵，叫Embedding（为什么不是linear？） 它的矩阵应该是[vocab_size, hiddenstate] 即存着每一个字的，hiddenstate的样子， 它就把每一个字对应的hiddenstate取出来了。模型输出embedding前最后的维度是hiddenstate，假设是256，这256其实就蕴含了到底倾向于哪些词 这个隐藏含义。 我们又要给它翻译过来，但用的是self.lm_head = nn.Linear(  
self.config.hidden_size, self.config.vocab_size, bias=False  
) 输出的vocab，应该意思是，每一个词都有自己的概率，这就叫logits。  
比如说输出的第一行 [0.1, 1.5, -0.9, 0.3, 0.2] 这个维度vocab，因为有vocab个概率，  
我们当初输入的Y呢，我们取第一个，就是这一行中的某一个。比如Y是【1，2，3，4】 。 所以Y是一维的，logits是二维的，logits的第一行第一行 [0.1, 1.5, -0.9, 0.3, 0.2] 就蕴含着，对输出Y[0]即1的信心是1.5 当然要softmax了，才能得出真正的概率。 我们就是考提高这里的信心的，损失函数就是用在这里，我理解的对吗？

## **为你解惑：“为什么Embedding不是Linear？”**

这是一个非常深刻的问题，它涉及到**计算效率**和**概念清晰度**。

从**数学效果**上看，nn.Embedding层做的事情，和用一个nn.Linear层乘以一个**one-hot编码**的向量是**完全等价的**。

我们来看一下：

- 假设 vocab_size = 5，hidden_size = 3。
    
- nn.Embedding的权重矩阵 W_emb 形状是 [5, 3]。
    
- 我们要查找ID为2的词的向量。
    

**nn.Embedding的方式**:  
input_id = 2  
output = W_emb[2] (直接取出第2行)

**nn.Linear的方式**:

1. **创建One-Hot向量**: 把整数ID 2 转换成一个one-hot向量：  
    input_onehot = [0, 0, 1, 0, 0] (这是一个1x5的行向量)
    
2. **创建Linear层**: linear_layer = nn.Linear(5, 3, bias=False)。它的权重矩阵 W_lin 的形状是 [3, 5] (注意，是 [out, in])。为了和Embedding比较，我们让 W_lin = W_emb.T (转置)。
    
3. **矩阵乘法**: output = input_onehot @ W_lin.T  
    [0, 0, 1, 0, 0] @ [[w₀₀, w₀₁, w₀₂], [w₁₀, ...], [w₂₀, w₂₁, w₂₂], ...]  
    这个矩阵乘法的结果，正好就是**精确地挑出了W_lin.T（也就是W_emb）的第2行**！
    

**既然数学上等价，为什么还要专门发明一个nn.Embedding层呢？**

1. **计算效率 (最重要的原因)**:
    
    - 想象一下 vocab_size = 32000。
        
    - **Linear方式**: 你需要先创建一个巨大的、32000维的、极其稀疏的one-hot向量（里面只有一个1，其他全是0），然后把它和一个 [32000, 4096] 的巨大矩阵做一次完整的矩阵乘法。这个计算是**极其浪费**的！绝大部分的乘法都是和0相乘。
        
    - **Embedding方式**: 它在底层实现上，根本不是做矩阵乘法，而是一个**高效的查表（lookup/gather）**操作。它直接根据索引去内存里定位到那一行数据并返回。这个操作的计算复杂度远低于稀疏矩阵乘法。
        
2. **存储效率**:
    
    - 在处理一个batch时，input_ids只需要存储整数，形状是[bsz, seq_len]。
        
    - 如果用one-hot方式，你需要把输入数据扩展成 [bsz, seq_len, vocab_size]，这会消耗巨大的内存/显存，在vocab_size很大时是不可接受的。
        
3. **概念清晰度**:
    
    - nn.Embedding这个名字，清晰地表达了这一层的**意图**：我们正在进行“嵌入”操作，即把离散的ID映射到连续的向量空间。
        
    - 如果用nn.Linear，虽然效果一样，但代码的意图就不那么清晰了，读者需要多绕一个弯才能明白这里是在处理one-hot输入。
        

**结论**：nn.Embedding可以看作是**为处理one-hot输入的nn.Linear层的一个极其高效的、专门优化的实现**。它在数学效果上等价，但在计算和存储效率上有着天壤之别。

## 计算损失代码
拉平

```python
logits = model(input_ids=X, pixel_values=image_tensors)

        logits_flat = logits.view(-1,logits.size(-1))

        Y_flat = Y.view(-1)

        loss = loss_function(logits,Y_flat)
```


# 一. 今日成就
1. llm  完成了预训练的所有代码！挺有成就感
2. 算法 复习了滑动窗口和哈希表的题

# 二. 今日思考

1. 损失函数理解不够，记录。loss的计算流程，我完整地厘清了，点赞！还有dataloader用enumerate  把step 和data 取出来
2. 今天上课上的难受的呀。本来20分钟就能复习完的算法题能拖到1个半小时。所以算法远远没有达到本来的目标
3. 关于课我已经解决好了。一个一个说。知识工程，讲很老的知识图谱，专家系统那边，没价值。习概，没价值。认知计算，老师很棒，讲的很好，但内容是比较抽象，比较哲学的人工智能概论的感觉，对求职无价值。操作系统，老师讲的非常棒，老师也很好... 模式识别，cv的，无价值。 微机原理 ， 计算机底层，对求职无价值。待评估的剩下机器学习和算法设计，我周二，周五，去两次，也就是下面开始就上两节体验一下。
4. 签到是委托朋友的，他会把我所有的课全代签了。同时我把俩小组作业包了，认知计算的，多模态数据处理分析，操作系统的，docker思想与应该。对求职和我的技能都有帮助。机器学习和算法设计还没上，有作业的话我也包了。然后关于平常的课内学习，我还有点头疼。认知计算很好过，往年都90分 那我认为考试前几天突击都可以。操作系统也是一样的，往年划重点很准，而且我们学的比较少，可以突击。但平常我也想学，我不知道到底去不去听课。微机原理，挂科率百分之五十。。。那就每两周或者每一周做下练习吧。剩下模式识别机器学习算法设计待评估


## 三. 明日规划

明天直接去c楼，从早上8点半到晚上9点半，电脑就放在这。中途吃饭，还有16点听机器学习，下午吃饭健身，其余时间做llm和算法。
llm，最好直接开始训练了。 然后调整sft代码，调整笔记。算法，二叉树快推进。 晚上的时候，要按计划，进行项目软启动。开始第一个项目的架构，框图分析


