

---
# 今日随笔

明天带纸笔！算法还是用笔推一下，另外llm的一些矩阵操作也可以！
哎呀，困 疲惫  有一个还是没落实 就是短休息的时候 我还是去看阅兵解析了 看不腻啊  
下次还是落实冥想吧 主要是冥想像任务一样 这样有点难主动积极地做
cs336？？
企业宣讲会？？
# 今日困惑

## word2vec
因为我们word2vec有两个模型，我们最终训练好的N×K矩阵不是有俩个吗？ 仍然做求和吗？  
还有就是 word2vec是静态的，ok， 那transformer搞出的embedding为什么就是动态的？ 什么叫动态静态？ 那训练好了embedding不都是不变的吗？


# 一. 今日成就

1. llm:笔记[[transf架构]] 更新 计算瓶颈、共享权重的知识 。输出笔记[[transr的多头与多层]]   [[tran-归一化]] 。建立新知识文件夹，语义表达。输出[[词向量OOV问题]] [[如何训练出词向量？]]   [[分词]]  [[词向量-句子相似度]]   
2. 算法：输出[[二叉树构造]]  俩道题
# 二. 今日思考
1. transformer：归一化的公式没有手动写一下，是直观理解。但”用layernorm而不是batchnorm“是完全自己的话输出，多头与多层的设计与原因，计算瓶颈（自己推理时间复杂度，<font color="#f79646">但反向传播是前向传播两倍理解粗浅</font>）、共享权重。
2. 语义表达：在[[如何训练出词向量？]] 彻底厘清词向量，词嵌入、embedding的概念。 厘清了传统静态词向量训练，以及bert如何使用transformer训练动态的词向量！bert架构的理解是最终要的！ mask挖空方式，以及CLS标签池化/平均向量池化。[[分词]] 里，梳理了传统分词到当前的llm的tokenizer，建立直觉，但觉得不是非常清晰。 [[词向量OOV问题]] 蛮简单的，后面的tokenizer就不存在这个问题了。 [[词向量-句子相似度]] ，明白了传统bert，通过cls训练词向量的弊端，了解到了S-BERT的思想
3. 算法，昨天以及之前的递归，要么是很简单，以朴素节点为观察者的递归，如翻转二叉树，翻转列表，要么是带返回值的递归，仍然以朴素节点但稍微难想，如路径总和。今天相当于观察者，是一个构造者的视角，自己拿着蓝图，为整个大厦添砖加瓦，然后再把蓝图传给下一个观察者。有点难想。其中”序列化和反序列化“，更是炸出了我对代码构造的能力不足。今天这俩题是本周最值得重刷的
4. 今天明显感觉困。 晚上拒绝了玩游戏。  有一个还是没落实 就是短休息的时候 我还是去看阅兵解析了 看不腻啊  。 长休息也是，看地缘政治，国际局势这些视频，还有点上瘾....下次还是落实冥想吧 主要是冥想像任务一样 这样有点难主动积极地做
5. 以后学习带纸笔！算法还是用笔推一下，另外llm的一些矩阵操作也可以！


# 三. 明日规划

明日有朋友约出来玩玩，我打算下午唱歌+吃饭+散步，作为休息。突然发现几乎所有社交都是我主导内容，时间，这些规划。其实也蛮爽的。
明天呢，就当休息的一天。每周末就选一天休息吧，弹性一点，这周就周六休息。。早上可以起晚点补充睡眠。下午出去，所以早上和中午的连续时间开始笔试真题。晚上回来的时候1-2个小时整理笔试的笔记或者llm知识卡片。
剩下有俩个不是明日，而是未来的想法。
首先，我无意见看到cs336 斯坦福的大模型课程 才发布的，从今年春开始的吧，今年秋也在进行。我记得大一学c的时候，就收集到信息，如果简历上写csxxx（斯坦福课） 的lab 会很加分的。  我要把这个大模型lab，或者说assignment也提上日程吗？不过我注意到蛮底层的，不调用torch库去实现一些技术。
- 不！ 收藏了。实习成功，在求职时做，实习失败，考研后做。
第二个是，注意到各大厂都陆续来我们学校做秋招宣讲。字节更是4天后就来了， 后面10月应该有阿里腾讯美团     那我的疑惑是，我要去线下了解实习岗位吗？ 因为我感觉，至少他们有活动对秋招同学面对面交流，进行简历修改，面试直通。那也可以咨询一下实习岗位吧？
可是我现在根本没简历，俩个项目也没成型啊？字节9月9晚宣讲，我要在这之前”编“一个简历吗？ 到时候说我不打算现在实习，打算俩个月后... 这有可行性吗
- 可行！ 准备好话术，准备好简历，项目没成型就写进行中，因为这不是面试的简历，只是一个个人名片！  先简短介绍，再高质量提问（关于刚才的宣讲内容，关于公司工程的应用），最后再引到我的咨询（有相关岗位吗）   。  诶，7号8号就把简历和话术打磨出来。 不试试怎么知道！！都是有学长学姐的，不怕！


