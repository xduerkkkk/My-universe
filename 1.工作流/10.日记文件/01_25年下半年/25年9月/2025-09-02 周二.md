长文深度解析，可以有，但有必要的话把他们打散成原子级的知识卡片。  
知识卡片可以在最开始的时候粘贴ai总结或文章，但当天必须用自己的话复述一遍，重写一遍。  
现在我目前的笔记有很多，都是放在一个大文件夹里，一是没有分类， 于周末写MOC。那我突然就想起，我为什么不直接在大文件夹里新建小文件夹呢，这不也是moc吗，还是说这样有弊端？
<font color="#f79646">有，</font>
- **用文件夹：** 你必须做出一个**唯一的、排他的**选择。这张卡片，到底是应该放在 📁 Transformer 文件夹里，还是应该放在 📁 归一化方法 文件夹里？无论你放在哪里，你在另一个地方就找不到了。这就是刚性。

二是没有打散， 打散成原子级，做知识卡片。长文和知识卡片都应该留在文件夹里，不是说删除长文。这俩个的目标是不一样的。  
所以说，当天做知识卡片或写长文， 周末的话归纳分类，把知识卡片迁移到手机的anki，当然，这个知识卡片有属于那种，需要记忆的，容易忘记的，再迁移

---
# 今日随笔



# 今日困惑
## 训练时的状态记录， 模型的保存机制

## optimizer，scheduler，scaler

```python
class GradScaler:
    def __init__(self):
        # self._scale 是一个【FP32】的张量，比如 tensor(65536.0, dtype=torch.float32)
        self._scale = torch.tensor(65536.0, device="cuda", dtype=torch.float32)

    def scale(self, loss_fp16: torch.Tensor) -> torch.Tensor:
        # 1. 把输入的 FP16 loss，先转换成 FP32
        loss_fp32 = loss_fp16.float() # .float() 是 .to(torch.float32) 的简写
        
        # 2. 在 FP32 精度下，进行乘法运算
        scaled_loss_fp32 = loss_fp32 * self._scale
        
        # 3. 返回这个 FP32 的结果
        return scaled_loss_fp32

```
我再次总结一下。  
首先loss是在autocast环境下计算的，精度是fp16，为了计算的效率！但是也有部分有精度需求的，会被autograd识别，仍然保存fp32.  
计算完loss我们开始反向传播了。 我们刚才是fp16的loss，特别小， 那么计算梯度时，此时梯度仍然是fp16.就可能也是特别特别小， 这会造成，存入.grad时，直接变为0. 我们得先scale一下 放大loss。 放大后再进行backward() 这叫做反向传播，这个动作是计算梯度，并且打上.grad的标签。 所以这个动作，是为了弥补fp16精度不够的缺点  
接下来呢， 我们每个属性的grad有了，就可以参数更新了。  
我们使用的是optimizer，这下我们捕获.grad了，但我们必须把刚才的scale除过去。 除过去没问题的，不会变成0的呀。  
所以本来，很小的数放不了fp16，经过我们放大再缩小，反而能放到fp16了。  
接着我们正式更新权重，刚才还埋了雷，就是刚才scale如果把梯度scale成inf了，那么unscale就无效了，也就是我们无法还原本来的梯度了。 那我们直接scaler.step（optimiezer）而不是optimizer.step() 会检查有没有这情况，有的话就跳过更新。  
这里的更新，还有个操作，就是把fp16恢复成fp32. 但我的理解是， 值大小肯定就没变呀，毕竟是小精度恢复成高精度。

整体流程，我的理解对不对
## deque！ 好多次了，一直忘
deque(iterable=(), maxlen=None)

## 平衡二叉树as
一棵树是平衡二叉树  当且仅当

1. 它的 左子树 是平衡二叉树
    
2. 它的 右子树 是平衡二叉树
    
3. 当前节点左右子树高度差 ≤ 1
计算的时候，保存中间结果！不要重复调用递归

## 二叉树的直径
简直是一模一样 
主函数 + 递归辅助函数’
vs
嵌套函数 + nonlocal
## 复习大杂烩
### 1. Autograd 机制 (自动求导)

**你应该掌握的程度**: **理解其核心思想和工作流程，而不是数学推导。**

你需要能清晰地向面试官解释：

1. **动态计算图 (Dynamic Computational Graph)**:
    
    - **是什么**: 能解释清楚PyTorch的计算图是“**即时生成**”的。每执行一次forward，都会动态地构建一个新的图来记录计算历史。这与TensorFlow 1.x的静态图有何不同。
        
    - **有什么用**: 知道loss张量是如何通过.grad_fn属性，一步步地**反向追溯**到所有参与计算的叶子节点（模型参数）的。
        
2. **loss.backward() 的作用**:
    
    - **做了什么**: 能清晰地描述出，调用.backward()后，PyTorch会：  
        a. 启动Autograd引擎。  
        b. 应用**链式求导法则**。  
        c. 计算出loss相对于**每一个** requires_grad=True 的参数的**梯度**。  
        d. 将计算出的梯度**累加**到对应参数的 **.grad** 属性上。
        
3. **tensor.grad 属性**:
    
    - **是什么**: 知道它是用来存储梯度的“容器”。
        
    - **为什么需要optimizer.zero_grad()**: 能解释清楚PyTorch的梯度是**累加**的，如果不清零，历史梯度会干扰当前的参数更新。
        

**面试官可能的问题**:

- “能讲讲PyTorch的自动求导是怎么工作的吗？”
    
- “loss.backward()这行代码执行完后，发生了什么？”
    
- “为什么我们在每个训练步开始前都要optimizer.zero_grad()？”
    

---

### 2. Optimizer 更新公式 (Adam / AdamW)

**你应该掌握的程度**: **理解其核心思想和关键参数，而不是默写公式。**

你需要能向面试官解释清楚Adam/AdamW是如何**超越**传统梯度下降（SGD）的。

1. **核心思想**:
    
    - **动量 (Momentum)**: Adam/AdamW不仅仅看**当前**的梯度方向，还会“参考”**过去**的梯度方向。能用“**下山**”的比喻来解释——一个带有动量的球，不仅会沿着当前最陡峭的路走，还会因为惯性，更容易冲出一些小的“坑洼”（局部最优点）。
        
    - **自适应学习率 (Adaptive Learning Rate)**: Adam/AdamW会为**每一个参数**，都维护一个**独立的、自适应的学习率**。能解释清楚这是如何实现的：通过记录每个参数梯度的**平方**（二阶矩），来判断这个参数的更新是否“稳定”。
        
        - 如果一个参数的梯度一直很大很稳定，就说明它在正确的方向上，可以给它一个稍大的有效学习率。
            
        - 如果一个参数的梯度忽大忽小、来回震荡，就说明它可能在最优点附近，应该给它一个较小的有效学习率，让它“冷静”下来。
            
2. **Adam vs. AdamW**:
    
    - **W代表什么**: **Weight Decay (权重衰减)**。
        
    - **区别在哪**: 能讲清楚Adam的权重衰减和L2正则化是耦合在一起的，有时效果不好。而**AdamW将权重衰减和梯度更新解耦**，直接在更新权重时，从原始权重里减去一小部分。这被证明在训练Transformer等大模型时**效果更好、更稳定**。
        
    - **结论**: 知道为什么AdamW是当前训练LLM的**首选**。
        

**面试官可能的问题**:

- “你一般用什么优化器？为什么选择它？”
    
- “能简单讲讲Adam的原理吗？它比SGD好在哪里？”
    
- “Adam和AdamW有什么区别？”
    

---

### 3. Scheduler (学习率调度器)

**你应该掌握的程度**: **理解其“为什么”，并了解几种主流的“是什么”。**

1. **为什么需要Scheduler**:
    
    - 能清晰地解释，在训练的不同阶段，我们对学习率有**不同**的需求。
        
    - **初期**: 需要**较大**的学习率，让模型快速跨越“平坦”区域，加速收敛。
        
    - **后期**: 需要**较小**的学习率，让模型能在最优点附近进行“精细微调”，避免“震荡”而错过最小值。
        
    - **Warmup (预热)**: 知道为什么在训练的最开始，我们通常会让学习率从一个很小的值**线性增长**到设定的初始学习率。这是为了防止模型在训练初期（参数都是随机的，梯度可能很大）因为学习率过大而“飞出去”（数值不稳定）。
        
2. **了解几种主流策略**:
    
    - **StepLR**: 每隔N个epoch/step，学习率乘以一个固定的因子（比如0.1）。简单粗暴。
        
    - **CosineAnnealingLR (余弦退火)**: **这是你需要重点掌握的！** 知道它的形状像一个余弦函数的四分之一周期，学习率从初始值平滑地、非线性地下降到接近0。这是目前训练Transformer**最常用、效果最好**的策略之一。MiniMind作者手写的get_lr就是它。
        
    - **ReduceLROnPlateau**: 当某个监控指标（比如验证集loss）在一段时间内不再改善时，就降低学习率。更具自适应性。
        

**面试官可能的问题**:

- “你在训练时会使用学习率调度器吗？为什么？”
    
- “能介绍一两种你常用的学习率策略吗？”
    
- “什么是Warmup？它有什么用？”
    

**总结**:  
这三个知识点，你不需要去背诵复杂的数学公式，但你需要能够**用直观的语言，清晰地向别人解释它们的核心思想、解决了什么问题、以及不同方案之间的区别和联系**。把这些掌握了，你的机器学习基础知识就非常扎实了。

## 类和实例
TrainArgs = TrainArgs()  
os.makedirs(TrainArgs.output_dir, exist_ok=True) 我突然想到 这样用实例引用属性，和不用TrainArgs = TrainArgs()这一行 直接引用类 有什么区别？

## vlm 的 pretrain和sft
好。下面再次回到整个VLM项目。 我现在还是不太能理解， 首先我们本次项目的预训练数据，就是单轮图文对话，没问题吧！  
那么我们的visonencoder，是使用现成的vit， 是clip模型中的vit单独摘录出来，对吗？这个会让图片向量靠近图片含义的文字向量。 然后我们的语言模型也是训练好的。 那我们这个投影层， LLAva架构只是个linear嘛， 就是学把图片彻底翻译成文字 ，就是说vit出来是靠近文字向量， 它的向量维度实际都跟文本向量不一致， 而这里的linear直接是投影，把图片向量完全转化成文字向量的形状，我理解的没问题吧？ 那我们预训练时，只训练这个投影层， 就是为了让它翻译准确一点，对吗？  
然后第二轮sft，我们训练的是llm注意力机制的qkv矩阵，我们是为了让模型能理解图片，所以用的多轮指令对话，是这样吗？  
所以两轮中，投影层只在第一次训练 llm只在第二次训练 vit全程冻结 我的理解对吗
# 一. 今日成就
- llm：昨天是写完pretrian代码，今天是完全完善，并细究里面能复习的点！
并且，sft代码也可以算完成了，实际上只需要修改一点点代码
- 算法：一个番茄钟重温，落实了二叉树迭代遍历+层序遍历。两个番茄钟将二叉树递归做了下， 一个基础的二叉树深度，两个要用到辅助函数的平衡二叉树、二叉树的直径
# 二. 今日思考
记录知识卡片
- [[训练时的状态记录， 模型的保存机制]]
- [[类和实例]]
- [[deque]]
- [[VLM_from_scratch]]中，pretrain和sft的区别
- [[递归的三步思想！]]
- [[迭代遍历]]
- [[平衡二叉树、二叉树直径]]
- [[optimizer，scheduler，scaler]]
其中  后三篇只是笔记，＋粗略思考，并没有做到知识卡片的程度

同时呢，还找到了自己下一步复习的方向，那就是autograd，scheduler，optimizer 过一遍
反向传播基础过一遍 

## 三. 明日规划

明天 跑一个smoketest（是叫这个吗） 把代码看能不能跑通 能跑通就暂时放了 趁知识热，去写lora微调的代码。又可以复习一波基础知识。不过，早上应该先把昨天遗留的知识卡片做完。
那没做完是为啥呢，就是晚上还是太想继续学。
今天18点到19点没做粗略总结，总结全留在23点后导致的。 那么明天晚上学习前粗略总结执行一下，应该没问题。
今天白天，每个番茄钟后都写了心得，观察下来心态不错的，就是发现自己简单的基础还不牢固有点灰心，但立马调整好了，暴露问题是好事。
再就是面经和面试录音视频分心带来焦虑，已经想通解决，以后一点不看，直到11月份。
算法继续二叉树，我已经规划了5天二叉树计划。


