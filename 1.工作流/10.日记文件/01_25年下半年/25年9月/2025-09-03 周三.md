

---
# 今日随笔



# 今日困惑
## 微调数据处理

with open('data_vl.json', 'w', encoding='utf-8') as f:  
json.dump(conversations, f, ensure_ascii=False, indent=2)  
然后就和我们自己的dataset的getitem一样了，应用template，然后生成input_id。 那我突然有疑惑了， 那为什么非得变成json？ 为什么不直接把数据，提取caption，提取image，然后应用template

**如何构建一个可扩展、可维护的数据处理管道。** 而“定义一个标准化的中间格式”，正是这个问题的最佳答案。

## 回溯
感觉可以顺便复习回溯了？ 回溯不用单独复习的感觉

值返回递归”和“过程式递归”的区别  回溯就是过程式递归


# 一. 今日成就
1. llm：lora微调（peft库版） 代码全解析， 输出文章[[训练流程对比]] 。 复习最基础的深度学习概念，整理autograd optimizer scheduler的知识点 输出文章 [[autograd、optimizer、scheduler]]
2. 算法：完成二叉树的三道路径总和 输出[[二叉树路径总和]] [[参数传递与作用域]] 

# 二. 今日思考
- 其实lora微调就是多了一个peft库。但是呢，这个代码是用huggingface训练的嘛，而我之前是纯手写dataset 和trainepoch，  所以在文章中，我记录了手写时的数据处理，和使用库的工业级的处理，他们的区别。 让我联想道， 微调也有llamafactory框架， 但我之前仍然是手写微调，还写过lora旁路矩阵，冻结参数这种细致的（不过有ai辅助，且没做笔记）。 我觉得这种对比着写笔记，收获特别大
- 将autograd optimizer scheduler笔记写透吃透，有点成就感的。 
- 还回顾了一下自己设计的模型架构，好，确实是 标准的llama   vlm架构呢 就是标准的llava 
- 今天算法题，路径回归，可算是递归思想的考验了，没通过考验啊。。后面肯定要再刷的。 然后呢， 顺便回顾了回溯，我感觉回溯模板就可以顺便回顾了，很简单的。
- 嗯..今天其实学习时长不够，被阅兵和阅兵解读视频分去了好多时长啊哈哈。我其实察觉到了一次，但是察觉到了依然没抵挡住看阅兵的诱惑， 毕竟是祖国，原谅自己一次吧

## 三. 明日规划
计划特别多，再次调整了第一阶段，先不在这放了。加油！！！！！！！！！我一定可以的！！！！ 从明天开始，我会发关于llm技术的文章，把之前模糊的索引打磨高清，并且通过输出倒逼精品，哈哈！



