# 📝学习内容

1. flashattention的详细学习
2. 回顾进程、异步，各个进程之间的通信
3. 学习模型推理的几个项目，学习架构的设计 
4. 写压测代码，处理工程问题
5. 项目上，自己借助ai，去跑docker，熟悉docker
6. 算法上，学习多源最短路径的各个算法，将bfs，dfs再次刷题，复习多源bfs和dfs回溯
输出笔记 [[flashattention]]
完成vllm压测并绘图

![[2025-8月第二周（8.11-8.17）-周报-1755488525417.jpeg]]

![[2025-8月第二周（8.11-8.17）-周报-1755488534708.jpeg]]



![[2025-8月第二周（8.11-8.17）-周报-1755488539187.jpeg]]





# 💎学习总结

## 做的好的
- 严格采用番茄钟，与上周状态一样。

## 做的不好的
- 这周llm知识是比较少的，都在看项目和写代码， 但我觉得这依然可以输出笔记！
- 关于问题的记录也是一样的，  我感觉看源码的方法是需要改进，目前看完也不知所云，应该多点思考
总之是笔记的输出和问题的记录，这块这周总结的少，说白了是主动思考不够！ 下周需要格外注意

# 📒遇到的问题

## 困惑一 Bellman_ford
<font color="#f79646">Bellman_ford为啥是直接“松弛” n-1次  然后就一定找到最短路径？</font>
```
初始化edges ： 点1，点2，权值
初始化minDist   除了minDist【1】为0外其他都是无穷大

for循环 最多遍历n-1次：
	updated = False  用于提前打断循环，如果不更新了，那就得到结果了，不用再费时间循环了
	for循环 遍历每个边：
		对每个边进行松弛操作
		即 if minDist【点1】 ！= 无穷 且 mindist【点1】+weight < mindist【点2】
		意思是如果点2以点1为桥梁，能距离源点更近的话
		更新mindist【点2】 = mindist【点1】 + weight
```
为什么遍历n-1次呢，其实这个算法也不知道到底遍历多少次，我就能找到最短路径，他只知道最坏情况下，就遍历n-1次，所以把总次数设立为n-1.中途不update了再跳出就行。

所以我们遍历次数大于n，和遍历n-1次，得到的结果是一模一样的。
但是！ 若图中有负权回路，那就会变小。所以可以通过第n次遍历，比较第n-1次遍历，判断图中是否有负权回路

## 困惑二 flashattention的qkv
<font color="#f79646">我以为flashattention的softmax是qkv全部相乘后计算的，其实就是先算p（注意力分数）再乘v，这个p是softmax过的？</font> 
是，这里精确一下名称：S是qk相乘，n×n的矩阵，是注意力分数。然后应用softmax，得到的注意力权重矩阵P， 最后的输出，PV，称作O。flashattention是把S、P的计算，一步到位，采用tiling和online softmax

## 困惑三 怎么进行online softmax的？
维护全局变量sum和max， 我们直接把结果先写入n乘d_n，也就是O那个输出矩阵， 后续再不断基于新sum和max对结果进行更新

## 困惑四 flashattention的空间复杂度和时间复杂度
softmax是针对n乘n矩阵的一列，进行运算的。不过，这一列也被我们分块了。那么flashattention，需要存储的，是当前列，目前最大softmax值，还有当前时间步为止的，sun和，。  这俩，都是o（1
）的空间复杂度     最后就是，n乘n_d这个输出矩阵。 所以总的空间复杂度就是O（n）

那访问次数呢？ 我们一共访问了哪些？ 访问的时间复杂度是多少
标准
- **读取**：Q,K,V (各一次) + S (一次) = 3Nd+$N^2$
    
- **写入**：S (一次) + O (一次) = $N^2$+Nd
    
- **总计**：$N^2$+4Nd
flashattention：
- **读取**：Q,K,V (各一次) = 3Nd
    
- **写入**：O (一次) = Nd
    
- **总计**：4Nd
## 困惑五 项目调用
<font color="#f79646">整个项目 用户是如何调用的。 我虽然下了源码，但看官方文档又看到是pip install xxx，为什么？ 然后我看pip后用户在命令行输入参数就能部署并运行模型。我好奇的是，这个项目是怎么做到让用户调用的？ 也就是怎么封装给外界的？到底是哪些文件起了作用</font>
pip install只有核心的python脚本和接口文件 
**`setup.py` / `pyproject.toml`**：这些是 Python 打包配置文件。它们告诉 `pip` 如何构建和安装项目，包括如何编译和包含 C++/CUDA 扩展。
用户输入命令 -> 触发执行 cli.py 里的 main 函数 -> main 函数再调用其他Python代码 -> 这些Python代码最终调用到由 pybind11 “翻译”过来的C++函数 -> C++代码执行高性能的大模型推理。
pip install ftllm 是直接下载**别人已经编译好的、打包成wheel的“成品”**

## 困惑六  回顾进程、异步，各个进程之间的通信
<font color="#f79646">每个进程都是同步阻塞的，但通过多进程实现并发。 这句话什么意思？？？？？ 多进程难道不就是异步吗？？？？</font>
首先，线程是进程的执行单位，是最小单位，我们平常cpu调度就用的线程。所以我们把线程想象成一个“工人”，进程想象为一个项目组。我们公司执行任务都是交给项目组嘛，调度项目组。但是cs世界里，及其在意打工人的安排，假如说os的调度器是项目经理，他会及其在意每一个打工人的安排，他会直接调度打工人，更加细化！所以平常说的最多的应该是线程。 一个cpu核，一个时间下，只能调用一个线程。他可以从始至终调用一个线程，完成任务。线程老老实实完成手底下的任务，这个任务中如果出现特殊情况，需要读写硬盘， 结果发现，哎呀硬盘干其他事呢，我就等着吧。于是此线程原地等待硬盘，不在进行其他操作，这叫**同步阻塞**。那如果，线程说不等待硬盘，给硬盘说"你如果好了叫我，我去干不需要你的事情了“，这就叫**异步**！如果他一开始呼叫多个进程，一个时间下，只能调用一个线程，他会快速切换线程完成各自的任务， 一个线程干0.01s 下一个0.01s立刻叫另一个线程干一下他的活 下一个... 这样，用户体感上，就好像他们在同时干活。但其实是交替干活，一个时间下只有一个线程干活，所以真实情况一定比同时干活慢。 这叫并发。 如果我们有多核cpu，那我们可以真正做到一个核调用一个进程，多个进程同时开始，一个时间下有多个进程干自己的活，用户不进是体感上同时，实际上也同时，这叫并行。
所以同步和异步只是说线程“在等结果时，所处的状态”，与并行并发的概念本身没有任何关系，并行并发只关心整体的推进，是多个线程一起啊还是交替多个线程啊，有的关系只是，如果做到每个线程都是异步状态，并发就很容易实现，因为并发需要不断切换、调用线程，你要有个线程堵塞了那不就完了，。
asyncio 的核心是单线程并发，我们刚才提到并发的举例是多线程并发。单线程并发就是一个线程，即一个工人，去交替执行任务。然后这个线程也是很聪明，拥有异步的概念， 当事件循环运行到一个标记为 await 的操作时（通常是耗时的 I/O 操作），它不会傻等。相反，它会将这个任务挂起，然后立即去执行其他已经准备好的任务。当之前挂起的 I/O 操作完成后，事件循环会收到通知，并在下一个合适的时机切换回去继续执行该任务。


## 困惑七 floyd算法
<font color="#f79646">floyd算法，为啥把k放到最外层？ 我明白这符合直觉，但我想象不到放内层会怎样</font>。
```
for k in range(1, n+1):
        for i in range(1, n+1):
            for j in range(1, n+1):
                grid[i][j] = min(grid[i][j], grid[i][k] + grid[k][j])
```
嗯... 这么理解吧，这个k是关键的中间步骤，  
## 困惑八 异步并发测试不预热吗？


**需要预热，但预热的方式不同。**

你的直觉非常敏锐。对于吞吐量测试，“预热”的目标是确保：

1. 模型和CUDA内核已经被加载并编译好。
    
2. vLLM的调度器和内存池已经准备就绪。
    

在我们的新脚本里，**engine = AsyncLLMEngine.from_engine_args(...) 这一步，就已经完成了大部分重量级的预热工作了。** vLLM在引擎初始化时，就会加载模型、分配内存、准备好一切。

所以，我们不需要像在延迟测试里那样，特意跑一次generate来预热。当我们开始计时并调用asyncio.gather时，系统已经处于“战斗准备”状态了。

## 困惑九 并发代码
因为我们初始化的是async的engine，
llm.generate不会立刻开始生成
而是一个生成器
我们使用for out in generator:
把生成器的最后一个结果逼出来，这个结果就是最终的回应

```python
async def get_all_outputs(generator):

        final_output = None

        async for out in generator:

            final_output = out  # 最后一个是最完整的

        return final_output  # ← 注意这里不再是 list

  

    

    tasks = []

    for i in range(args.concurrency):

        prompt = prompts_dataset[i]

        tasks.append(

            get_all_outputs(llm.generate(prompt, sampling_params,request_id=f'req-{i}'))

        )
        
    for output in results:

       total_tokens += len(output.outputs[0].token_ids)
```


# 其余困惑

再就是一些工程问题


### git操作
- 想**同步远端**用 `git pull`；
    
- 想**把本地推上去**用 `git add . && git commit && git push`



### wsl连不上网
 
开vpn就会这样 确定端口  不知道v2rayn端口是啥  旧版clash是7890 
```
netstat -ano | findstr LISTENING | findstr :7890
netstat -ano | findstr LISTENING | findstr :1080
```
最关键的，直接终端设置

```
export http_proxy=http://172.24.224.1:7890 
export https_proxy=http://172.24.224.1:7890
```

powershell里跑，能确定本地代理端口
### vllm为什么跑不起来
1. **桌面环境**会大量占用GPU显存。
    
2. **推理框架**本身有显著的显存开销。
    
3. **内存碎片**和**总容量**共同决定了程序能否运行。
### autodl 连接
f1，remote connect 然后直接输入root@connect.nmb2.seetacloud.com 不用输入端口


# 🚀下周计划

1.推进多模态的学习
2.优化一下时间的利用，提高效率
3.思考一下，如果既利用ai辅助自己理解知识，又让自己能独立思考，输出总结？
4.算法二刷“代码随想录”，核心题重写，其他题写伪代码，然后再总结出属于自己的，本专题的模板



配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=1, KV Cache=auto
平均单条推理延迟: 1.0766 秒配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=2, KV Cache=auto
平均单条推理延迟: 0.4587 秒  配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=1, KV Cache=auto, 并发量=4
总耗时: 4.2784 秒
总处理Token数 (输入+输出): 379
吞吐率: 88.5845 tokens/秒 配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=2, KV Cache=auto, 并发量=4
总耗时: 2.0738 秒
总处理Token数 (输入+输出): 329
吞吐率: 158.6495 tokens/秒 --- 吞吐量测试结果 ---
配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=1, KV Cache=auto, 并发量=8
总耗时: 4.2750 秒
总处理Token数 (输入+输出): 698
吞吐率: 163.2766 tokens/秒 --- 吞吐量测试结果 ---
配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=2, KV Cache=auto, 并发量=8
总耗时: 2.8779 秒
总处理Token数 (输入+输出): 689
吞吐率: 239.4090 tokens/秒 --- 吞吐量测试结果 ---
配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=1, KV Cache=auto, 并发量=16
总耗时: 2.6442 秒
总处理Token数 (输入+输出): 780
吞吐率: 294.9817 tokens/秒 配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=2, KV Cache=auto, 并发量=16
总耗时: 1.6170 秒
总处理Token数 (输入+输出): 779
吞吐率: 481.7465 tokens/秒  现在要处理32个并发请求

--- 吞吐量测试结果 ---
配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=2, KV Cache=auto, 并发量=32
总耗时: 3.1333 秒
总处理Token数 (输入+输出): 1908
吞吐率: 608.9520 tokens/秒 现在要处理32个并发请求

--- 吞吐量测试结果 ---
配置: 模型=../Qwen2-7B-Instruct, 量化=None, TP=1, KV Cache=auto, 并发量=32
总耗时: 4.4568 秒
总处理Token数 (输入+输出): 1835
吞吐率: 411.7292 tokens/秒  现在要处理4个并发请求

--- 吞吐量测试结果 ---
配置: 模型=../Qwen-7B-Instruct-AWQ, 量化=AWQ, TP=1, KV Cache=auto, 并发量=4
总耗时: 0.9305 秒
总处理Token数 (输入+输出): 216
吞吐率: 232.1396 tokens/秒 在要处理8个并发请求

--- 吞吐量测试结果 ---
配置: 模型=../Qwen-7B-Instruct-AWQ, 量化=AWQ, TP=1, KV Cache=auto, 并发量=8
总耗时: 0.8484 秒
总处理Token数 (输入+输出): 324
吞吐率: 381.8851 tokens/秒 现在要处理16个并发请求

--- 吞吐量测试结果 ---
配置: 模型=../Qwen-7B-Instruct-AWQ, 量化=AWQ, TP=1, KV Cache=auto, 并发量=16
总耗时: 1.4534 秒
总处理Token数 (输入+输出): 579
吞吐率: 398.3744 tokens/秒 现在要处理32个并发请求

--- 吞吐量测试结果 ---
配置: 模型=../Qwen-7B-Instruct-AWQ, 量化=AWQ, TP=1, KV Cache=auto, 并发量=32
总耗时: 1.8344 秒
总处理Token数 (输入+输出): 1184
吞吐率: 645.4389 tokens/秒