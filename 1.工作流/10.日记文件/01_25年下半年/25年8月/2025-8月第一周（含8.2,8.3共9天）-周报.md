# 📝学习内容
1. 复习transformer，这块耗费蛮长时间，害怕学大模型推理的时候一知半解，把transformer所有流程都过了一遍，其中的重要概念比如Q矩阵K矩阵，Block架构、时间复杂度都过了一遍。手写了注意力点积和多头注意力。
2. 了解静态批处理、动态批处理
3. 学习kvcache，以及对比不使用kvcache的时间复杂度
4. 了解量化的数据类型，最简单的线性量化的公式、训练后量化的含义
5. 了解量化感知训练、混合精度训练
6. 了解AWA框架、AutoGPTQ框架，以及他们的重要思想和关键参数
7.  weightonly对比 W8A8
8. 显存管理
9. 学习vllm框架，核心思想pagedattention
10. 将vllm所有源码 所有流程通读，学习，包括了scheduler、blockmanager
11. 算法题，复习dfs、bfs，学习并查集、最小生成树，共刷8道题![[2025-8月第一周（含8.2,8.3共9天）-周报-1754885722001.jpeg]]
输出笔记[[01transformer]] [[模型量化]] [[模型推理流程概览]] [[vllm]] [[vllm-scheduler]]

![[2025-8月第一周（含8.2,8.3共9天）-周报-1754885506629.jpeg]]

# 💎学习总结

## 做的好的
- 严格采用番茄钟，在番茄钟内注意力高度集中，减少分神的情况
- 主动输出笔记
- 把每日出现的问题记录下来，用自己的话或ai解答回答一下。周报的时候，把所有问题（不包括答案）粘贴过来，自己再复述一下答案，感觉很不错。
## 做的不好的
- 还是有偷懒的时候，学习时间可以往上提
- 自己手动总结还是心态上阻力蛮大， 开始的时候畏难， 老想把ai的答案粘贴到笔记，一旦开始了又很顺畅，所以后续多逼着自己总结，无论多小的问题，还有ai解答的问题，都用自己话说一说。
- 这个学习碰到的困惑还挺多的，仍然要积极总结，记录

# 📒遇到的问题
### 困惑一：argmax 中的 dim 参数

<font color="#f79646"> dim=1是什么意思？之前使用单个prompt的时候 就不填dim 为什么？</font>

先搞清argmax采样是对谁采样，是prompt输进去后，经过transformer架构，decoder最终输出的向量，这个向量是由batchsize，seq_l, d_model （transformer架构流动的向量维度）  与d_model,  vocab_size（语言模型头） 相乘得来的  ， 最后的输出张量batchsize，seq_l, vocab_size 就是logits
那么`logits[:-1,:]`就是argmax的操作对象， 最后一个词嘛，
此时张量形状变为了batch_size,vocb_size 。我们argmax采样，当然是看词汇表，所以维度是1.
单个prompt的话，batchsize是1，1张量形状是` [1, vocab_size]`。
. 在很多计算框架（如PyTorch, TensorFlow）中，维度为1的轴经常会被自动“挤压”（squeeze）掉。所以` [1, vocab_size]` 的张量就变成了一个只有一维的向量，形状是` [vocab_size]`。
自然不用加维度数了。

### 困惑二：kvcache的position_ids
<font color="#f79646">为什么非要加position_ids？位置编码不是大模型内部就处理了吗</font>
是，模型在无论何时都需要一个位置信息。
模型默认是怎么知道位置信息呢？
根据输入序列长度，挨个读。
来个情景，比如说输入”我爱吃“，模型看到seq_l =3 自动就能把我爱吃三个字，各自位置的位置编码计算出来。生成下一个词，苹
现在是”我爱吃苹“，但生成下一个词的时候，模型需要重新读一下这个序列长度，哦，四个字。那我即将生成的字，就是第五个。  
所以模型是靠序列长度知道位置的。
那kvcache呢？
定格到输入我爱吃，得到苹的瞬间
之前是”我爱吃苹“让模型重新从头到尾读一遍，现在我们有缓存，就直接把苹这个字喂给模型，形成的向量加到cache矩阵就好了。那现在输入的只有一个字诶，seq_len是1，模型会以为，哦，这个苹字，是句子的第一个词？
实则不然。所以我们要显示告知这是第几个词，加入position_ids
### 困惑三：padding、truncation
<font color="#f79646">为什么之前（处理单序列时）没用到padding和truncation，现在（批处理时）就用到了呢</font>

padding是为了解决多个prompt输入时，格式统一。喂给模型的总字数一样。
因为gpu之所以快，就是可以对**已经规整**的、长方体形状的数据块进行一次性操作，就是矩阵乘法，嘛。你要矩阵，你还能句子长度不一样了？？？
truncation单序列也有可能需要啊，本质就是个截断过长的序列，迁就模型能力这个硬件架构限制。

### 困惑四：静态批处理与动态批处理

<font color="#f79646">为什么说，我们现有的这种‘凑齐人再一起进出’的批处理方法，虽然比单人处理好，但效率还是太低了</font>
这个静态批处理，出现的问题叫做前端堵塞、后端堵塞。 本质仍然要所有顾客服务好才把所有顾客放出来，那如果A顾客就需要服务5分钟，B顾客服务1小时，那A还一直呆在里面，很浪费时间。
continus batching就是你好了你就出去。
### 困惑五：怎么预留kvcache显存
首先，我们再次明确我们存的是什么。是输入词向量与W_K相乘的k向量，是activation而不是weights。V同理。然后我们看看它的形状，就是一个`[d_model]` 然后我们为整个序列预留，
我们假设请求最大长度是`N_max` 现在形状是`N_max,d_model`  这可以说就是cache矩阵形状了
还少了点什么，一个是所有layers， 有block_nums个block，就乘block_nums，还有batchsize。 最后在乘2

显存占用 (Bytes) = 批处理大小(b) × 最大序列长度(N_max) × block_nums(l) × d_model × 2 × 每个元素的字节数

### 困惑六：int4weightonly 和 W8A8区别
W8A8就是权重和激活值都int8量化，很适合在线推理
int4weightonly就是部署成本低了，部署大模型到手机，个人电脑上。

### 困惑七：pagedattention思想
<font color="#f79646">逻辑到物理的映射，如何映射，映射的规则记录在页表里，是这样吗？ 那虚拟内存到底是什么意思，到底有什么作用呢？ 是不是就是，让外面人“以为是连续的空间”“看起来是连续的空间”，这样好管理，实际内部实现是物理存储的分块存储罢了？</font>
映射， 都是通过物理块id和逻辑块id的一一对应实现。虚拟内存就是为了解决内存碎片，之前，如果数据直接分配到gpu上，那必须是连续的，gpu靠基地址+偏移量找数据，加载到它的寄存器里，它就能进行计算，不连续的话数据都找不到啊。  pagedattention呢，就是将数据放到不连续的空间，哪里有空就放哪里。将这些物理地址的数据加载到寄存器，管你连续不，我加载了，就能计算了。**不连续地存储数据，一定效率最高！** 那对程序而言呢，为每个程序提供一个私有的、巨大的、从0开始的连续地址空间，程序自己感觉非常爽，它以为自己独占了整个内存，并且所有内存都是整齐排列的。 **实际情况**都有工作人员在背后默默地工作，将这些虚拟地址翻译成**物理内存中零散的、不连续的物理地址**。
### 困惑八：swapped策略
<font color="#f79646">下面又有一个场景。就是请求过于多，以至于预留到底推理空间不够了。 vllm采取的是，把后来的请求kvcache先swap到cpu上，后面空闲了再加载，是这样吗？ 那对应到现实情况，这个用户是后来的请求，需要给前面的请求腾位置，我可不可以理解为某个用户收到的回答戛然而止？</font>
这说的就是swapped策略， 用户不会戛然而止，会稍微卡顿一下。
### 困惑九：sequence的block分配
呀 都忘了为什么会有这个困惑  
那就一个一个分呗  还能放下就放，放不下再给你个block
要注意的点是，一个block，可以存放多个token_id


### 困惑十： 对prefill阶段“新客人”的检查
<font color="#f79646">有两次对太长的客人进行拒绝 一次是num_prefill_tokens > self.prompt_limit 一次是使用can_allocate 检查 可不可以理解为，第一个是人为限制的，公司不想有太长的过来，就算我能接待我也不想接待。 第二个是，你的行李其实符合最初规定，但仍然太长了，今天空间有限制，就算我们全部剩余空间服务你也无法服务，所以走吧 是这样吗？</font>
第二个有点不太对。我们梳理一下prefill阶段，检查的所有流程
1. **prompt_limit**：**人为规定**的业务上限。我最多允许多长的prompt进入，我不管硬件如何实际情况如何，我就是不接待大于limit的
    
2. can_allocate's NEVER check：这是**硬件决定**的物理上限。无关今天的情况，这是计算gpu的总块数的显存，与你对比，就是你一个人都能把我整个硬件撑爆，拜拜了您
    
3. **单次新客Token上限 (max_num_batched_tokens)**：**人为规定**的流量控制参数。我最多，就想接待这么些<u>新人</u>。
    
4. **总在场人数上限 (max_num_seqs)**：也是**人为规定**的流量控制参数。你要知道，现在除了新顾客，还有running的老顾客啊，  这些都是人，  所有人， 总数我也有规定，我不想接待太多的<u>人</u>。


### 困惑十一：decoding饥饿问题
<font color="#f79646">prompt_run=True：设置一个标志，告诉LLMEngine，“嘿，这次的名单里有新人，请使用Prefill Kernel来执行！” 我想起llmengine每次要么在prefill要么在decoding，那如果条件特别巧合，一直能有新人来，那不就一直prefill吗？ 还是说，我们的这些人为规定，巧妙地平衡了prefill和decoding的次数</font>
如果新请求源源不断地到来，并且每次调度都能满足那“四层筛选”，那么调度器理论上**确实会一直执行Prefill阶段**，导致那些已经在running队列里、等待进行Decoding的老请求，被**无限期地“暂停”**，无法取得进展。
vllm设计了一个参数 _passed_delay(self, now: float)
调度器总是会**优先**尝试从waiting队列拉新人（因为swapped为空时，代码就先走这个分支）。但是，一旦它成功地执行了一次Prefill，self.prev_prompt就会被设为True。在接下来的极短时间内（由delay_factor决定），_passed_delay会返回False，**从而强制阻止调度器连续不断地执行Prefill**。

这段强制冷却时间，就是不让decoding没被执行过的保底策略。

### 困惑十二： Copy on Write思想
<font color="#f79646">什么是copy on write思想？vllm又怎么用到了？</font>

先来个日常例子，就比如三个人看同一个文档，这个文档就叫”共享文档“。但此时突然有个人想修改，系统会给他新开一个文档，把原文档复制过来，让他在这个新文档，旧内容上，进行修改。其余两个人，看到的文档内容，是未修改的。   也和git branch 的思想有点类似吧。
这个决策，就叫copy on write，只有你要真的用，真的修改的时候，我再给你创建个新空间，copy过来，而不是一开始，三个人看一个文档，就直接复制出三个文档让大家看。  
所以copy on write是一个节省内存的操作。
对应的vllm框架，就是**并行采样**和**Beam Search**时，seq_group的共享prompt物理块

容易混淆的，是prefixcaching过程， 
让**两个或多个完全独立**的请求（SequenceGroup），如果它们恰好有**相同的前缀内容**，可以共享同一份物理块。在前缀缓存的场景下，那个被共享的、代表前缀的物理块，理论上是“神圣不可侵犯”的，它不会被写入，因此也就不需要被复制

Copy-on-Write的触发，**不是**因为一个块被共享了，而是因为一个**被共享的块，将要被写入**。
# 🚀下周计划
llm：3-5天内完成ai infra的学习 
然后开启多模态学习
算法：整体流程就是，先把代码随想录的图论算法思想掌握总结，再刷四五天的dfs、bfs，然后就二刷一遍代码随想录，经典的题默写，其余题至少伪代码准确无误，然后每个专题完了输出自己的总结，把每个专题是套路、思想总结好。 接着就hot100，每个保质保量， hot100完了 codetop网站继续无专题刷题。 目前每天1-1.5h地持续推进
项目：重新设计一个后端架构的项目， 再就是结合着项目，对于langchain与rag，可以持续复习了， 看源码，看面试题等。
