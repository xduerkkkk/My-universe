

---
# 今天想说的话
复习：
- torch的变形

问题 
## 困惑一 进入vit的attention层 发现**仍然使用了位置编码，是传统的sincos，我没理解不是已经有2drepo了吗"**
: **这是你最后、也是最关键的一个困惑点，我们把它彻底讲透！**
- **你看到的 cos, sin 就是 2D RoPE 的最终形态！**
    
- **RoPE (Rotary Positional Embedding) 的本质**，不是像传统位置编码那样把位置信息“加”到输入里，而是通过**旋转**的方式“乘”进去。这个旋转操作，正是通过**余弦 (cosine)** 和**正弦 (sine)** 来实现的。
    
- **回顾一下流程**：
    
    1. 在最外层的 VisionTransformer 的 forward 里，有一个 self.rot_pos_emb(...)，它根据 grid_thw (网格尺寸) 计算出了每个patch对应的**二维旋转角度**。
        
    2. 然后，这个旋转角度被转换成了 cos 和 sin 两个张量，并被打包成 position_embeddings。
        
    3. 这个 position_embeddings 被一路传递，最终送到了我们现在看的这个Attention模块里。
        
    4. 在这里，apply_rotary_pos_emb_vision(q, k, cos, sin) 这个函数，就执行了RoPE的核心操作：用 cos 和 sin 值去**旋转** Q矩阵和K矩阵中的每一个向量。
        
- **所以，你看到的 cos 和 sin 并不是传统的“sincos位置编码”，而是 2D RoPE 位置编码的最终执行者。** 传统的sincos位置编码是 pos_encoding + input (相加)，而RoPE是 rotate(input, pos_encoding) (旋转/相乘)。你在这里看到的是RoPE，不是传统的相加式位置编码。这个困惑解开后，你的理解就完整了。
## 困惑二 2drope和mrope
下面把两者拆开讲，再给一张“通俗图”帮助记忆。

---

### ① 2D-RoPE：只在 **Vision Transformer 内部**

- **作用域**：ViT 的 self-attention。
    
- **坐标维度**：二维 `(h, w)` —— 每个 patch 在图片里的行、列。
    
- **实现方式**：把 hidden dims 分成两半，一半用 h 位置做 RoPE，另一半用 w 位置做 RoPE，再拼接。
    
- **目的**：让 ViT 知道“左上角的 patch”和“右下角的 patch”在二维空间里的真实距离。
    

> 举例：一张 224×224 图切成 14×14 的 patch，2D-RoPE 给每个 patch 发一张“二维坐标身份证”。

---

### ② MRoPE：在 **大语言模型内部**

- **作用域**：LLM 的 self-attention（文本、图像、视频 token 混在一起以后）。
    
- **坐标维度**：**三维**
    
    - **t（时间）**：文本 token 递增计数；同一帧/图的所有视觉 token 共享同一个 t。
        
    - **h（高度）**：视觉 token 在图像里的行号。
        
    - **w（宽度）**：视觉 token 在图像里的列号。
        
- **实现方式**：把 hidden dims 切成三段，分别用 t、h、w 产生 cos/sin，再拼接。
    
- **目的**：
    
    1. 让 LLM 知道“第 3 个字”后面跟着“第 3 帧左上角的 patch”；
        
    2. 让不同帧、不同分辨率、不同 FPS 的视频都能对齐到“绝对时间轴”。
        

---

### ③ 一张脑图帮你记忆

Copy

```
┌───────────────┐        ┌──────────────────────┐
│  Vision ViT   │        │      LLM Decoder     │
│               │        │                      │
│  patch grid   │        │ text token position  │
│   (h, w)      │        │   ↓ t ↑              │
│   ↓ 2D-RoPE   │        │ image token position │
└───────────────┘        │   ↓ t,h,w ↑ MRoPE   │
                         └──────────────────────┘
```

---

### ④ 一句话总结

- **2D-RoPE** 像“拼图背面的小坐标”，只在拼图盒子（ViT）里用。
    
- **MRoPE** 像“整本书的页码 + 行号 + 列号”，把文字、插图、视频帧都编进同一条时间轴，让大模型翻页时不会串行。

## 困惑三 dit和LVLM的区别
## 今日流程

```
   def __init__(self, config):

        super().__init__(config)

        self.visual = Qwen2_5_VisionTransformerPretrainedModel._from_config(config.vision_config)

        self.language_model = Qwen2_5_VLTextModel._from_config(config.text_config)

        self.rope_deltas = None  # cache rope_deltas here
```
### visual

看看forward Qwen2_5_VisionTransformerPretrainedModel
是vit+连接层
windowsid的PatchEmbed, 2dRoPE 先在进入vit前重排计算好，再注入vit模型里
（每个winows组自己内部做attention，其实原始方法的patch也是个windows，这个方法也无非是把组分的更细一点） 
vit模型里就是归一化 attention 残差  +  归一化 mlp 残差
attention就是标准的transformer attention 哦对，还有2drope 
vit中 
1. **第一站 (Processor)**: 你理解了数据是如何被预处理成模型认识的input_ids, pixel_values和image_grid_sizes。
    
2. **第二站 (Forward入口)**: 你看到了图文特征是如何在Embedding空间通过masked_scatter“手术”般地拼接在一起的。
    
3. **第三站 (VisionTransformer)**: 你拆解了ViT的五脏六腑（PatchEmbed, RoPE, Block, Merger），并理解了它通过**“重排-计算-恢复”**的策略来高效实现窗口注意力。
    
4. **第四站 (VisionBlock)**: 你掌握了标准Transformer Block的“注意力+前馈网络”的两段式结构。
    
5. **第五站 (VisionAttention)**: 你深入到最核心的自注意力模块，亲眼见证了Q, K, V的生成、**2D RoPE的“旋转”应用**、以及注意力分数的完整计算流程。

## 今日目标
第一个为主线任务
1. [ ] 
2. [ ] 
3. [ ] 

## 主线任务拆解
最终目标：
里程碑：



## 今日完成
- [ ] 
- [ ] 
- [ ] 

## 今日总结

## 明日计划
- [ ] 
- [ ] 
- [ ] 




