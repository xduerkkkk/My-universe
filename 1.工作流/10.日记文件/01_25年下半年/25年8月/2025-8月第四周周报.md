# 📝学习内容
- Qwen2.5VL源码解读完毕
- Qwen2.5-Omni技术报告
- VLM-R1技术报告
- 设计自己的VLM架构
- 设计VLM中的LLM架构，是复习手段


输出笔记:
[[Qwen2.5-Omni 技术报告]]
[[VLM-R1 技术报告]]
[[03模型架构及数据流]]


# 整理困惑

## 困惑一 进入vit的attention层 发现**仍然使用了位置编码，是传统的sincos，我没理解不是已经有2drepo了吗"**

哎呀这是Repo流程没弄清。之前Repo出现是计算旋转角，attention出现是apply_rotary_pos_emb_vision(q, k, cos, sin) 这个函数，执行了RoPE的核心操作：用 cos 和 sin 值去**旋转** Q矩阵和K矩阵中的每一个向量。
## 困惑二 2drope和mrope的区别
2DRope是在vit，patching时发挥作用的。相当于给patch标记二维坐标让 ViT 知道“左上角的 patch”和“右下角的 patch”在二维空间里的真实距离。
Mrope是在LLM 的 self-attention时发挥作用，此时文本、图片、视频都以向量的形式混合在一起，我们的目的是...

## 困惑三 dit和LVLM的区别
dit是利用transformer，让文本与图片混合，达到文生图的方法，LVLM是vision encoder + 连接层 + 纯粹的LLM，是一种架构，这个是多模态模型，是理解文本与图片 输出文本的架构

## 困惑四 len（）都能用在哪些数据结构 Counter 能传入什么对象
都是应用在可迭代对象吧？？ 文件对象、生成器（generator）都是可迭代的，但你不能对它们用len()，因为它们的长度是不确定的，只有迭代到最后才知道。
## 困惑五 GIL的存在怎么了？
其实，完成多个任务，我们可以单进程多线程，也可以多进程单线程。按通讯来讲，单进程多线程效率最高，但GIL的存在，使得python不能单进程多线程，具体原理是...
## 困惑六 Python（特指官方的CPython解释器）是如何管理内存的
生成对象，有变量引用就存在着，发现没人引用了，就删除了。
## 困惑七 归一化为什么还要设置可学习的weight参数
额， 归一化后，再次给每个参数乘权重，让重要的继续重要，不要让归一化拉低了它的相对重要性


