# 📝学习内容
1. vision tranformer原理
2. clip图文对齐原理，数据构建
3. unconditional unet扩散模型，原理，以及代码
4. conditional unet扩散模型 原理，以及代码（crossattention实现）
5. stable diffusion3架构
6. qwen-vl技术报告 重要的是整理笔记，构建读技术报告的方法
7. qwen-vl源码（未执行完） 重要的是学习调试的方法，学习如何读源码

输出笔记：
- [[clip]]
- [[condition unet]]
- [[diffusion model]]
- [[Qwen-VL源码]]
- [[Qwen-VL技术报告]]
- [[vision transformer]]
# 💎学习总结
## 做的好的
- 严格采用番茄钟，同时还多了正向计时
- 有更多的思考用来优化学习方法
- 开始正式复习基础知识

## 做的不好的
- 时间投入不够  周末只要不在图书馆效率就极其低下
- 笔记应该要主动充实、完善、回顾。 自己只做了写，写的质量也不管，也不完善。


# 📒遇到的问题
好像比较少，这周后面三天学习时间不够，且在看技术报告和qwen源码 没有知识上的问题。同时感觉第一次接触就能理解的东西也多了， 困惑少了，输出的笔记多。
## 困惑一 vit生成patch，为什么代码用一个卷积和就把铺平展开搞定了

假设：
- 输入图像：224×224×3
- Patch size：16×16
我们先按正常逻辑分析， 每个小块图像（patch），它一定是16，16，3的，最后是是个768维向量。即每个patch对应生成一个768维的向量
一共有$14*14$个patch。 ，即196个patch。
也就是，最终生成，196个768维向量。
#### 卷积实现方式：

- 使用一个 **卷积核大小 = 16×16，步长 = 16，输出通道 = 768** 的卷积层。
- 输入：224×224×3
- 输出：14×14×768
卷积核会和图像上一个`16×16×3` 区域做逐元素乘法再求和；输出一个**单个数值**
我们有 **768 个这样的卷积核**（因为 out_channels=768），所以每个 patch 会输出 768 个数 → 一个 768 维向量



## 困惑二 diffusion model和stable diffusion的代码实现有什么不同吗

Diffusion Model 是一类生成模型的总称。其中，**无条件扩散模型**，比如经典的DDPM（**Denoising Diffusion Probabilistic Models**），它的去噪过程是随机的，生成内容完全取决于训练数据的分布。早期的扩散模型直接在**像素空间**进行操作，计算成本很高。为了解决这个问题，后来发展出了**潜在扩散模型 (LDM)**，也就是 **Stable Diffusion** 的基础，它在一个由VAE压缩出的**潜在空间 (Latent Space)** 中进行扩散，极大地提升了效率。
核心骨架基本没变， U-Net（下采样、上采样、跳跃连接）  LDM只是多了一个latent。   

后来随着发展呢，就出现了以**stable diffusion**为代表的文生图模型。叫有条件扩散。一般我们用某种方式，在训练去噪时，把文本信息/视频信息/音频信息 注入到去噪过程
最核心的技术是**Cross-Attention**。这个机制被集成到降噪网络的核心架构中。业界主要有两种主流的核心架构来承载这个过程
- 在原有的U-Net的ResNet块之间，插入了Cross-Attention层( **Stable Diffusion 1.x 和 2.x** 使用的架构)
- 整个骨干网络就是一个Transformer，它同样利用其内部的Cross-Attention机制来融合文本等条件信息(称为dit，**Stable Diffusion 3** 和 **Sora** 使用的)
## 困惑三 latent空间的必要性 怎么量化地表述
直接在像素空间做扩散的主要问题是计算成本过高。
举个例子，一张标准的512x512的RGB图片，它的数据维度是**512 * 512 * 3，大约是78万**。而Stable Diffusion使用的VAE可以将它压缩到一个**64x64x4，大约是1.6万**的潜在空间里。数据维度降低了将近**50倍**。因为扩散过程的核心计算（U-Net的前向传播）的复杂度与数据维度直接相关，所以在潜在空间里进行降噪，可以极大地降低训练和推理所需的**计算量和显存**，让模型能在消费级显卡上运行。”  

## 困惑四 无条件diffusion的unet，为什么也要加入selfattention，卷积+position编码还不够吗？
深层卷积的理论感受野可以覆盖全图，但这种依赖是**非常间接**的。左眼的信息需要经过**很多层**卷积的“淡化”和“混合”，才能慢慢地传递到右眼所在的区域。信息在长距离传递时，损耗严重。且，卷积核的形状和权重在整张图片上是**共享和固定**的。无论图片内容是什么，它都用同一套“模板”去处理局部信息。它很难根据图片内容，**动态地**调整关注点。
而位置编码只是告诉像素位置，这个位置信息要有人用呀。
谁用？ 是selfattention拿来用。 position对卷积没用的。
综上，resnetBlock+position的基础上，attention的作用
- 建立直接和动态的长距离依赖
- 借助位置信息进行关系理解
ResNet Block负责高效的局部特征提取，而Self-Attention负责全局的结构协调和一致性，两者结合才能生成高质量、高真实感的图像

## 困惑五 python特性
<font color="#f79646">c++是编译 python为什么和c++是完全不同的？ 我还记得什么解释性语言 这个名词又是啥意思？什么叫类型检查是在程序运行时进行的？ 为什么？ </font>

计组学过的，给忘了。 与编译型语言对应的就是解释型语言，编译型语言会将我们的代码变成汇编语言，再由汇编程序处理为机器语言，再执行机器语言。
解释型其实也编译，python是这样的 
源码 → **编译器** → 字节码（.pyc）→ **解释器/虚拟机** → 一条条执行。

所以 Python 也“编译”，只是它编译到字节码就停了，再由解释器在运行时把字节码一条条解释成机器动作。  
因此说“Python 不是编译”是口语化说法，准确说应是“Python 不是**提前完全编译到机器码**

解释型语言的好处是启动快，调试更灵活，但运行开销大
正因为是解释型语言，所以类型检查只能在程序运行时进行呀， 编译型语言编译的时候就能发现问题，在运行前。 

## 注意的一些点
- 扩散模型训练预测噪音时，是一步到位预测的，没有时间步迭代，在推理生成时，是根据时间步一步步减噪音的。 
- clip怎么在diffusion模型中体现？ 就是crossattention时， 我们给的图文对，这里就是一个clip模型的体现。 图文要对齐，俩个向量要接近
- crossattention中，图片是Q，文本是K和V

# 🚀下周计划

提速
- 周一完成qwenvl源码
- 周二完成qwen部署、微调的学习。如果仍有时间，看看qwen-image的技术报告
- 周三 了解全模态模型  如果仍有时间，看看qwen-image的源码
- 周四 学习多模态推理
- 周五周六 训练多模态模型  和当时那个minimodel一样 这次mini多模态 也写教程
- 周天 缓冲天   上面那些任务的调整池。 如果有时间，就去学习多模态微调
预计9.3到9.4把多模态完结

算法照常，目前挺好的。 二刷完代码随想录，就hot100练习面试题， 大厂真题练习笔试题，交替进行。现在先把自己的模板搞好

项目原先想搞后端的，现在认为先搁置。到后面集中处理项目。
现在复习python，dl/ml的一些基础知识，不断进行整理。
