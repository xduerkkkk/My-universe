

---
# 今天想说的话


今天是优化日总结的第一天，通过日总结逼迫自己把所有成果复习一遍，而不是记流水账！
把日总当成自己的技术帖子，我预设的第三阶段，简历阶段，就可以会看每一天的日记了。
周报以后概述，如果总想着周报去做思考、问题的总结，那其实量会很少。
## 一.今日成果

1. llm学习。 今天完整的把llm架构设计完毕，总共350行代码。 昨天是完成了归一化、位置编码 ，和attention层的前部部分。 今天完成attention层后面的点积及多头注意力合并， FeedForward层， TransformerBlock， Model类， ForCausalLM封装huggingface类模型。  
2. 算法 将双指针完结， 并详细规划了后面的算法任务。明天开始，二叉树的复习与模板整理  
3. 健身40min 背  
    
## 二 .思考
    

- attention_mask 理解不深，以及pytorch方法不熟练，torch.trilu, diagonal参数，masked_fill_方法，布尔参数。
    
- feedforward层，使用了类llama的门控机制。这里可以追加知识点，即各种开源大模型是如何设计具体的mlp和block的。后面整理
    
- block， 自己写代码才能发现自己以前没想清楚的细节啊！ norm在一个block里有两次，而且是独立学习的
    
- LLMModel 这里又再次发现对rope位置编码不熟悉，我们要根据pastkeyvalue的长度，计算出我们要插入位置编码的地方。  
    还有一起没注意的地方，就是kvcache到底怎么去用的。 我们要构造all_presents ，把每一次blocks list后的kvvalue记录下来  
    。这里暴露出数据流动的不清晰。 后续需要，从头到位把各个地方维度标出来·，数据流动图画出来，后面的微调项目的构造才能得心应手
    
- ForCausalLM 只是利用lm_head 输出logits，还有封装成CausalLMOutputWithPast。generate方法也能被封装，但我还是手动实现了一个，不是真的用，只是教学意义。 一下又发现问题了， next_token_logits = logits[:, -1, :] 这个切片我写错了。 以及argmax后，要unsqueeze(-1)，忘了说明维度并不熟记于心
    
- 算法上，自己确实没总结出什么。如果说有收获的话，就是发现，有的题是掌握模板了，套用模板，或者复合结构，用A模板+B模板，但有的题，是纯思维。 模板只是很基础的，还考的思维，有点数学，又有点脑筋急转弯的感觉。这种题怎么攻破呢？ 我觉得就放到周末的笔试真题环节吧，现在就做模板题，这种思维题先放了。
    
- 基础复习， 一遍设计llm一遍复习基础知识。 比如pytorch张量形状有关的各个方法，我是把每个方法都手写例子记在juptyer了， 但没保证完全熟练。没事，忘了就再实操一下。 还有就是python的类型提示，Optional，Tuple 这种， 再次理解。register_buffer 的意义，instance object的区别 （instance有语境，object无语境，万物皆对象）  
## 遇到的挑战与明日计划  

挑战：算法今天专注度不够。 llm的话，还没有把数据流动梳理。 就是有一种，”还没有特别吃透“的直觉  
明日计划：首先就把这llm数据流再次吃透了，回顾各个零件。 Rope编码，记录下来。后续常看。 接着看模型架构。  
这些看完了就编写pretrain和posttrain的代码了，这个过程能复习dl的基础应用知识。

- **【待办】绘制LLM前向传播的Tensor维度变化图。**
    
- **【待办】整理一篇关于torch.cat vs torch.stack的笔记。**
    
- **【待办】调研并对比 LLaMA, Qwen, ChatGLM 在Block设计上的三个主要异同点**




## attenout 的合并多头、最终投影

## attention_mask
- 如何构造？
- 加在哪？

## mask_fill_ 方法

## **SwiGLU**

## 调用函数 实例变量（instance variable）

## register_buffer

## 类型提示，Optional，Tuple
 
## xx.sort() 与 sorted(xx)

## 今日目标
第一个为主线任务
1. [ ] 
2. [ ] 
3. [ ] 

## 主线任务拆解
最终目标：
里程碑：



## 今日完成
- [ ] 
- [ ] 
- [ ] 

## 今日总结

## 明日计划
- [ ] 
- [ ] 
- [ ] 




