

---
# 今天想说的话


## 问题一
<font color="#f79646">block逻辑</font>
我以为所有token碰到的block群都是新的，空的。 实际上一个seq里，几个token共用一个block群，每个token填block


## 问题二
<font color="#f79646">有两次对太长的客人进行拒绝 一次是num_prefill_tokens > self.prompt_limit 一次是使用can_allocate 检查 可不可以理解为，第一个是人为限制的，公司不想有太长的过来，就算我能接待我也不想接待。 第二个是，你的行李其实符合最初规定，但仍然太长了，今天空间有限制，就算我们全部剩余空间服务你也无法服务，所以走吧 是这样吗？</font>
第一个没错，第二个有点问题

| 检查点        | prompt > prompt_limit         | can_allocate == NEVER       |
| ---------- | ----------------------------- | --------------------------- |
| **性质**     | **业务/策略限制**                   | **物理/硬件限制**                 |
| **判断依据**   | prompt的token数 vs. **人为设定的上限** | prompt所需的块数 vs. **GPU的总块数** |
| **是否关心现状** | **否**                         | **否** (只关心总容量)              |
| **比喻**     | “本店不接待超大旅行团” (规定)             | “您的旅行团大到我们整个酒店都住不下” (容量)    |

所以，一个prompt要想被拒绝，它需要：

1. 首先，它的长度本身可能就超过了prompt_limit，直接被“门规”拦下。
    
2. 即使没超过prompt_limit，但如果它换算成物理块之后，还是大到几乎能撑爆整个GPU，那它还是会被“物理极限”给拦下。
## 问题三
<font color="#f79646">就是说，如果swap是空的，那就经过四层筛选，看有没有新客人能进来做prefill。 第一层是是否超了公司规定最长行李（人为规定），第二是是否超过公司总容量，第三个是加上这位客人是否超过单次调度允许最大值，就是接待新客人最大值（我觉得这也是人为规定的对吗）， 第四个是，如果他被接待，变成running，总running人数有没有超规定（也是人为规定？） 是这样吗？ 还有个问题，你说prompt_run=True：设置一个标志，告诉LLMEngine，“嘿，这次的名单里有新人，请使用Prefill Kernel来执行！” 我想起llmengine每次要么在prefill要么在decoding，那如果条件特别巧合，一直能有新人来，那不就一直prefill吗？ 还是说，我们的这些人为规定，巧妙地平衡了prefill和decoding的次数</font>

### 对“四层筛选”的确认

> “如果swap是空的，那就经过四层筛选... 第三...（我觉得这也是人为规定的对吗）， 第四个...（也是人为规定？） 是这样吗？”

**完全正确！** 你的理解非常精准。我们来确认一下：

1. **行李规定 (prompt_limit)**：是的，**人为规定**的业务上限。
    
2. **酒店容量 (can_allocate's NEVER check)**：这是**硬件决定**的物理上限。
    
3. **单次新客Token上限 (max_num_batched_tokens)**：是的，**人为规定**的流量控制参数。
    
4. **总在场人数上限 (max_num_seqs)**：是的，也是**人为规定**的流量控制参数。
    

所以，在这四层筛选中，有三层都是**可配置的、人为设定的策略参数**，只有一层是不可逾越的硬件鸿沟。这些参数共同构成了调度器的“行为准则”。

---

### 对“一直Prefill”的精妙提问

> “我想起llmengine每次要么在prefill要么在decoding，那如果条件特别巧合，一直能有新人来，那不就一直prefill吗？ 还是说，我们的这些人为规定，巧妙地平衡了prefill和decoding的次数”

这是一个顶级的问题！你指出了这种“模式统一”调度策略一个**潜在的、非常真实的风险**——**“Decoding饥饿（Decoding Starvation）”**。

**你的推断是完全正确的**：如果新请求源源不断地到来，并且每次调度都能满足那“四层筛选”，那么调度器理论上**确实会一直执行Prefill阶段**，导致那些已经在running队列里、等待进行Decoding的老请求，被**无限期地“暂停”**，无法取得进展。

**那么，vLLM是如何解决这个问题的呢？**  
答案就在我们之前分析过的、但当时看似不起眼的一个检查点里：

while self._passed_delay(now) and self.waiting:

我们来深入理解一下self._passed_delay(now)这个函数的**真实意图**。

它的伪代码逻辑大致是这样的：

codePython

```
def _passed_delay(self, now: float) -> bool:
    # 如果上一次调度，我们处理的是Decoding阶段 (没有新人进来)
    if not self.prev_prompt: 
        # 那么这次可以无条件地尝试接纳新人
        return True
    
    # 如果上一次调度，我们处理的是Prefill阶段 (有新人进来了)
    else: 
        # 计算距离上次Prefill过去了多久
        time_since_last_prompt = now - self.prev_time
        
        # 只有当经过了一个很短的延迟时间后，才允许再次进行Prefill
        return time_since_last_prompt > self.scheduler_config.delay_factor
```

- self.prev_prompt：就是我们之前讨论的那个“上次有没有进新人”的标志。
    
- delay_factor: 这是一个非常小的、人为设定的时间值（比如几毫秒）。
    

**这个机制如何巧妙地平衡Prefill和Decoding？**

1. **Prefill优先，但有“冷却时间”**：调度器总是会**优先**尝试从waiting队列拉新人（因为swapped为空时，代码就先走这个分支）。但是，一旦它成功地执行了一次Prefill，self.prev_prompt就会被设为True。在接下来的极短时间内（由delay_factor决定），_passed_delay会返回False，**从而强制阻止调度器连续不断地执行Prefill**。
    
2. **为Decoding创造机会**：在这段强制的“冷却时间”里，while循环的条件不满足，调度器就不会去waiting队列里拉新人。代码会跳过这个大的if分支，进入到处理running和swapped队列的逻辑中。这就**保证了Decoding阶段的请求，至少有机会被执行**。
    

**结论**：


## 问题四

## 今日目标
第一个为主线任务
1. [ ] 
2. [ ] 
3. [ ] 

## 主线任务拆解
最终目标：
里程碑：



## 今日完成
- [ ] 
- [ ] 
- [ ] 

## 今日总结

## 明日计划
- [ ] 
- [ ] 
- [ ] 




