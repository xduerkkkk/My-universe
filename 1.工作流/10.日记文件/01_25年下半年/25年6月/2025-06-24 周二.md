

---
# 今天想说的话

cuda out of memory怎么办？量化模型、减小喂入模型的数据集数量、转移gpudevice 
也引入了一个问题，如何判断预留memory大小的？

autotokenizer和automodel的选项
dataset可以加载数据，也可以手动用python的读文件、json.load等等

实际实现lora微调，也就是引入peft库写个LoraConfig，然后get_peft_model(model, config)即可，我的理解对吗？实现Qlora就是在建立model阶段加个4bit量化，后面和lora一样，是这样吗？

## 今日目标
1. [ ] 
2. [ ] 
3. [ ] 

## 今日计划
- 上午：
    - [ ] 07:00 - 08:00：计划 1
    - [ ] 09:00 - 10:00：计划 2
- 下午：
    - [ ] 13:00 - 14:00：计划 3
    - [ ] 15:00 - 16:00：计划 4
- 晚上：
    - [ ] 19:00 - 20:00：计划 5
    - [ ] 20:30 - 21:30：计划 6

## 今日完成
- [ ] 完成 1
- [ ] 完成 2
- [ ] 完成 3

## 今日总结

## 明日计划
- [ ] 明日目标 1
- [ ] 明日目标 2
- [ ] 明日目标 3




